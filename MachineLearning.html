<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  MachineLearning - Time渐行渐远
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="Time渐行渐远" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:dmlcoding.com ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="collections.html">COLLECTION</a></li>
        
        <li id=""><a target="_self" href="tools.html">TOOLS</a></li>
        
        <li id=""><a target="_blank" href="about.html">ABOUT</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; Time渐行渐远</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_self" href="collections.html">COLLECTION</a></li>
        
        <li><a target="_self" href="tools.html">TOOLS</a></li>
        
        <li><a target="_blank" href="about.html">ABOUT</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="BigData.html">BigData</a></li>
        
            <li><a href="Python.html">Python</a></li>
        
            <li><a href="Linux.html">Linux</a></li>
        
            <li><a href="Life.html">Life</a></li>
        
            <li><a href="Spark.html">Spark</a></li>
        
            <li><a href="Hive.html">Hive</a></li>
        
            <li><a href="Yarn.html">Yarn</a></li>
        
            <li><a href="Scala.html">Scala</a></li>
        
            <li><a href="VPS.html">VPS</a></li>
        
            <li><a href="MachineLearning.html">MachineLearning</a></li>
        
            <li><a href="Writer.html">Writer</a></li>
        
            <li><a href="Shell.html">Shell</a></li>
        
            <li><a href="Algorithm.html">Algorithm</a></li>
        
            <li><a href="Mac.html">Mac</a></li>
        
            <li><a href="Presto.html">Presto</a></li>
        
            <li><a href="Mysql.html">Mysql</a></li>
        
            <li><a href="Maven.html">Maven</a></li>
        
            <li><a href="Kafka.html">Kafka</a></li>
        
            <li><a href="Java.html">Java</a></li>
        
            <li><a href="DevTools.html">DevTools</a></li>
        
            <li><a href="Hbase.html">Hbase</a></li>
        
            <li><a href="ELK.html">ELK</a></li>
        
            <li><a href="Druid.html">Druid</a></li>
        
            <li><a href="Docker.html">Docker</a></li>
        
            <li><a href="DesignPattern.html">DesignPattern</a></li>
        
            <li><a href="Computer.html">Computer</a></li>
        
            <li><a href="Redis.html">Redis</a></li>
        
            <li><a href="Zookeeper.html">Zookeeper</a></li>
        
            <li><a href="BlockChain.html">BlockChain</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="15420931314995.html">
                
                  <h1>Objective vs. Cost vs. Loss vs. Error Function</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p><strong><u>The function we want to minimize or maximize is called the objective function, or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function - these terms are synonymous. The cost function is used more in optimization problem and loss function is used in parameter estimation.</u></strong></p>

<p>The loss function (or error) is for a single training example, while the cost function is over the entire training set (or mini-batch for mini-batch gradient descent). Therefore, a loss function is a part of a cost function which is a type of an objective function.  Objective function, cost function, loss function: are they the same thing? | </p>

<p>StackExchange</p>

<ul>
<li> Loss function is usually a function defined on a data point, prediction and label, and measures the penalty. For example:
<ul>
<li> square loss l(f(xi|θ),yi)=(f(xi|θ)−yi)2, used in linear regression</li>
<li> hinge loss l(f(xi|θ),yi)=max(0,1−f(xi|θ)yi), used in SVM</li>
<li> 0/1 loss l(f(xi|θ),yi)=1⟺f(xi|θ)≠yi, used in theoretical analysis and definition of accuracy</li>
</ul></li>
<li> Cost function is usually more general. It might be a sum of loss functions over your training set plus some model complexity penalty (regularization). For example:
<ul>
<li> Mean Squared Error MSE(θ)=1N∑Ni=1(f(xi|θ)−yi)2</li>
<li> SVM cost function SVM(θ)=∥θ∥2+C∑Ni=1ξi (there are additional constraints connecting ξi with C and with training set)</li>
</ul></li>
<li> Objective function is the most general term for any function that you optimize during training. For example, a probability of generating training set in maximum likelihood approach is a well defined objective function, but it is not a loss function nor cost function (however you could define an equivalent cost function). For example:
<ul>
<li> MLE is a type of objective function (which you maximize)</li>
<li>Divergence between classes can be an objective function but it is barely a cost function, unless you define something artificial, like 1-Divergence, and name it a cost</li>
</ul></li>
<li> Error function - Backpropagation; or automatic differentiation, is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function. This technique is also sometimes called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers. Backpropagation | Wikipedia</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/11/13</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='MachineLearning.html'>MachineLearning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15398508250689.html">
                
                  <h1>Isolation Forest</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>scikit-learn返回:是异常点(-1)或者不是异常点(1)</p>

<p><img src="media/15398508250689/15398521143637.jpg" alt=""/></p>

<p>孤立森林--&gt;查看数据页面,如上所示<br/>
原始数据的所有列,预测出来是否是异常值,也即是是否偏离(偏移即是-1),偏移度也就是decision_function算出来的值,返回样本的异常评分，值越小表示越有可能是异常样本<br/>
data,model.predict(X_train),model.decision_function(X_train)</p>

<pre class="line-numbers"><code class="language-text">df=pd.concat([pd.DataFrame(X_train),pd.Series(clf.predict(X_train)), pd.Series(clf.decision_function(X_train))], axis=1)

df.columns = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/10/18</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='MachineLearning.html'>MachineLearning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15396891344806.html">
                
                  <h1>聚类算法评估指标</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h1 id="toc_0">Not Given Label</h1>

<h2 id="toc_1">1、Compactness(紧密性)(CP)</h2>

<h1 id="toc_2">代码实现</h1>

<pre class="line-numbers"><code class="language-text"># -*- coding: utf-8 -*-
&#39;&#39;&#39;
    Created by hushiwei on 2018/10/16
    Desc : 进行聚类算法的评估指标计算
    Note : CP指数,SP指数,DB,DVI
&#39;&#39;&#39;

# 导入包
from numpy import *
import numpy as np
import pandas as pd
from sklearn.datasets import make_blobs  # 导入产生模拟数据的方法
from sklearn.cluster import KMeans  # 导入kmeans 类
import time
from MachineLearning.cluster_evaluate.calculate_dvi import dunn

def distEclud(vecA, vecB, axis=None):
    &#39;&#39;&#39;
    求两个向量之间的距离,计算欧几里得距离
    same as : np.linalg.norm(np.asarray(datas[0].iloc[0].values) - np.asarray(datas[0].iloc[1].values))
    :param vecA: 中心点
    :param vecB: 数据点
    :param axis: np.sum的参数
    :return:
    &#39;&#39;&#39;
    return np.sqrt(np.sum(np.power(vecA - vecB, 2), axis=axis))


def calculate_cp_i(cluster_center, cluster_point):
    &#39;&#39;&#39;
    计算聚类类各点到聚类中心的平均距离
    :param cluster_center: 聚类中心点
    :param cluster_point: 聚类样本点
    :return:
    &#39;&#39;&#39;
    return np.average(distEclud(cluster_center, cluster_point, axis=1))


def calculate_cp(cluster_centers, cluster_points):
    &#39;&#39;&#39;
    CP,计算每一个类各点到聚类中心的平均距离
    CP越低意味着类内聚类距离越近
    :param cluster_centers: 聚类中心点
    :param cluster_points: 聚类样本点
    :return:
    &#39;&#39;&#39;

    cps = [calculate_cp_i(cluster_centers[i], cluster_points[i]) for i in arange(len(cluster_centers))]
    cp = np.average(cps)
    return cp


def calculate_sp(cluster_centers):
    &#39;&#39;&#39;
    SP,计算各聚类中心两两之间的平均距离
    SP越高意味类间聚类距离越远
    :param cluster_centers:
    :return:
    &#39;&#39;&#39;
    k = len(cluster_centers)
    res = []
    for i in arange(k):
        tmp = []
        for j in arange(i + 1, k):
            tmp.append(distEclud(cluster_centers[i], cluster_centers[j]))
        res.append(np.sum(tmp))

    sp = (2 / (k * k - k)) * np.sum(res)
    return sp


def calculate_db(cluster_centers, cluster_points):
    &#39;&#39;&#39;
    DB,计算任意两类别的类内距离平均距离(CP)之和除以两聚类中心距离求最大值
    DB越小意味着类内距离越小 同时类间距离越大
    :param cluster_centers: 聚类中心点
    :param cluster_points: 聚类样本点
    :return:
    &#39;&#39;&#39;
    n_cluster = len(cluster_centers)
    cps = [calculate_cp_i(cluster_centers[i], cluster_points[i]) for i in arange(n_cluster)]
    db = []

    for i in range(n_cluster):
        for j in range(n_cluster):
            if j != i:
                db.append((cps[i] + cps[j]) / distEclud(cluster_centers[i], cluster_centers[j]))
    db = (np.max(db) / n_cluster)
    return db


def calculate_dvi(cluster_points):
    &#39;&#39;&#39;
    DVI计算,任意两个簇元素的最短距离(类间)除以任意簇中的最大距离(类内).
    DVI越大意味着类间距离越大 同时类内距离越小。
    :param cluster_points:
    :return:
    &#39;&#39;&#39;
    # calcuation of maximum distance
    d1 = []
    d2 = []
    d3 = []
    # 类内最大距离
    for k, cluster_point in cluster_points.items():
        # 遍历每一个簇中每一个元素
        for i in range(len(cluster_point)):
            temp1 = cluster_point.iloc[i].values
            for j in range(len(cluster_point)):
                temp2 = cluster_point.iloc[j].values

                # 求簇内两元素的距离
                dist = distEclud(temp1, temp2)
                d1.insert(j, dist)

            d2.insert(i, max(d1))
            d1 = []

        d3.insert(k, max(d2))
        d2 = []
    xmax = max(d3)

    # calcuation of minimun distance
    d1 = []
    d2 = []
    d3 = []
    d4 = []
    # 类间最小距离
    for k, cluster_point in cluster_points.items():
        # 遍历每一个簇中每一个元素
        for j in range(len(cluster_point)):
            temp1 = cluster_point.iloc[j].values
            for key in cluster_points.keys():
                if not key == k:
                    other_cluster_df = cluster_points[key]
                    for index in range(len(other_cluster_df)):
                        temp2 = other_cluster_df.iloc[index].values
                        dist = distEclud(temp1, temp2)
                        d1.insert(index, dist)

                    d2.insert(key, min(d1))
                    d1 = []
            d3.insert(j, min(d2))
            d2 = []
        d4.insert(k, min(d3))
    xmin = min(d4)

    dunn_index = xmin / xmax
    return dunn_index


if __name__ == &#39;__main__&#39;:
    # 1. 产生模拟数据
    N = 1000
    centers = 4
    X, Y = make_blobs(n_samples=N, n_features=2, centers=centers, random_state=28)
    # df = pd.DataFrame(X, columns=list(&#39;abcdefghij&#39;))
    # df.to_csv(
    #     &#39;/Users/hushiwei/PycharmProjects/gai_platform/data/data_spliter/local_debug_data/cluster_data/cluster_data.csv&#39;,
    #     sep=&#39;,&#39;, index=False)
    # sys.exit()

    # 2. 模型构建
    km = KMeans(n_clusters=centers, init=&#39;random&#39;, random_state=28)
    km.fit(X)

    # 模型的预测
    y_hat = km.predict(X[:10])
    print(y_hat)

    print(&quot;所有样本距离所属簇中心点的总距离和为:%.5f&quot; % km.inertia_)
    print(&quot;所有样本距离所属簇中心点的平均距离为:%.5f&quot; % (km.inertia_ / N))

    print(&quot;所有的中心点聚类中心坐标:&quot;)
    cluster_centers = km.cluster_centers_
    print(cluster_centers)

    print(&quot;score其实就是所有样本点离所属簇中心点距离和的相反数:&quot;)
    print(km.score(X))

    # 统计各个聚簇点的类别数目
    r1 = pd.Series(km.labels_).value_counts()

    kmlabels = km.labels_  # 得到类别，label_ 是内部变量
    X = pd.DataFrame(X, columns=list(&#39;ab&#39;))
    r = pd.concat([X, pd.Series(kmlabels, index=X.index)], axis=1)  # 详细输出每个样本对应的类别，横向连接（0是纵向），得到聚类中心对应的类别下的数目

    r.columns = list(X.columns) + [&#39;class&#39;]  # 重命名表头  加一列的表头
    print(r.head())

    # 根据聚类信息,将数据进行分类{聚类id:聚类点}
    cluster_centers = {k: value for k, value in enumerate(cluster_centers)}
    cluster_points = {label: r[r[&#39;class&#39;] == label].drop(columns=[&#39;class&#39;]) for label in np.unique(km.labels_)}

    print(&#39;~&#39; * 10, &#39;evaluate kmeans&#39;, &#39;~&#39; * 10)

    cp = calculate_cp(cluster_centers, cluster_points)
    print(&#39;cp : &#39;, cp)

    sp = calculate_sp(cluster_centers)
    print(&#39;sp : &#39;, sp)

    dp = calculate_db(cluster_centers, cluster_points)
    print(&#39;dp : &#39;, dp)

    start=time.time()
    # dvi = calculate_dvi(cluster_points)
    # print(&#39;dvi : &#39;, dvi)


    a = [point.values for point in cluster_points.values()]
    di=dunn(a)
    print(&#39;di : &#39;,di)

    print(&quot;dvi cost &quot;,time.time()-start)

</code></pre>

<p>dvi的计算,代码2</p>

<pre class="line-numbers"><code class="language-text"># -*- coding: utf-8 -*-
&#39;&#39;&#39;
    Created by hushiwei on 2018/10/23
    Desc : 计算dvi
    Note : 这个和那个,有区别
&#39;&#39;&#39;

import numpy as np
from sklearn.metrics.pairwise import euclidean_distances


def delta(ck, cl):
    values = np.ones([len(ck), len(cl)]) * 10000

    for i in range(0, len(ck)):
        for j in range(0, len(cl)):
            values[i, j] = np.linalg.norm(ck[i] - cl[j])

    return np.min(values)


def big_delta(ci):
    values = np.zeros([len(ci), len(ci)])

    for i in range(0, len(ci)):
        for j in range(0, len(ci)):
            values[i, j] = np.linalg.norm(ci[i] - ci[j])

    return np.max(values)


def dunn(k_list):
    &quot;&quot;&quot; Dunn index [CVI]

    Parameters
    ----------
    k_list : list of np.arrays
        A list containing a numpy array for each cluster |c| = number of clusters
        c[K] is np.array([N, p]) (N : number of samples in cluster K, p : sample dimension)
    &quot;&quot;&quot;
    deltas = np.ones([len(k_list), len(k_list)]) * 1000000
    big_deltas = np.zeros([len(k_list), 1])
    l_range = list(range(0, len(k_list)))

    for k in l_range:
        for l in (l_range[0:k] + l_range[k + 1:]):
            deltas[k, l] = delta(k_list[k], k_list[l])

        big_deltas[k] = big_delta(k_list[k])

    di = np.min(deltas) / np.max(big_deltas)
    return di


def delta_fast(ck, cl, distances):
    values = distances[np.where(ck)][:, np.where(cl)]
    values = values[np.nonzero(values)]

    return np.min(values)


def big_delta_fast(ci, distances):
    values = distances[np.where(ci)][:, np.where(ci)]
    values = values[np.nonzero(values)]

    return np.max(values)


def dunn_fast(points, labels):
    &quot;&quot;&quot; Dunn index - FAST (using sklearn pairwise euclidean_distance function)

    Parameters
    ----------
    points : np.array
        np.array([N, p]) of all points
    labels: np.array
        np.array([N]) labels of all points
    &quot;&quot;&quot;
    distances = euclidean_distances(points)
    ks = np.sort(np.unique(labels))

    deltas = np.ones([len(ks), len(ks)]) * 1000000
    big_deltas = np.zeros([len(ks), 1])

    l_range = list(range(0, len(ks)))

    for k in l_range:
        for l in (l_range[0:k] + l_range[k + 1:]):
            deltas[k, l] = delta_fast((labels == ks[k]), (labels == ks[l]), distances)

        big_deltas[k] = big_delta_fast((labels == ks[k]), distances)

    di = np.min(deltas) / np.max(big_deltas)
    return di


def big_s(x, center):
    len_x = len(x)
    total = 0

    for i in range(len_x):
        total += np.linalg.norm(x[i] - center)

    return total / len_x


def davisbouldin(k_list, k_centers):
    &quot;&quot;&quot; Davis Bouldin Index

    Parameters
    ----------
    k_list : list of np.arrays
        A list containing a numpy array for each cluster |c| = number of clusters
        c[K] is np.array([N, p]) (N : number of samples in cluster K, p : sample dimension)
    k_centers : np.array
        The array of the cluster centers (prototypes) of type np.array([K, p])
    &quot;&quot;&quot;
    len_k_list = len(k_list)
    big_ss = np.zeros([len_k_list], dtype=np.float64)
    d_eucs = np.zeros([len_k_list, len_k_list], dtype=np.float64)
    db = 0

    for k in range(len_k_list):
        big_ss[k] = big_s(k_list[k], k_centers[k])

    for k in range(len_k_list):
        for l in range(0, len_k_list):
            d_eucs[k, l] = np.linalg.norm(k_centers[k] - k_centers[l])

    for k in range(len_k_list):
        values = np.zeros([len_k_list - 1], dtype=np.float64)
        for l in range(0, k):
            values[l] = (big_ss[k] + big_ss[l]) / d_eucs[k, l]
        for l in range(k + 1, len_k_list):
            values[l - 1] = (big_ss[k] + big_ss[l]) / d_eucs[k, l]

        db += np.max(values)
    res = db / len_k_list
    return res
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/10/16</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='MachineLearning.html'>MachineLearning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15395734385439.html">
                
                  <h1>模型评估 K-S值和AUC的区别</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>在模型建立之后，必须对模型的效果进行评估，因为数据挖掘是一个探索的过程，评估-优化是一个永恒的过程。在分类模型评估中，最常用的两种评估标准就是K-S值和AUC值。</p>

<p>可能有人会问了，为什么不直接看正确率呢？你可以这么想，如果一批样本中，正样本占到90%，负样本只占10%，那么我即使模型什么也不做，把样本全部判定为正，也能有90%的正确率咯？所以，用AUC值够保证你在样本不均衡的情况下也能准确评估模型的好坏，而K-S值不仅能告诉你准确与否，还能告诉你模型对好坏客户是否有足够的区分度。</p>

<p>下面分别看两种评估指标的概念。</p>

<h1 id="toc_0">一、ROC曲线和AUC值</h1>

<p>在逻辑回归、随机森林、GBDT、XGBoost这些模型中，模型训练完成之后，每个样本都会获得对应的两个概率值，一个是样本为正样本的概率，一个是样本为负样本的概率。把每个样本为正样本的概率取出来，进行排序，然后选定一个阈值，将大于这个阈值的样本判定为正样本，小于阈值的样本判定为负样本，然后可以得到两个值，<strong>一个是真正率，一个是假正率。</strong></p>

<p>真正率即判定为正样本且实际为正样本的样本数/所有的正样本数，假正率为判定为正样本实际为负样本的样本数/所有的负样本数。每选定一个阈值，就能得到一对真正率和假正率，由于判定为正样本的概率值区间为[0,1]，那么阈值必然在这个区间内选择，因此在此区间内不停地选择不同的阈值，重复这个过程，就能得到一系列的真正率和假正率，以这两个序列作为横纵坐标，即可得到ROC曲线了。<strong>而ROC曲线下方的面积，即为AUC值。</strong></p>

<p>对于AUC值，也许有一个更直观的理解，那就是，在按照正样本概率值对所有样本排序后，任意选取一对正负样本，正样本排在负样本之前的概率值，即为AUC值。也就是说，当所有的正样本在排序后都能排在负样本之前时，就证明所有的样本都被正确分类了，此时的AUC值也会为1。那么AUC值也就很好算了，如果有N个负样本，其中正样本有M个，那么可取的所有带正样本的样本对数对于排在第一位的正样本来说，有M+N-1个，但其中包含M-1对（正，正）的样本，而对于后面所有的正样本而言，能够取到的正样本概率大于负样本对数肯定小于其位置-1。</p>

<h1 id="toc_1">K-S曲线</h1>

<p>KS(Kolmogorov-Smirnov)值越大，表示模型能够将正、负客户区分开的程度越大。KS值的取值范围是[0，1] </p>

<p>通常来讲，KS&gt;0.2即表示模型有较好的预测准确性。</p>

<p>K-S曲线其实数据来源和本质和ROC曲线是一致的，只是ROC曲线是把真正率和假正率当作横纵轴，而K-S曲线是把真正率和假正率都当作是纵轴，横轴则由选定的阈值来充当。</p>

<p>下面这一段解释得更详细的K-S和AUC的区别是参考的这篇博客：</p>

<p><a href="https://blog.csdn.net/sinat_30316741/article/details/80018932">https://blog.csdn.net/sinat_30316741/article/details/80018932</a></p>

<blockquote>
<p>由于ks值能找出模型中差异最大的一个分段，因此适合用于cut_off，像评分卡这种就很适合用ks值来评估。但是ks值只能反映出哪个分段是区分最大的，而不能总体反映出所有分段的效果，因果AUC值更能胜任。 <br/>
ROC值一般在0.5-1.0之间。值越大表示模型判断准确性越高，即越接近1越好。ROC=0.5表示模型的预测能力与随机结果没有差别。 <br/>
KS值表示了模型将+和-区分开来的能力。值越大，模型的预测准确性越好。一般，KS&gt;0.2即可认为模型有比较好的预测准确性。</p>
</blockquote>

<p>好了，引用结束。 <br/>
K-S值一般是很难达到0.6的，在0.2~0.6之间都不错。一般如果是如果负样本对业务影响极大，那么区分度肯定就很重要，此时K-S比AUC更合适用作模型评估，如果没什么特别的影响，那么用AUC就很好了。</p>

<h1 id="toc_2">三、KS的计算和曲线绘制</h1>

<p>代码如果要用，最好自己改改，因为这个数据格式是我自己用的，计算方式也是跟数据格式有关系的，工作中用的可能不一样，所以最好是自己根据自己的写一个，这个也只是我根据自己的数据格式来写的，重要的是思路，不是代码本身。而且我画的曲线是折线，最好是自己考虑写一个平滑曲线的。</p>

<pre class="line-numbers"><code class="language-text">import matplotlib.pyplot as plt
#第一个参数是模型的预测值，第二个参数是模型的真实值
def draw_ks_curve(predict_result,true_result):
    tpr_list = []  #存放真正率数据
    fpr_list = []  #存放假正率数据
    dif_list = []  #存放真假正率差值
    max_ks_dot = []

    for i in np.arange(0,1.1,0.1):
        tpr = 0
        fpr = 0
        for j in range(len(predict_result)):
            if list(predict_result[j])[0]&gt;i and true_result[j]==1:
               tpr = tpr+1
               tpr_list.append(tpr)
            if list(predict_result[j])[0]&gt;i and true_result[j]==0:
               fpr = fpr+1
               fpr_list.append(fpr)
        tpr = tpr/sum(true_result)
        fpr = fpr/(len(true_result)-sum(true_result))
    fig = plt.figure(num=1, figsize=(15, 8),dpi=80)     #开启一个窗口，同时设置大小，分辨率
    plt.plot(np.arange(0,1,0.1),tpr_list)
    plt.plot(np.arange(0,1,0.1),fpr_list)

</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/10/15</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='MachineLearning.html'>MachineLearning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15299155104492.html">
                
                  <h1>HMM(摘自:统计学习方法)</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>HMM(隐马尔可夫模型的定义):隐马尔可夫模型是关于时序的概率模型,描述由一个隐藏的马尔可夫链随机生成不可观察的状态随机序列,再由各个状态生成一个观测而产生观测随机序列的过程.隐藏的马尔可夫链随机生成的状态的序列,称为状态序列(state sequence);每个状态生成一个观测,而由此产生的观测的随机序列,称为观测序列(observation sequence).序列的每一个位置又可以看作是一个时刻.</p>


                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                   <a href="15299155104492.html">Read more</a>&nbsp;&nbsp; 
                    <span class="date">2018/6/25</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='MachineLearning.html'>MachineLearning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="MachineLearning_1.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="https://s.gravatar.com/avatar/2d9bdeec5ad2754d8a47063df1346ee8?s=80" /></div>
            
                <h1>Time渐行渐远</h1>
                <div class="site-des">Coding Changing The World</div>
                <div class="social">







<a target="_blank" class="weibo" href="http://weibo.com/hswnice" title="weibo">Weibo</a>
<a target="_blank" class="twitter" target="_blank" href="https://twitter.com/HswTime" title="Twitter">Twitter</a>
<a target="_blank" class="github" target="_blank" href="https://github.com/Timehsw" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:hswv5@foxmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="BigData.html"><strong>BigData</strong></a>
        
            <a href="Python.html"><strong>Python</strong></a>
        
            <a href="Linux.html"><strong>Linux</strong></a>
        
            <a href="Life.html"><strong>Life</strong></a>
        
            <a href="Spark.html"><strong>Spark</strong></a>
        
            <a href="Hive.html"><strong>Hive</strong></a>
        
            <a href="Yarn.html"><strong>Yarn</strong></a>
        
            <a href="Scala.html"><strong>Scala</strong></a>
        
            <a href="VPS.html"><strong>VPS</strong></a>
        
            <a href="MachineLearning.html"><strong>MachineLearning</strong></a>
        
            <a href="Writer.html"><strong>Writer</strong></a>
        
            <a href="Shell.html"><strong>Shell</strong></a>
        
            <a href="Algorithm.html"><strong>Algorithm</strong></a>
        
            <a href="Mac.html"><strong>Mac</strong></a>
        
            <a href="Presto.html"><strong>Presto</strong></a>
        
            <a href="Mysql.html"><strong>Mysql</strong></a>
        
            <a href="Maven.html"><strong>Maven</strong></a>
        
            <a href="Kafka.html"><strong>Kafka</strong></a>
        
            <a href="Java.html"><strong>Java</strong></a>
        
            <a href="DevTools.html"><strong>DevTools</strong></a>
        
            <a href="Hbase.html"><strong>Hbase</strong></a>
        
            <a href="ELK.html"><strong>ELK</strong></a>
        
            <a href="Druid.html"><strong>Druid</strong></a>
        
            <a href="Docker.html"><strong>Docker</strong></a>
        
            <a href="DesignPattern.html"><strong>DesignPattern</strong></a>
        
            <a href="Computer.html"><strong>Computer</strong></a>
        
            <a href="Redis.html"><strong>Redis</strong></a>
        
            <a href="Zookeeper.html"><strong>Zookeeper</strong></a>
        
            <a href="BlockChain.html"><strong>BlockChain</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15423390877662.html">Pandas 中map, applymap and apply的区别</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15420931314995.html">Objective vs. Cost vs. Loss vs. Error Function</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15402913817590.html">将python生成的base64图片,在浏览器上展示出来</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15398508250689.html">Isolation Forest</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15396891344806.html">聚类算法评估指标</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?2a025c7742b90ad62b1e767e1b7f2f29";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  </body>
</html>
