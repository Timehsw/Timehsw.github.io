<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Time渐行渐远]]></title>
  <link href="http://dmlcoding.com/atom.xml" rel="self"/>
  <link href="http://dmlcoding.com/"/>
  <updated>2018-08-23T13:13:23+08:00</updated>
  <id>http://dmlcoding.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Mac 下高速免费下载百度云文件]]></title>
    <link href="http://dmlcoding.com/15343116796404.html"/>
    <updated>2018-08-15T13:41:19+08:00</updated>
    <id>http://dmlcoding.com/15343116796404.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">下载安装Aira2客户端</h1>

<p>地址: <a href="https://github.com/yangshun1029/aria2gui/releases">https://github.com/yangshun1029/aria2gui/releases</a></p>

<span id="more"></span><!-- more -->

<p><img src="media/15343116796404/15343175848538.jpg" alt="" style="width:900px;"/></p>

<h1 id="toc_1">安装Ghrome插件 BaiduExporter</h1>

<p>下载地址: <a href="https://github.com/acgotaku/BaiduExporter/archive/master.zip">https://github.com/acgotaku/BaiduExporter/archive/master.zip</a></p>

<p>这个只能这样安装<br/>
<img src="media/15343116796404/15343119219617.jpg" alt="" style="width:941px;"/></p>

<p><img src="media/15343116796404/15343119745283.jpg" alt="" style="width:1343px;"/><br/>
<img src="media/15343116796404/15343122583040.jpg" alt="" style="width:614px;"/></p>

<p><img src="media/15343116796404/15343121945795.jpg" alt="" style="width:743px;"/></p>

<p>ok,搞定了</p>

<h1 id="toc_2">总结</h1>

<p>所有的包上传百度云备份一份</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[windows10和mac共享文件]]></title>
    <link href="http://dmlcoding.com/15317970391028.html"/>
    <updated>2018-07-17T11:10:39+08:00</updated>
    <id>http://dmlcoding.com/15317970391028.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">一,windows10访问mac电脑</h1>

<h2 id="toc_1">1,先新建一个用于共享的登录账户</h2>

<p>在系统偏好设置-&gt;用户与群组-&gt;点击黄色的锁-&gt;输入当前账户密码-&gt;点击左侧➕ ,新建一个<em>仅限共享</em>的用户(注意:是将普通账户改为仅限共享,记住密码)。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_2">2,开启共享</h2>

<p>系统偏好设置 -&gt; 共享 -&gt; 点开黄色的锁输入密码 -&gt; 勾选文件共享和远程登录 -&gt; 选中共享文件 -&gt; 点击共享文件夹的➕，添加要共享的文件夹 -&gt; 点击用户➕添加新建的第1步中添加的帐号 -&gt; 点击选项勾选使用SMB -&gt; 勾选windows文件共享中的新添加的账户点击完成.</p>

<h2 id="toc_3">3,将mac加入到windows的工作组中</h2>

<p>系统偏好设置-&gt;网络-&gt;选择你所要设置的网络-右侧下方的高级按钮-wins选项卡.注意工作组要在windows上查找，wins服务器也要在windows上查找匹配。</p>

<h2 id="toc_4">4,windows中访问共享文件</h2>

<p>windows+R -&gt; 输入:&quot;\mac ip地址&quot; ,如 \192.168.1.102 -&gt; 在弹出的对话框中输入mac中添加的账户和密码就能访问了.</p>

<h1 id="toc_5">二，Mac访问windows10:</h1>

<h2 id="toc_6">1,打开共享设置</h2>

<p>1，控制面板-&gt; 网络和Internet -&gt; 网络和共享中心 -&gt; 高级共享设置 中专用和所有网络的都开启共享的设置.<br/>
2，文件资源管理器（默认选中快速访问）-&gt; 查看 -&gt; 选项 -&gt; 查看 -&gt; 勾选 使用共享向导(推荐).</p>

<h2 id="toc_7">2,在windows上新建用于mac登录的账户</h2>

<p>windows+R -&gt; 输入:netplwiz -&gt; 点击添加 -&gt; 选择不使用Microsoft帐户 -&gt; 本地账户 -&gt; 输入用户名，密码等信息（注意记住用于mac登录）.</p>

<h2 id="toc_8">3,设置要共享的文件夹</h2>

<p>1,选中文件夹右键 -&gt; 属性 -&gt; 共享 -&gt; 高级共享 -&gt; 勾选共享此文件夹 -&gt; 选中权限 进行相应的权限设置 -&gt; 点击应用，确定.<br/>
2,再次右键要共享的文件夹 -&gt; 属性 -&gt; 共享 -&gt; 共享(s) -&gt; 下拉箭头，添加之前第2部中增加的账户 -&gt; 设置权限级别.</p>

<h2 id="toc_9">4,mac中访问windows中的共享文件夹</h2>

<p>点击Finder -&gt; 左侧会有共享的设备 -&gt; 点击 连接身份 -&gt;输入第2步中创建的账户名和密码就可以访问了.</p>

<h2 id="toc_10">注意:</h2>

<p>2.3.4步就可以了</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HMM(摘自:统计学习方法)]]></title>
    <link href="http://dmlcoding.com/15299155104492.html"/>
    <updated>2018-06-25T16:31:50+08:00</updated>
    <id>http://dmlcoding.com/15299155104492.html</id>
    <content type="html"><![CDATA[
<p>HMM(隐马尔可夫模型的定义):隐马尔可夫模型是关于时序的概率模型,描述由一个隐藏的马尔可夫链随机生成不可观察的状态随机序列,再由各个状态生成一个观测而产生观测随机序列的过程.隐藏的马尔可夫链随机生成的状态的序列,称为状态序列(state sequence);每个状态生成一个观测,而由此产生的观测的随机序列,称为观测序列(observation sequence).序列的每一个位置又可以看作是一个时刻.</p>

<span id="more"></span><!-- more -->

<p><img src="media/15299155104492/15299160364725.jpg" alt=""/></p>

<h1 id="toc_0">隐马尔可夫模型的相关定义</h1>

<p>隐马尔可夫模型由初始概率分布,状态转移概率分布以及观测概率分布确定.隐马尔可夫模型的形式定义如下:<br/>
   设Q是所有可能的状态集合,V是所有可能的观测的集合,其中,N是可能的状态数,M是可能的观测数.</p>

<p>\[\begin{aligned}<br/>
   Q &amp; = {q_1,q_2,\cdots,q_N}  &amp;\text{Q是所有可能的状态集合} \\<br/>
   V &amp; = {v_1,v_2,\cdots,v_M} &amp; \text{V是所有可能的观测的集合} <br/>
\end{aligned}\]</p>

<p>I是长度为T的状态序列,O是对应的观测序列<br/>
  \[\begin{aligned}<br/>
   I &amp; = {i_1,i_2,\cdots,i_T}  &amp;\text{I是长度为T的状态序列} \\<br/>
   O &amp; = {o_1,o_2,\cdots,o_T} &amp; \text{O是对应的长度为T的观测序列} <br/>
\end{aligned}\]</p>

<p>A是状态转移概率矩阵:<br/>
\[A=[a_{ij}]_{N*N} \\\<br/>
其中,a_{ij}=P(i_{t+1}=q_j|i_t=q_i),\qquad i=1,2,\cdots,N;j=1,2,\cdots,N \\\<br/>
表示在时刻t处于状态q_i的条件下在时刻t+1转移到状态q_j的概率\]</p>

<p>B是观测概率矩阵:<br/>
\[B=[b_j(k)]_{N*M} \\\<br/>
其中,b_j(k)=P(o_t=v_k|i_t=q_j),\qquad k=1,2,\cdots,M;j=1,2,\cdots,N \\\<br/>
表示在时刻t处于状态q_j的条件下生成观测v_k的概率\]</p>

<p>\(\pi\)是初始状态概率向量<br/>
\[\pi=(\pi_i) \\\<br/>
其中,\pi_i=P(i_1=q_i),\qquad i=1,2,\cdots,N \\\<br/>
表示在时刻t=1处于状态q_i的概率\]</p>

<p>隐马尔可夫模型由初始状态概率向量\(\pi\),状态转移概率矩阵A和观测概率矩阵B决定,\(\pi\)和A决定状态序列,B决定观测序列.因此,隐马尔可夫模型\(\lambda\)可以用三元符合表示,即<br/>
\[\lambda=(\pi,A,B) \\\<br/>
A,B,\pi 称为隐马尔可夫模型的三要素\]</p>

<p>状态转移概率矩阵A与初始状态概率向量\(\pi\)确定了隐藏的马尔可夫链,生成不可观测的状态序列.观测概率矩阵B确定了如何从状态生成观测,与状态序列综合确定了如何产生观测序列.</p>

<p>从定义可知,隐马尔可夫模型作了两个基本假设:<br/>
(1)齐次马尔可夫假设,即假设隐藏的马尔可夫链在任意时刻t的状态只依赖于前一时刻的状态,与其他时刻的状态及观测无关,也与时刻t无关.<br/>
   \[P(i_t|i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(i_t|i_{t-1}),t=1,2,\cdots,T\]<br/>
(2)观测独立性假设,即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态,与其他观测及状态无关.<br/>
\[P(o_t|i_T,o_T,i_{T-1},o_{T-1},\cdots,i_{t+1},o_{t+1},i_t,o_t,i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(o_t|i_t)\]</p>

<p>观测序列的生成过程</p>

<p>根据隐马尔可夫模型定义,可以将一个长度为T的观测序列\(O=(o_1,o_2,\cdots,o_T)\)的生成过程描述如下:<br/>
输入:隐马尔可夫模型\(\lambda=(A,B,\pi)\),观测序列长度T;<br/>
输出:观测序列\(O=(o_1,o_2,\cdots,o_T)\)<br/>
(1) 按照初始状态分布\(\pi\)产生状态\(i_1\)<br/>
(2) 令t=1<br/>
(3) 按照状态\(i_t\)的观测概率分布\(b_{i_t}(k)生成o_t\)<br/>
(4) 按照状态\(i_t\)的状态转移概率分布\({a_{i_t j_{t+1}}}产生状态i_{t+1},i_{t+1}=1,2,\cdots,N\)<br/>
(5) 令t=t+1;如果t&lt;T,转步(3);否则,终止</p>

<h1 id="toc_1">前向算法</h1>

<p>首先定义前向概率:给定隐马尔可夫模型\(\lambda\),定义到时刻t部分观测序列\(o_1,o_2,\cdots,o_t\)且状态为\(q_i\)的概率为前向概率,记作<br/>
\[\alpha_t(i)=P(o_1,o_2,\cdots,o_t,i_t=q_i|\lambda)\]<br/>
可以递推地求得前向概率\( \alpha_t(i)\) 及观测序列概率 \(P(O|\lambda)\)</p>

<p><strong>观测序列概率的前向算法:</strong><br/>
输入:隐马尔可夫模型\(\lambda\),观测序列O;<br/>
输出:观测序列概率\(P(O|\lambda)\)<br/>
(1) 初值<br/>
\[\alpha_1(i)=\pi_ib_i(o_1),\qquad  i=1,2,\cdots,N\]<br/>
(2) 递推 <br/>
\[对t=1,2,\cdots,T-1, \\\<br/>
\alpha_{t+1}(i)= \bigg[\sum_{j=1}^N\alpha_t(j)\alpha_{ji} \bigg] b_i(\alpha_{t+1}),\qquad i=1,2,\cdots,N\]<br/>
(3) 终止<br/>
\[P(O|\lambda)=\sum_{i=1}^N\alpha_T(i)\]</p>

<p>前向算法,步骤(1)初始化前向概率,是初始时刻的状态\(i_1=q_i\)和观测\(o_1\)的联合概率.步骤(2)是前向概率的递推公式,计算到时刻t+1部分观测序列为\(o_1,o_2,\cdots,o_t,o_{t+1}\)且在时刻t+1处于状态\(q_i\)的前向概率,</p>

<p><img src="media/15299155104492/15299161632092.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[EM算法]]></title>
    <link href="http://dmlcoding.com/15299156181001.html"/>
    <updated>2018-06-25T16:33:38+08:00</updated>
    <id>http://dmlcoding.com/15299156181001.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[拉格朗日乘子法&KKT条件]]></title>
    <link href="http://dmlcoding.com/15289333499679.html"/>
    <updated>2018-06-14T07:42:29+08:00</updated>
    <id>http://dmlcoding.com/15289333499679.html</id>
    <content type="html"><![CDATA[
<p>如果不理解拉格朗日乘子法和KKT条件的相关原理,则不可能真正理解SVM的计算方法.</p>

<p>这是我们学习SVM的大前提</p>

<h1 id="toc_0">优化问题</h1>

<p>通常,我们要求解的函数优化问题,大致可分为以下3类</p>

<ul>
<li>无约束条件的优化问题:<br/>
\[min f(X)\]</li>
<li>只有等式约束的优化问题:<br/>
\[min f(X) \\\<br/>
s.t: h_i(X)=0,i \in {1,2,...,n}\]</li>
<li><p>含有不等式约束的优化问题:<br/>
\[min f(X) \\\<br/>
s.t: h_i(X)=0,i \in {1,2,...,n} \\\<br/>
     g_j(X) \leq 0, j \in {1,2,...,n} \]</p>
<span id="more"></span><!-- more -->        </li>
</ul>

<p>其中大写的X表示所有自变量的集合(也可以说是所有样本点的集合),\(h_i(X)\)表示等式约束条件,\(g_i(X)\)表示不等式约束条件.这里用\(min f(X)\)表示求取最优值的目标函数,其实不一定是求取最小值,求最大值也是可以的(取相反数即可).但是一般最优化都是取小值.</p>

<p>注意:默认所有解决的最优问题都是凸优化问题(即求取凸函数的最优解).拉格朗日乘子法一定是适用于凸优化问题,而不一定适用于其他非凸问题.</p>

<p>回到最上面的三类优化问题,解决方法是明确的,参考如下(注意:默认都是针对凸优化问题):</p>

<ul>
<li>对于无约束条件的优化问题,直接对其各个自变量求导,令导数为0,得全局最优解.</li>
<li>对于只有等式约束的优化问题,利用拉格朗日乘子法,得到全局最优解.</li>
<li>对于含不等式约束的优化问题,利用KKT条件,得全局最优解.</li>
</ul>

<h1 id="toc_1">拉格朗日乘子法</h1>

<p>举个例子来描述拉格朗日乘子法的基本思路:<br/>
\[min f(X)=2x_1^2+3x_2^2+7x_3^2 \\\<br/>
s.t : 2x_1+x_2=1,\quad 2x_2+3x_3=2\]</p>

<p>这个例子正式只有等式约束条件的优化问题,求解思路如下:<br/>
首先想到直接对3个自变量分别求偏导,令其偏导数都为0,则此时\(x_1,x_2,x_3\)都是0,看函数也是这样,当3个自变量都是0时,函数取得最小值0.但是这显然不符合约束条件的.</p>

<p>怎么把约束条件考虑进去呢?用拉格朗日乘子法,把改写后的约束条件乘以一个系数,然后以加的形式带入目标函数,该函数称之为拉格朗日函数.先看看改写后的约束条件:<br/>
\[s.t : 2x_1+x_2-1=0,\quad 2x_2+3x_3-2=0\]<br/>
左右移项,实际上没发生任何变化.其后,将改写后的约束条件带入目标函数,构造拉格朗日函数:<br/>
\[L(X,\alpha)=2x_1^2+3x_2^2+7x_3^2+\alpha_1(2x_1+x_2-1)+\alpha_2(2x_2+3x_3-2)\]</p>

<p>可见,当求得的最优解满足约束条件时,\(L(x,\alpha)\)与原始的目标函数并没有差别(后面都是0).这样,我们再对这个拉格朗日函数求关于各个自变量的偏导,并令这些偏导数为0.<br/>
\[<br/>
\begin{aligned}<br/>
\dfrac{\partial f(X)}{\partial x_1} &amp; =4x_1+2\alpha_1=0 \implies  x_1=-\frac12\alpha_1 \\<br/>
 \dfrac{\partial f(X)}{\partial x_2} &amp; =6x_2+\alpha_1+2\alpha_2=0 \implies  x_2=-\frac16\alpha_1 - \frac13\alpha_2 \\ <br/>
\dfrac{\partial f(X)}{\partial x_3} &amp; =14x_3+3\alpha_2=0 \implies  x_3=-\frac{3}{14}\alpha_2<br/>
\end{aligned}\]</p>

<p>现在,将用系数\(\alpha_1,\alpha_2\)表示的3个自变量带入约束条件,可得如下方程:<br/>
\[<br/>
\begin{aligned}<br/>
-\dfrac{7}{6}\alpha1 - \dfrac{1}{3}\alpha_2 -1 &amp; =0 \\<br/>
-\dfrac{1}{3}\alpha1 - \dfrac{55}{44}\alpha_2 -2 &amp; =0 <br/>
\end{aligned}\]</p>

<p>两个未知数,两个方程,可以求解得到\(\alpha_1=-0.45,\alpha_2=-1.41\),再带入自变量的表达式,即求得最优解.</p>

<h1 id="toc_2">拉格朗日乘子法的形式化描述</h1>

<p>用拉格朗日乘子法解决只有等式约束条件的优化问题,可以被如下形式化的描述:<br/>
现有优化问题:<br/>
    \[min f(X) \\<br/>
    s.t: h_i(X)=0,i \in {1,2,...,n}\]<br/>
则需要先得到拉格朗日函数\(L(X,\alpha)=f(X)+\sum_{i=1}^n \alpha_i \cdot h_i(X)\),对该函数的各个自变量求偏导,令偏导数为0,则可以求出各个自变量的含系数\(\alpha\)的代数式,再带入约束条件,解得\(\alpha\)后,可最终得到最优解.</p>

<h1 id="toc_3">KKT条件</h1>

<p>上面说的拉格朗日乘子法,解决的是只有等式约束条件的优化问题,而现实中更普遍的情况是求解含有不等式约束条件的问题.</p>

<p>还是通过一个例子讲解,优化问题如下:<br/>
\[min f(X)=x_1^2-2x_1+1+x_2^2+4x_2+4 \\\<br/>
s.t : x_1+10x_2 &gt; 10 \\\<br/>
10x_1-x_2 &lt; 10\]</p>

<p>第一步:把约束条件改写如下:<br/>
\[s.t : 10-x_1-10x_2&lt;0 \\\<br/>
10x_1-x_2-10&lt;0\]</p>

<p>改写的目的是方便后面的计算,且这种改写没有改变约束条件本身,所以不影响.</p>

<p>第二步:与拉格朗日乘子法相同,给约束条件乘以系数后加入目标函数,构成拉格朗日函数\(L(X,\beta)\)<br/>
\[L(X,\beta)=x_1^2-2x_1+1+x_2^2+4x_2+4+\beta_1(10-x_1-10x_2)+\beta_2(10x_1-x_2-10)\]</p>

<p>第三步:对拉格朗日函数的各个自变量求偏导:<br/>
\[<br/>
\begin{aligned}<br/>
\dfrac{\partial f(X)}{\partial x_1} &amp; =2x_1-\beta_1+10\beta_2 -2 \\<br/>
 \dfrac{\partial f(X)}{\partial x_2} &amp; =2x_2-10\beta_1-10\beta_2+4<br/>
\end{aligned}\]</p>

<p>做完以上3步,先不往下进行了.先给出KKT条件的形式化描述如下.</p>

<h1 id="toc_4">KKT条件的形式化描述</h1>

<p>KKT条件用于解决带有不等式约束条件的优化问题,可以被如下形式化描述:<br/>
现有优化问题:<br/>
\[min f(X) \\\<br/>
    s.t: h_i(X)=0,i \in {1,2,...,n} \\\<br/>
         g_j(X) \leq 0, j \in {1,2,...,n} \]<br/>
根据这个优化问题可以得到拉格朗日函数:<br/>
\[L(X,\alpha,\beta)=f(X)+\sum_{i=1}^m\alpha_i \cdot h_i(X)+\sum_{j=1}^m\beta_j \cdot g_j(X)\]</p>

<p>那么KKT条件就是函数的最优值必定满足下面3个条件(这3个条件是重点中的重点)</p>

<ul>
<li>\(L(X,\alpha,\beta)\)对各个自变量求导为0</li>
<li>h(X)=0</li>
<li>\(\sum_{i=1}^m\beta_j \cdot g_j(X)=0,\beta_j \geq 0\)</li>
</ul>

<p>3个条件中,前两个很好理解,不说了.关键在于第3个条件,这也是这篇文章中最难理解的部分.我尝试解释一下这个条件的原理.</p>

<p>首先,我们已知\(\beta_j \geq0,而g_j(X)\leq 0\)(这样设计的目的是要求目标函数的梯度和约束条件的梯度必须反向).也就是说,想要条件3成立,则每一项\(\beta_j \cdot g_j(X)都等于0\),再换个说法,对于每一项来说,要么\(\beta_j=0\),要么\(g_j(X)=0\).这就是条件3所要表达的真实含义.</p>

<p>下面解释为什么这样的条件可以用来寻找最优解.</p>

<p>假设X由2个自变量\(x_1,x_2\)组成,那么我们很容易想象出目标函数f(X)的图像,它就是一个三维空间中的曲面;再假设此时优化问题有3个不等式约束条件\(g_1(X),g_2(X),g_3(X)\).那现在可以把优化问题的图像画出来,如下图所示.画的是该问题再\(x_1 x_2\)平面上的投影,虚线为f(X)的等高线,那现在就有左图和右图两种不同情况了.<br/>
<img src="media/15289333499679/15289408757643.jpg" alt=""/></p>

<p>左图表示的是min f(X)在不等式的约束范围内,且不在不等式的约束面上.这时,f(X)的最优解与不等式的约束实际上没关系了,我们就令\(\beta_1=\beta_2=\beta3=0\)成立,而且此时,\(L(X,\alpha,\beta)\)对各个自变量求导为0的解就是最优解.换句话说,这种情况需要条件1,3同时成立.</p>

<h1 id="toc_5">参考</h1>

<p><a href="https://blog.csdn.net/on2way/article/details/47729419">https://blog.csdn.net/on2way/article/details/47729419</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SVM(支持向量机)]]></title>
    <link href="http://dmlcoding.com/15285262100303.html"/>
    <updated>2018-06-09T14:36:50+08:00</updated>
    <id>http://dmlcoding.com/15285262100303.html</id>
    <content type="html"><![CDATA[
<p>支持向量机(Support Vecor Machine,SVM)本身是一个<strong>二元分类算法</strong>,是对感知器算法模型的一种扩展,现在的SVM算法支持线性分类和非线性分类的分类应用,并且也能够直接将SVM应用于回归应用中.同时通过OvR或者OvO的方式我们也可以将SVM应用在多元分类领域中.在不考虑集成算法,不考虑特定的数据集的时候,在分类算法中SVM可以说是特别优秀的.</p>

<p><strong>线性可分(Linearly Separable)</strong>:在数据集中,如果可以找出一个超平面,将两组数据分开,那么这个数据集叫做线性可分数据.</p>

<p><strong>线性不可分(Linear Inseparable)</strong>:在数据集中,没法找出一个超平面,能够将两组数据分开,那么这个数据集就叫做线性不可分数据.</p>

<p><strong>分割超平面(Separating Hyperplane)</strong>:将数据集分割开来的直线/平面叫做分割超平面.</p>

<p><strong>间隔(Margin)</strong>:数据点到分割超平面的距离称为间隔.</p>

<p><strong>支持向量(Support Vector)</strong>:离分割超平面最近的那些点叫做支持向量.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">线性可分SVM</h1>

<p>在感知器模型中,算法是在数据中找出一个划分超平面,让尽可能多的数据分布在这个平面的两侧,从而达到分类的效果,但是在实际数据中这个符合我们要求的超平面是可能存在多个的.<br/>
<img src="media/15285262100303/15285276077823.jpg" alt=""/></p>

<p>在感知器模型中,我们可以找到多个可以分类的超平面将数据分开,并且优化时希望所有的点都离超平面尽可能的远,但是实际上离超平面足够远的点基本上都是被正确分类的,所以这个是没有意义的;反而比较关心那些离超平面很近的点,这些点比较容易分错.所以说我们只要让离超平面比较近的点尽可能的远离这个超平面,那么我们的模型分类效果应该就会比较不错了.(这个就是SVM的思想)<br/>
<img src="media/15285262100303/15285279858682.jpg" alt=""/></p>

<h2 id="toc_1">超平面和法向量</h2>

<p>先回顾一下同济高数上的一个例题,求解点到平面的距离.<br/>
<img src="media/15285262100303/15285475518085.jpg" alt=""/></p>

<p>常见的平面概念是在三维空间中定义的:\(Ax+By+Cz+D=0\).<br/>
而d维空间中的超平面由下面的方程确定:\(w^Tx+b=0\),其中w和x都是d维列向量,\(x=(x_1,x_2,...,x_d)^T\)为平面上的点,\(w=(w_1,w_2,...,w_d)^T\)为平面的法向量.b是一个实数,代表平面与原点之间的距离.</p>

<p><strong>点到超平面的距离</strong></p>

<p>假设点\(x^{&#39;}为超平面A:w^Tx+b=0上的任意一点,则点x到A的距离为x-x^{&#39;}在超平面法向量w上的投影长度:\)</p>

<p>\[d=\dfrac{|w^T(x-x^{&#39;})|}{||w||}=\dfrac{|w^Tx+b|}{||w||} \qquad 注意:w^Tx^{&#39;}=-b,x^{&#39;}为超平面上一点\]</p>

<p>这里,\(||w||是w的L_2范数\)</p>

<p><strong>超平面的正面和反面</strong><br/>
一个超平面可以将它所在的空间分为两半,它的法向量指向的那一半对应的一面是它的正面,另一面则是它的反面.</p>

<p><strong>法向量的意义</strong><br/>
法向量的大小是坐标原点到分离超平面的距离,垂直于分离超平面,方向由分离超平面决定.</p>

<h2 id="toc_2">线性可分SVM</h2>

<p>支持向量到超平面的距离为:<br/>
\[\because w^Tx+b=\pm1 \\\<br/>
\because y \in \{+1,-1\} \\\<br/>
\therefore \dfrac{|y(w^Tx+b)|}{||w||}=\dfrac{1}{||w||}\]</p>

<p>备注:在SVM中支持向量到超平面的函数距离一般设置为1<br/>
<img src="media/15285262100303/15285491659569.jpg" alt=""/></p>

<p>SVM模型是让所有的分类点在各自类别的支持向量的两边,同时要求支持向量尽可能的远离这个超平面,用数学公式表示如下:</p>

<p>注意:<br/>
\[<br/>
\begin{cases}<br/>
w^T=(w_1,w_2,\cdots,w_n)\\\ <br/>
||w||=\sqrt{w_1^2+w_2^2+\cdots+w_n^2}<br/>
\end{cases}<br/>
\]</p>

<p>\[<br/>
\begin{cases}<br/>
\max \limits_{w,b}\dfrac{1}{||w||} &amp;\text {目标}\\\ <br/>
s.t:y^{(i)}\left(w^Tx^{(i)}+b\right) \geq 1,i=1,2,...,m &amp;\text {限制条件}<br/>
\end{cases}<br/>
\]</p>

<p>一般求最优化问题,都是求最小值,因此稍微将这个优化问题转换一下<br/>
\[<br/>
\begin{cases}<br/>
\min \limits_{w,b}\dfrac12||w||^2 &amp;\text {目标}\\\ <br/>
s.t:y^{(i)}\left(w^Tx^{(i)}+b\right) \geq 1,i=1,2,...,m &amp;\text {限制条件}<br/>
\end{cases}<br/>
\]</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[聚类算法]]></title>
    <link href="http://dmlcoding.com/15284222434956.html"/>
    <updated>2018-06-08T09:44:03+08:00</updated>
    <id>http://dmlcoding.com/15284222434956.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->

<h1 id="toc_0">什么是聚类算法</h1>

<h1 id="toc_1">聚类算法的度量</h1>

<h1 id="toc_2">聚类的思想</h1>

<h1 id="toc_3">K-Means算法</h1>

<h1 id="toc_4">K-Means++算法</h1>

<h1 id="toc_5">K-Means||算法</h1>

<h1 id="toc_6">Canopy算法</h1>

<h1 id="toc_7">Mini Batch K-Means算法</h1>

<h1 id="toc_8">聚类算法的衡量指标</h1>

<h1 id="toc_9">层次聚类方法</h1>

<p>层次聚类方法对给定的数据集进行层次的分解,直到满足某种条件为止,传统的层次聚类算法主要分为两大类算法:<br/>
凝聚的层次聚类(AGNES算法:AGglomerative NESting):采用自底向上的策略.最初将每个对象作为一个簇,然后这些簇根据某些准则被一步一步合并,两个簇间的距离可以由这两个不同簇中距离最近的数据点的相似度来确定;聚类的合并过程反复进行直到所有的对象满足簇数目.</p>

<p>分裂的层次聚类(DIANA算法:DIvisive ANALysis):采用自顶向下的策略.首先将所有对象置于一个簇中,然后按照某种既定的规则逐渐细分为越来越小的簇(比如最大的欧式距离),直到达到某个终结条件(簇数目或者簇距离达到阈值).</p>

<p>AGNES和DIANA算法优缺点:</p>

<ul>
<li>简单,理解容易</li>
<li>合并点/分裂点选择不太容易</li>
<li>合并/分类的操作不能进行撤销</li>
<li>大数据集不太适合</li>
<li>执行效率较低</li>
</ul>

<p>AGNES算法中簇间距离</p>

<ul>
<li>最小距离(SL聚类)
<ul>
<li>两个聚簇中最近的两个样本之间的距离(single/word-linkage聚类法)</li>
<li>最终得到模型容易形成链式结构</li>
</ul></li>
<li>最大距离(CL聚类)
<ul>
<li>两个聚簇中最远的两个样本的距离(complete-linkage聚类法)</li>
<li>如果存在异常值,那么构建可能不太稳定</li>
</ul></li>
<li>平均距离(AL聚类)
<ul>
<li>两个聚类中样本间两两距离的平均值(average-linkage聚类法)</li>
<li>两个聚簇中样本间两两距离的中值(median-linkage聚类法)</li>
</ul></li>
</ul>

<h1 id="toc_10">密度聚类方法</h1>

<h1 id="toc_11">谱聚类</h1>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[集成学习]]></title>
    <link href="http://dmlcoding.com/15255701784620.html"/>
    <updated>2018-05-06T09:29:38+08:00</updated>
    <id>http://dmlcoding.com/15255701784620.html</id>
    <content type="html"><![CDATA[
<p>集成学习的思想是将若干个学习器(分类器&amp;回归器)组合之后产生一个新学习器.弱分类器(weak learner)指那些分类准确率只稍微好于随机猜测的分类器(error rate&lt; 0.5).</p>

<p>集成算法的成功在于保证弱分类器的多样性(Diversity).而且集成不稳定的算法也能够得到一个比较明显的性能提升.</p>

<p>常见的集成学习思想有:</p>

<ul>
<li>Bagging</li>
<li>Boosting</li>
<li>Stacking</li>
</ul>

<span id="more"></span><!-- more -->

<h2 id="toc_0">为什么需要集成学习</h2>

<ol>
<li>弱分类器间存在一定的差异性,这会导致分类的边界不同,也就是说可能存在错误.那么将多个弱分类器合并后,就可以得到更加合理的边界,减少整体的错误率,实现更好的效果.</li>
<li>对于数据集过大或者过小,可以分别进行划分和有放回的操作产生不同的数据子集,然后使用数据子集训练不同的分类器,最终再合并成为一个大的分类器.</li>
<li>如果数据的划分边界过于复杂,使用线性模型很难描述情况,那么可以训练多个模型,然后再进行模型的融合.</li>
<li>对于多个异构的特征集的时候,很难进行融合,那么可以考虑每个数据集构建一个分类模型,然后将多个模型融合.</li>
</ol>

<h1 id="toc_1">Bagging方法</h1>

<p>Bagging方法又叫自举汇聚法(Bootstrap Aggregating),思想是:在原始数据集上通过有放回的抽样的方式,重新选择出S个新数据集来分别训练S个分类器的集成技术.也就是说这些模型的训练数据中允许存在重复数据.</p>

<p>Bagging方法训练出来的模型在预测新样本分类的时候,会使用<strong>多数投票</strong>或者求<strong>均值</strong>的方式来统计最终的分类结果.</p>

<p>Bagging方法的弱学习器可以是基本的算法模型,eg:Linear,Ridge,Lasso,Logistic,Softmax,ID3,C4.5,CART,SVM,KNN等</p>

<p>备注:Bagging方式是有放回的抽样,并且每个子集的样本数量必须和原始样本数量一致,但是子集中允许存在重复数据.</p>

<p><img src="media/15255701784620/15259417755837.jpg" alt="Bagging方法-训练过程"/></p>

<h2 id="toc_2">随机森林(Random Forest)</h2>

<p>在Bagging策略的基础上进行修改后的一种算法</p>

<ol>
<li>从原始样本集(n个样本)中用Bootstrap采样(有放回重采样)选出n个样本;</li>
<li>从所有属性中随机选择k个属性,选择出最佳分割属性作为节点创建决策树;</li>
<li>重复以上两步m次,即建立m棵决策树;</li>
<li>这m个决策树形成随机森林,通过投票表决结果决定数据属于哪一类.</li>
</ol>

<h2 id="toc_3">随机森林的推广算法</h2>

<p>RF(随机森林)算法在实际应用中具有比较好的特性,应用也比较广泛,主要应用在:分类,回归,特征转换,异常点检测.常见的RF变种算法如下:</p>

<ul>
<li>Extra Tree</li>
<li>Totally Random Trees Embedding(TRTE)</li>
<li>lsolation Forest</li>
</ul>

<h2 id="toc_4">Extra Tree</h2>

<p>Extra Tree是RF的一个变种,原理基本和RF一样,区别如下:</p>

<ol>
<li>RF会随机采样作为子决策树的训练集,而Extra Tree每个子决策树采样原始数据集训练;</li>
<li>RF在选择划分特征点的时候会和传统决策树一样,会基于信息增益,信息增益率,基尼系数,均方差等原则来选择最优特征值;而Extra Tree会随机的选择一个特征值来划分决策树,</li>
</ol>

<p>Extra Tree因为是随机选择特征值的划分点,这样会导致决策树的规模一般大于RF所生成的决策树.也就是说Extra Tree模型的方差相对于RF进一步减少.在某些情况下,Extra Tree的泛华能力比RF的强.</p>

<h2 id="toc_5">Totally Random Trees Embedding(TRTE)</h2>

<p>TRTE是一种非监督的数据转化方式.将低维的数据集映射到高维,从而让映射到高维的数据更好的应用于分类回归模型.</p>

<p>TRTE算法的转换过程类似RF算法的方法,建立T个决策树来拟合数据.当决策树构建完成后,数据集里的每个数据在T个决策树叶子节点的位置就定下来了,将位置信息转换为向量就完成了特征转换操作.</p>

<h2 id="toc_6">Isolation Forest(IForest)</h2>

<p>IForest是一种异常点检测算法,使用类似RF的方式来检测异常点;IForest算法和RF算法的区别在于:</p>

<ol>
<li>在随机采样的过程中,一般只需要少量数据即可;</li>
<li>在进行决策树构建过程中,IForest算法会随机选择一个划分特征,并对划分特征随机选择一个划分阈值.</li>
<li>IForest算法构建的决策树一般深度max_depth是比较小的</li>
</ol>

<p>区别原因:目的是异常点检测,所以只要能够区分异常的即可,不需要大量数据;另外在异常点检测的过程中,一般不需要太大规模的决策树.</p>

<p>对于异常点的判断,则是将测试样本x拟合到T棵决策树上.计算在每棵树上该样本的叶子节点的深度\(h_t(x)\).从而计算出平均深度\(h(x)\);然后就可以使用下列公式计算样本点x的异常概率值,p(s,m)的取值范围为[0,1],越接近于1,则是异常点的概率越大.</p>

<p>\[p(x,m)=2^{-\dfrac{h(x)}{c(m)}}\]<br/>
\[c(m)=2\ln(m-1)+\xi-2\dfrac{m-1}{m} : m为样本个数,\xi为欧拉常数\]</p>

<h2 id="toc_7">RF随机森林总结</h2>

<ul>
<li><p>RF的主要优点</p>
<ul>
<li>1. 训练可以并行化,对于大规模样本的训练具有速度的优势;</li>
<li>2. 由于进行随机选择决策树划分特征列表,这样在样本维度比较高的时候,仍然具有比较高的训练性能;</li>
<li>3. 可以给出各个特征的重要性列表;</li>
<li>4. 由于存在随机抽样,训练出来的模型方差小,泛化能力强;</li>
<li>5. RF实现简单;</li>
<li>6. 对于部分特征的缺失不敏感.</li>
</ul></li>
<li><p>RF的主要缺点</p>
<ul>
<li>1. 在某些噪音比较大的特征上,RF模型容易陷入过拟合;</li>
<li>2. 取值比较多的划分特征对RF的决策会产生更大的影响,从而有可能影响模型的效果.</li>
</ul></li>
</ul>

<h2 id="toc_8">随机森林算法案例</h2>

<pre class="line-numbers"><code class="language-text">```
```
</code></pre>

<h2 id="toc_9">RF scikit-learn相关参数</h2>

<p><img src="media/15255701784620/15260264540993.jpg" alt=""/></p>

<h2 id="toc_10">随机森林的思考</h2>

<p>在随机森林的构建过程中,由于各棵树之间是没有关系的,相对独立的;在构建的过程中,构建第m棵子树的时候,不会考虑前面的m-1棵树.</p>

<p>思考:</p>

<ol>
<li>如果在构建第m棵子树的时候,考虑到前m-1棵子树的结果,会不会对最终结果产生有益的影响?</li>
<li>各个决策树组成随机森林后,在形成最终结果的时候能不能给定一种既定的决策顺序呢?</li>
</ol>

<h1 id="toc_11">Boosting</h1>

<p>提升学习(Boosting)是一种机器学习技术,可以用于回归和分类的问题,它每一步产生弱预测模型(如决策树),并加权累加到总模型中;如果每一步的弱项模型的生成都是依据损失函数的梯度方式的,那么就称为梯度提升(Gradient boosting).</p>

<p>提升技术的意义:如果一个问题存在弱预测模型,那么可以通过提升技术的方法得到一个强预测模型.</p>

<p>常见的模型有:</p>

<ul>
<li>Adaboost</li>
<li>Gradient Boosting(GBT/GBDT/GBRT)</li>
</ul>

<p><img src="media/15255701784620/15260302457703.jpg" alt=""/></p>

<h2 id="toc_12">Adaboost算法公式</h2>

<p>Adaboost算法将基分类器的线性组合作为强分类器,同时给分类误差率较小的基本分类器以大的权重,给分类误差率较大的基分类器以小的权重值;构建的线性组合为:<br/>
\[f(x)=\sum_{m=1}^M\alpha_mG_m(x)\]</p>

<p>最终分类器是在线性组合的基础上进行Sign函数转换:<br/>
\[G(x)=sign(f(x))=sign \left[\sum_{m=1}^M\alpha_mG_m(x) \right]\]</p>

<p>Sign函数<br/>
<img src="media/15255701784620/15260310974094.jpg" alt="Sign函数"/></p>

<h2 id="toc_13">AdaBoost算法原理</h2>

<p>Adaptive Boosting是一种迭代算法.每轮迭代中会在训练集上产生一个新的学习器,然后使用该学习器对所有样本进行预测,以评估每个样本的重要性(Informative).换句话来讲就是,算法会为每个样本赋予一个权重,每次用训练好的学习器标注/预测各个样本,如果某个样本点被预测的越正确,则将其权重降低;否则提高样本的权重.权重越高的样本在下一个迭代训练中所占的比重就越大,也就是说越难区分的样本在训练过程中会变得越重要.</p>

<p>整个迭代过程直到错误率足够小或者达到一定的迭代次数为止.</p>

<p>样本加权</p>

<p><img src="media/15255701784620/15260307178045.jpg" alt="样本加权"/></p>

<p>最终的强学习器<br/>
\[G(x)=sign(f(x))=sign \left[\sum_{m=1}^M\alpha_mG_m(x) \right]\]</p>

<p>损失函数<br/>
\[loss=\dfrac{1}{n}\sum_{i=1}^nI(G(x_i)\neq y_i)\]</p>

<p>损失函数<br/>
\[loss=\dfrac{1}{n}\sum_{i=1}^nI(G(x_i)\neq y_i) \leq \dfrac{1}{n}\sum_{i=1}^ne^{(-y_if(x))}\]</p>

<p>第k-1轮的强学习器<br/>
\[f_{k-1}(x)=\sum_{j=1}^{k-1}\alpha_jG_j(x)\]</p>

<p>第k轮的强学习器<br/>
\[f_{k}(x)=\sum_{j=1}^{k}\alpha_jG_j(x) \qquad<br/>
f_k(x)=f_{k-1}(x)+\alpha_kG_k(x)<br/>
\]</p>

<p>损失函数<br/>
\[loss(\alpha_m,G_m(x))=\dfrac{1}{n}\sum_{i=1}^ne^{(-y_i(f_{n-1}(x)+\alpha_mG_m(x)))}\]</p>

<p>未完待续</p>

<h2 id="toc_14">...</h2>

<h2 id="toc_15">AdaBoost总结</h2>

<ul>
<li><p>daBoost的优点如下:</p>
<ul>
<li>可以处理连续值和离散值;</li>
<li>模型的鲁棒性比较强;</li>
<li>解释强,结构简单.</li>
</ul></li>
<li><p>AdaBoost的缺点如下:</p>
<ul>
<li>对异常样本敏感,异常样本可能会在迭代过程中获得较高的权重值,最终影响模型效果.</li>
</ul></li>
</ul>

<h1 id="toc_16">Stacking</h1>

<p>Stacking是指训练一个模型用于组合(combine)其他模型(基模型/基学习器)的技术.即首先训练出多个不同的模型,然后再以之前训练的各个模型的输出作为输入来新训练一个新的模型,从而得到一个最终的模型.一般情况下使用单层的Logistic回归作为组合模型<br/>
<img src="media/15255701784620/15260322012383.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[决策树]]></title>
    <link href="http://dmlcoding.com/15255685118456.html"/>
    <updated>2018-05-06T09:01:51+08:00</updated>
    <id>http://dmlcoding.com/15255685118456.html</id>
    <content type="html"><![CDATA[
<p>在决策树算法的学习过程中,信息增益是特征选择的一个重要指标.它定义为一个特征能够为分类系统带来多少信息,带来的信息越多,说明该特征越重要,相应的信息增益也就越大<br/>
<img src="media/15255685118456/15256176444642.jpg" alt=""/></p>

<h1 id="toc_0">信息熵(Entropy)</h1>

<p>\[H(X)=-\sum_{i=1}^mp_i\log_2(p_i)\]</p>

<span id="more"></span><!-- more -->

<ul>
<li>信息量:指的是一个样本/事件所蕴含的信息,如果一个事件的概率越大,那么就可以认为该事件所蕴含的信息越少.比如:总所周知的&quot;太阳从东方升起&quot;.因为是确定事件,所以不携带任何信息量.</li>
<li>信息熵:1948年,香农引入信息熵;<strong>一个系统越是有序,信息熵就越低;一个系统越是混乱,信息熵就越高.</strong> 所以,信息熵被认为是一个系统有序程度的度量</li>
<li>信息熵就是用来描述系统信息量的不确定度(复杂度)的.</li>
</ul>

<p>High Entropy(高信息熵):表示随机变量X是均匀分布的,各种取值情况是等概率出现的.<br/>
Low Entropy(低信息熵):表示随机变量X各种取值不是等概率出现.可能出现有的事件概率很大,有的事件概率很小.</p>

<h2 id="toc_1">条件熵H(Y|X)</h2>

<blockquote>
<p>给定条件X的情况下,随机变量Y的信息熵就叫做条件熵</p>
</blockquote>

<p>在计算条件熵的情况下,先计算一个小例子.看下面这张图<br/>
<img src="media/15255685118456/15256671505083.jpg" alt=""/></p>

<p>我们可以得出<br/>
P(X=数学)=4/8=0.5<br/>
P(Y=M)=4/8=0.5<br/>
P(X=数学,Y=F)=2/8=0.25<br/>
p(Y=M|X=英语)=0(由图可知,英语专业的都是女生)</p>

<p>根据这个公式,计算H(X)和H(Y)<br/>
\[H(X)=-\sum_{i=1}^mp_i\log_2(p_i)\]<br/>
因为:<br/>
P(X=数学)=0.5<br/>
P(X=英语)=0.25<br/>
P(X=IT)=0.25<br/>
所以:<br/>
\[H(X)=-0.5log_2{0.5}-0.25log_2{0.25}-0.25log_2{0.25}=1.5\]</p>

<p>因为:<br/>
P(Y=M)=4/8=0.5<br/>
P(Y=F)=4/8=0.5<br/>
\[H(Y)=-0.5log_2{0.5}-0.5log_2{0.5}=1\]</p>

<p>比如:当专业(X)为数学的时候,Y的信息熵的值为:H(Y|X=数学)</p>

<p><img src="media/15255685118456/15256671787875.jpg" alt=""/></p>

<p>摘出专业都是数学的这部分,性别符合均匀分布.因此条件熵为1<br/>
\[H(Y|X=数学)=1\]</p>

<p>给定条件X的情况下,所有不同X值情况下Y的信息熵的平均值叫做条件熵.<br/>
\[H(Y|X)=\sum_{j=1}P(X=v_j)H(Y|X=v_j)\]</p>

<p><img src="media/15255685118456/15256888532803.jpg" alt=""/></p>

<p>\[H(Y|X)=0.5*1+0.25*0+0.25*0=0.5\]</p>

<p>给定条件X的情况下,所有不同X值情况下Y的信息熵的平均值叫做条件熵.另外一个公司如下所示:<br/>
\[H(Y|X)=H(X,Y)-H(X)\]<br/>
事件(X,Y)发生所包含的熵,减去事件X单独发生的熵,即为在事件X发生的前提下,Y发生&quot;新&quot;带来的熵,这个也就是条件熵本身的概念.</p>

<p>参考:<a href="https://blog.csdn.net/pipisorry/article/details/51695283">https://blog.csdn.net/pipisorry/article/details/51695283</a></p>

<h2 id="toc_2">条件熵H(Y|X)公式推导</h2>

<p>\[<br/>
\begin{aligned}<br/>
H(Y|X) &amp; = \sum_{j=1}P(X=v_j)H(Y|X=v_j) \\\<br/>
&amp; = \sum_xP(X)H(Y|X) \\\<br/>
&amp; = \sum_xp(x)\left(-\sum_yp(y|x)\log(p(y|x))\right) \\\<br/>
&amp; = -\sum_x\sum_yp(x,y)log\left(\dfrac{p(x,y)}{p(x)}\right) \\\<br/>
&amp; = -\sum_x\sum_yp(x,y)log(p(x,y))-\left [-\sum_x \left (\sum_yp(x,y) \right )\log(p(x)) \right ] \\\<br/>
&amp; = H(X,Y)- \left [-\sum_xp(x)\log(p(x)) \right ] \\\<br/>
&amp; = H(X,Y)-H(X)<br/>
\end{aligned} <br/>
\]</p>

<h2 id="toc_3">信息增益</h2>

<p>信息增益恰好是:信息熵-条件熵</p>

<p>也就是说,信息增益代表了在一个条件下,信息复杂度(不确定性)减少的程度.</p>

<p>那么我们现在也很好理解了,在决策树算法中,我们的关键就是每次选择一个特征,特征有多个,那么到底按照什么标准来选择哪一个特征.</p>

<p>这个问题就可以用信息增益率来度量.如果选择一个特征后,信息增益率最大(信息不确定性减少的程度最大),那么我们就选取这个特征</p>

<h1 id="toc_4">决策树(Decision Tree)</h1>

<h2 id="toc_5">什么是决策树</h2>

<p>决策树是在已知各种情况发生概率的基础上,通过构建决策树来进行分析的一种方式,是一种直观应用概率分析的一种图解法;决策树是一种预测模型,代表的是对象属性与对象值之间的映射关系;决策树是一种树形结构,其中每个内部节点表示一个属性的测试,每个分支表示一个测试输出,每个叶节点代表一种类别;决策树是一种非常常用的有监督的分类算法.</p>

<p>决策树的决策过程就是从根节点开始,测试待分类项中对应的特征属性,并按照其值选择输出分支,直到叶子节点,将叶子节点的存放的类别作为决策结果.</p>

<p>决策树分为两大类:分类树和回归树,前者用于分类标签值,后者用于预测连续值,常用算法有ID3,C4.5,CART等</p>

<h2 id="toc_6">决策树的构建过程</h2>

<p>决策树算法的重点是决策树的构造;决策树的构造就是进行属性选择度量,确定各个特征属性之间的拓扑结构(树结构);构建决策树的关键步骤就是分裂属性,分裂属性是指在某个节点按照某一个类特征属性的不同划分构建不同的分支,其目标就是让各个分裂子集尽可能的&quot;纯&quot;(让一个分类子类中待分类的项尽可能的属于同一个类别).</p>

<p>构建步骤如下:</p>

<ol>
<li>将所有的特征看成一个一个的节点;</li>
<li>遍历每个特征的每一种分割方式,找到最好的分割点;将数据划分为不同的子节点.eg:\(N_1,N_2...N_m\);计算划分之后所有子节点的&quot;纯度&quot;信息;</li>
<li>对第二步产生的分割,选择出最优的特征以及最优的划分方式;得出最终的子节点:\(N_1,N_2...N_m\);</li>
<li>对子节点\(N_1,N_2...N_m\)分别继续执行2-3步,直到每个最终的子节点都足够&quot;纯&quot;.</li>
</ol>

<h2 id="toc_7">决策树特征属性类型</h2>

<p>属性类型当然可以是离散型和连续型.</p>

<p>根据特征属性的类型不同,在构建决策树的时候,采用不同的方式,具体如下:</p>

<ol>
<li>属性是离散值,而且不要求生成的是二叉决策树,此时一个属性就是一个分支.</li>
<li>属性是离散值,而且要求生成的是二叉决策树,此时使用属性划分的子集进行测试,按照&quot;属于此子集&quot;和&quot;不属于此子集&quot;分成两个分支.</li>
<li>属性是连续值,可以确定一个值作为分裂点split_point,按照&gt;split_point和&lt;=split_point生成两个分支.</li>
</ol>

<h2 id="toc_8">决策树分割属性选择</h2>

<p>决策树算法是一种&quot;贪心&quot;算法策略,只考虑在当前数据特征情况下的<strong>最好分割方式</strong>,不能进行回溯操作.</p>

<p>对于整体的数据集而言,按照所有的特征属性进行划分操作,对所有的划分操作的结果集的&quot;<strong>纯度</strong>&quot;进行比较.选择&quot;纯度&quot;越高的特征属性作为当前需要分割的数据集进行分割操作,持续迭代,直到得到最终结果.决策树是通过&quot;纯度&quot;来选择分割特征属性点的.</p>

<p>说了这么多&quot;纯&quot;,那么究竟该如何量化纯度值呢?</p>

<h2 id="toc_9">决策树量化纯度</h2>

<p>决策树的构建是基于<strong>样本概率</strong>和<strong>纯度</strong>进行构建操作的,那么进行判断数据集是否&quot;纯&quot;可以通过三个公式进行判断,分别是<strong>Gini系数</strong>,<strong>熵(Entropy)</strong>,<strong>错误率</strong>.<strong>这三个公式值越大,表示数据越&quot;不纯&quot;.越小表示越&quot;纯&quot;</strong>;实践证明这三个公式效果差不多,一般情况使用熵公式</p>

<p>\[Gini=1-\sum_{i=1}^nP(i)^2\]</p>

<p>\[H(Entropy)=-\sum_{i=1}^nP(i)\log_2(P(i))\]</p>

<p>\[Error=1-max  \sideset{_{i=1}^n}{}\lbrace P(i) \rbrace\]</p>

<h2 id="toc_10">决策树量化纯度</h2>

<p>当计算出各个特征属性的量化纯度值后使用<strong>信息增益度</strong>来选择出当前数据集的分割特征属性;<strong>如果信息增益度的值越大,表示在该特征属性上会损失的纯度越大,那么该属性就越应该在决策树的上层</strong>,计算公式为:<br/>
\[Gain=\Delta=H(D)-H(D|A)\]</p>

<p>Gain为A为特征对训练数据集D的信息增益,它为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差</p>

<h2 id="toc_11">决策树算法的停止条件</h2>

<p>决策树构建的过程是一个递归的过程,所以必须给定停止条件,否则过程将不会进行停止,一般情况有两种停止条件:</p>

<ol>
<li>当每个子节点只有一种类型的时候停止构建;</li>
<li>当前节点中记录数小于某个阈值,同时迭代次数达到给定值时,停止构建过程,此时使用\(\max(p(i))\)作为节点的对应类型</li>
</ol>

<p>方式一可能会使树的节点过多,导致过拟合(Overfiting)等问题;<br/>
比较常用的方式是使用方式二作为停止条件</p>

<h2 id="toc_12">决策树算法效果评估(重新听,补充这里的推导算法)</h2>

<p>决策树的效果评估和一般的分类算法一样,采用<strong>混淆矩阵</strong>来进行计算准确率,召回率,精确率等指标.</p>

<p>也可以采用叶子节点的纯度值总和来评估算法的效果.值越小,效果越好.</p>

<p>也就是决策树的损失函数(该值越小,算法效果越好)<br/>
\[C(T)=\sum_{t=1}^{leaf}\dfrac{|D_t|}{|D|}H(t)\]</p>

<h3 id="toc_13">决策树直观理解结果计算</h3>

<p><img src="media/15255685118456/15256176444642.jpg" alt=""/></p>

<h1 id="toc_14">决策树生成算法</h1>

<p>建立决策树的主要是以下三种算法</p>

<ul>
<li>ID3</li>
<li>C4.5</li>
<li>CART(Classification And Regression Tree)</li>
</ul>

<h2 id="toc_15">ID3算法</h2>

<p>ID3算法是决策树的一个经典的构造算法,内部使用信息熵以及信息增益来进行构建;每次迭代选择信息增益最大的特征属性作为分割属性.</p>

<p>也就是会用到前面写到的两个公式</p>

<p>\[H(D)=-\sum_{i=1}^nP(i)\log_2(P(i))\]<br/>
\[Gain=\Delta=H(D)-H(D|A)\]</p>

<h2 id="toc_16">ID3算法优缺点</h2>

<ul>
<li><p>优点</p>
<ul>
<li>决策树构建速度快,实现简单</li>
</ul></li>
<li><p>缺点</p>
<ul>
<li>计算依赖于特征数目较多的特征,而属性值最多的属性并不一定最优</li>
<li>ID3算法不是递增算法</li>
<li>ID3算法是单变量决策树,对于特征属性之间的关系不会考虑</li>
<li>抗噪性差</li>
<li>只适合小规模数据集,需要将数据放到内存中</li>
</ul></li>
</ul>

<h2 id="toc_17">C4.5算法</h2>

<p>在ID3算法的基础上,进行算法优化提出的一种算法(C4.5);现在C4.5已经是特别经典的一种决策树构造算法;使用<strong>信息增益率</strong>来取代ID3算法中的信息增益,在树的构造过程中会<strong>进行剪枝操作进行优化</strong>;能够自动完成对连续属性的离散化处理;C4.5算法在选中分割属性的时候选择信息增益率最大的属性,涉及到的公式为:<br/>
\[H(D)=-\sum_{i=1}^nP(i)\log_2(P(i))\]<br/>
\[Gain=\Delta=H(D)-H(D|A)\]<br/>
\[Gain\_ration(A)=\dfrac{Gain(A)}{H(A)}\]</p>

<h2 id="toc_18">C4.5算法优缺点</h2>

<ul>
<li><p>优点</p>
<ul>
<li>产生的规则易于理解</li>
<li>准确率较高</li>
<li>实现简单</li>
</ul></li>
<li><p>缺点</p>
<ul>
<li>对数据集需要进行多次顺序扫描和排序,所以效率较低</li>
<li>只适合小规模数据集,需要将数据放到内存中</li>
</ul></li>
</ul>

<p>总结:属性越多,信息增益率越大</p>

<h2 id="toc_19">CART算法</h2>

<p>使用<strong>基尼系数</strong>作为数据纯度的量化指标来构建的决策树算法就叫做CART(Classification And Regression Tree,分类回归树)算法.CART算法使用<strong>GINI增益</strong>作为分割属性选择的标准,选择GINI增益最大的作为当前数据集的分割属性;可用于分类和归类两类问题.强调备注:CART构建的是二叉树.</p>

<p>\[Gini=1-\sum_{i=1}^nP(i)^2\]<br/>
\[Gain=\Delta=Gini(D)-Gini(D|A)\]</p>

<h2 id="toc_20">ID3,C4.5,CART分类算法总结</h2>

<ol>
<li>ID3和C4.5算法只适合在小规模数据集上使用</li>
<li>ID3和C4.5算法都是单变量决策树</li>
<li>当属性值取值比较多的时候,最好考虑C4.5算法,ID3得出的效果会比较差</li>
<li>决策树分类一般情况只适合小数据量的情况(数据可以放内存)</li>
<li>CART算法是三种算法中最常用的一种决策树构建算法</li>
<li>三种算法的区别仅仅只是对于当前树的评价标准不同而已,<em>ID3使用信息增益</em>,<em>C4.5使用信息增益率</em>,<em>CART使用基尼系数</em></li>
<li>CART算法构建的一定是二叉树,ID3和C4.5构建的不一定是二叉树</li>
</ol>

<table>
<thead>
<tr>
<th>算法</th>
<th>支持模型</th>
<th>树结构</th>
<th>特征选择</th>
<th>连续值处理</th>
<th>缺失值处理</th>
<th>剪枝</th>
<th>特征属性多次使用</th>
</tr>
</thead>

<tbody>
<tr>
<td>ID3</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>C4.5</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益率</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>CART</td>
<td>分类,回归</td>
<td>二叉树</td>
<td>基尼系数,均方差</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody>
</table>

<h1 id="toc_21">案例一:使用鸢尾花数据分类(分类树案例)</h1>

<p>使用决策树算法API对鸢尾花数据进行分类操作,并理解及进行决策树API的相关参数优化</p>

<p><a href="http://archive.ics.uci.edu/ml/datasets/Iris">数据来源</a></p>

<h2 id="toc_22">鸢尾花数据集描述</h2>

<p>Data Set Information:</p>

<p>This is perhaps the best known database to be found in the pattern recognition literature. Fisher&#39;s paper is a classic in the field and is referenced frequently to this day. (See Duda &amp; Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. </p>

<p>Predicted attribute: class of iris plant. </p>

<p>This is an exceedingly simple domain. </p>

<p>This data differs from the data presented in Fishers article (identified by Steve Chadwick, spchadwick &#39;@&#39; espeedaz.net ). The 35th sample should be: 4.9,3.1,1.5,0.2,&quot;Iris-setosa&quot; where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,&quot;Iris-setosa&quot; where the errors are in the second and third features.</p>

<p>Attribute Information:</p>

<ol>
<li>sepal length in cm </li>
<li>sepal width in cm </li>
<li>petal length in cm </li>
<li>petal width in cm </li>
<li>class: <br/>
-- Iris Setosa <br/>
-- Iris Versicolour <br/>
-- Iris Virginica</li>
</ol>

<h1 id="toc_23">分类树和回归树的区别</h1>

<p>分类树采用信息增益,信息增益率,基尼系数来评价树的效果,都是基于概率值进行判断的;而分类树的叶子节点的预测值一般为叶子节点中概率最大的类别作为当前叶子的预测值</p>

<p>在回归树中,叶子节点的预测值一般为叶子节点中所有值的均值来作为当前叶子节点的预测值.所以在回归树中一般采用MSE作为树的评价指标,即均方差.<br/>
\[MSE=\dfrac{1}{n}\sum_{i=1}^n(y_i-\hat y_i)^2\]</p>

<p>一般情况下,只会使用CART算法构建回归树.</p>

<h1 id="toc_24">案例二:波士顿房屋租赁价格预测(回归树案例)</h1>

<h1 id="toc_25">决策树过拟合和欠拟合</h1>

<h1 id="toc_26">决策树优化策略</h1>

<h2 id="toc_27">剪枝优化</h2>

<p>决策树过渡拟合一般情况是由于节点过多导致的,剪枝优化对决策树的正确率影响是比较大的,也是最常用的一种优化方式</p>

<p>决策树的剪枝是决策树算法中最基本,最有用的一种优化方案,主要分为两大类</p>

<h3 id="toc_28">决策树的剪枝方案</h3>

<p><strong>前置剪枝</strong>:在构建决策树的过程中,提前停止.结果是决策树一般比较小,实践证明这种策略无法得到比较好的结果</p>

<p><strong>后置剪枝</strong>:在决策树构建好后,然后再开始裁剪,一般使用两种方式:<br/>
    1. 用单一叶子节点代替整个子树.叶子点的分类采用子树中最主要的分类;<br/>
    2. 将一个子树完全替代另外一颗子树.<br/>
    3. 后置剪枝的主要问题是计算效率问题,存在一定的浪费情况.</p>

<p>后置剪枝总体思路(交叉验证):</p>

<ul>
<li>由完全树\(T_0\)开始,剪枝部分节点得到\(T_1\),在此剪枝得到\(T_2\)...直到仅剩树根的树\(T_k\)</li>
<li>在验证数据集上对这k+1个树进行评价,选择最优树\(T_a\)(损失函数最小的树)</li>
</ul>

<h3 id="toc_29">决策树剪枝过程</h3>

<p>对于给定的决策树\(T_0\):</p>

<ol>
<li>计算所有内部非叶子节点的<strong>剪枝系数</strong></li>
<li>查找<strong>最小剪枝系数</strong>的节点,将其子节点进行删除操作,进行剪枝得到决策树\(T_k\);如果存在多个最小剪枝系数节点,选择包含<strong>数据项最多</strong>的节点进行剪枝操作</li>
<li>重复上述操作,直到产生的剪枝决策树\(T_k\)只有1个节点</li>
<li>得到决策树\(T_0T_1T_2...T_k\)</li>
<li>使用<strong>验证样本集</strong>选择最优子树\(T_a\)</li>
</ol>

<p>使用验证集选择最优子树的标准,可以使用原始损失函数来考虑:<br/>
\[loss=\sum_{t=1}^{leaf}\dfrac{|D_t|}{|D|}H(t)\]</p>

<h3 id="toc_30">决策树剪枝损失函数及剪枝系数</h3>

<p>原始损失函数<br/>
\[loss=\sum_{t=1}^{leaf}\dfrac{|D_t|}{|D|}H(t)\]</p>

<p>叶节点越多,决策树越复杂,损失越大;修正添加剪枝系数,修改后的损失函数为<br/>
\[loss_{\alpha}=loss+\alpha*leaf\]</p>

<p>考虑根节点为r的子树,剪枝前后的损失函数分别为loss(R)和loss(r),当这两者相等的时候,可以求得剪枝系数<br/>
\[loss_{\alpha}(r)=loss(r)+\alpha\]<br/>
\[loss_{\alpha}(R)=loss(R)+\alpha*R_{leaf}\]<br/>
\[\alpha=\dfrac{loss(r)-loss(R)}{R_{leaf}-1}\]</p>

<h2 id="toc_31">Random Forest(随机森林)</h2>

<p>利用训练数据随机产生多个决策树,形成一个森林.然后使用这个森林对数据进行预测,选取最多结果作为预测结果.</p>

<h1 id="toc_32">总结</h1>

<p>1.分类树和回归树<br/>
区别:<br/>
a. 分类树中使用信息熵,gini系数,错误率作为数&quot;纯度&quot;的度量指标;回归树中使用MSE,MAE作为树的&quot;纯度&quot;度量指标;<br/>
b.分类树使用叶子节点中包含最多那个类别作为当前叶子的预测值;回归树中使用叶子节点中包含的所有样本的目标属性的均值作为当前叶子的预测值.</p>

<p>2.决策树的构建过程<br/>
思想:<br/>
让每次分裂数据集的时候,让分裂之后的数据集更加的&quot;纯&quot;(让数据更有区分能力)</p>

<p>3.决策树分裂属性选择方式<br/>
a.基于最优划分的规则进行选择:迭代计算所有特征属性上所有划分方式后的&quot;纯度&quot;,选择划分后更加&quot;纯&quot;的一种方式(信息增益,信息增益率).---&gt;<br/>
只能说明在当前数据集上是最优的,所以可能会存在一定的过拟合情况.<br/>
b.基于随机的划分规则:每次划分的时候,都是先选择一定数目的特征,然后在这部分特征中选择出一个最优的划分特征.因为每次选的划分特征都是局部最优的,相对来讲,可以增加模型的鲁棒性,降低模型的过拟合性.</p>

<p>4.决策树的欠拟合,过拟合<br/>
a.可以通过增加树的深度来缓解决策树欠拟合这个问题<br/>
b.我们可以通过限制树的复杂程度来缓解这个过拟合的问题<br/>
c.因此也就是找到一个平衡</p>

<p>5.网格交叉验证(GridSearchCV)<br/>
6.决策树的效果评估</p>

<h1 id="toc_33">决策树可视化</h1>

<p>决策树可视化可以方便我们直观的观察所构建的树模型;决策树可视化依赖graphiz服务,所以我们在进行可视化之前,安装对应的服务.</p>

<h2 id="toc_34">安装</h2>

<p>下载地址:<a href="http://www.graphviz.org/">http://www.graphviz.org/</a><br/>
Mac安装步骤</p>

<pre class="line-numbers"><code class="language-text"># 安装graphviz服务
brew install graphviz

# 安装python的graphviz插件
pip install graphviz

# 安装python的pydotplus插件
pip install pydotplus

</code></pre>

<h2 id="toc_35">使用方式</h2>

<p><strong>方式一:将模型输出dot文件,然后使用graphviz的命令将dot文件转换为pdf格式的文件</strong></p>

<pre class="line-numbers"><code class="language-text">from sklearn import tree
with open(&#39;iris.dot&#39;,&#39;w&#39;) as f:
    f = tree.export_graphviz(model,out_file=f)


# 命令行执行dot命令 dot -Tpdf iris.dot -o iris.pdf
</code></pre>

<p><strong>方式二:直接使用pydotplus插件直接生成pdf文件进行保存</strong></p>

<pre class="line-numbers"><code class="language-text">from sklearn import tree
import pydotplus
dot_data = tree.export_graphviz(model,out_file=None)
graph=pydotplus.graph_from_dot_data(dot_data)
graph.write_pdf(&quot;iris2.pdf&quot;)
# graph.write_png(&quot;iris3.png&quot;)
</code></pre>

<p><strong>方式三:使用Image对象直接显示pydotplus生成的图片</strong></p>

<pre class="line-numbers"><code class="language-text">from sklearn import tree
from IPython.display import Image
import pydotplus

dot_data=tree.export_graphviz(model,put_file=None,
    feature_names=[&#39;sepal length&#39;,&#39;sepal width&#39;,&#39;petal length&#39;,&#39;petal width&#39;],
    class_names=[&#39;Iris-setosa&#39;,&#39;Iris-versicolor&#39;,&#39;Iris-virgincia&#39;],
    filled=True,rounded=True,
    special_characters=True)
    
graph=pydotplus.graph_from_dot_data(dot_data)
Image(graph.create_png)
</code></pre>

<h1 id="toc_36">参考</h1>

<p>[1]: </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KNN]]></title>
    <link href="http://dmlcoding.com/15277324362749.html"/>
    <updated>2018-05-31T10:07:16+08:00</updated>
    <id>http://dmlcoding.com/15277324362749.html</id>
    <content type="html"><![CDATA[
<p>K近邻(K-nearst neighbors,KNN)是一种基本的机器学习算法,所谓K近邻,就是K个最近的邻居的意思,说的是每个样本都可以用它最接近的K个邻居来代表.</p>

<p>比如:判断一个人的人品,只需要观察与他来往最密切的几个人的人品好坏就可以得出,即,&quot;近朱者赤,近墨者黑&quot;.</p>

<p>KNN算法既可以应用于分类应用中,也可以应用在回归应用中.KNN在做回归和分类的主要区别在于最后做预测的时候决策方式不同,<strong>KNN在分类预测时,一般采用多数表决法;而在做回归预测时,一般采用平均值法.</strong></p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">KNN算法原理</h1>

<ol>
<li>从训练集合中获取K个离待预测样本距离最近的样本数据;</li>
<li>根据获取得到的K个样本数据来预测当前待预测样本的目标属性值</li>
</ol>

<p><img src="media/15277324362749/15277329582308.jpg" alt=""/></p>

<p>如上图所示,绿色圆要被决定赋予那一个类呢,是红色三角形还是蓝色矩形?</p>

<ol>
<li>如果K=3,由于红色三角形所占比例为2/3,绿色圆将被赋予红色三角形这个类.(多数投票)</li>
<li>如果K=5,由于蓝色四方形比例为3/5,因此绿色圆被赋予蓝色矩形.</li>
</ol>

<h1 id="toc_1">KNN三要素</h1>

<p>在KNN算法中,非常重要的主要是三个因素:</p>

<ul>
<li><strong>K值的选择</strong>:对于K值的选择,一般根据样本分布选择一个较小的值,然后通过<strong>交叉验证</strong>来选择一个比较合适的最终值;当选择比较小的K值的时候,表示使用较小领域中的样本进行预测,训练误差会减小,但是会导致模型变得复杂,容易过拟合;当选择较大的K值的时候,表示使用较大领域中的样本进行预测,训练误差会增大,同时会使模型变得简单,容易导致欠拟合.</li>
<li><strong>距离的度量</strong>:一般使用欧式距离(即,欧几里得距离);</li>
<li><strong>决策规则</strong>:在分类模型中,主要使用多数表决法或者加权多数表决法;在回归模型中,主要使用平均值法或者加权平均值法.加权的时候,可以认为距离越近的权重也应该越大.</li>
</ul>

<p>\[D(x,y) = \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + ... + (x_n-y_n)^2} = \sqrt{\sum\limits_{i=1}^{n}(x_i-y_i)^2}\]</p>

<h1 id="toc_2">KNN预测规则</h1>

<p>既然KNN算法既可以用在分类场景中,也可以用在回归场景中,那么针对不同的场景,KNN也有不同的预测规则</p>

<h2 id="toc_3">KNN分类预测规则</h2>

<p>在KNN分类应用中,一般采用对数投票或者加权多数表决法.</p>

<p><img src="media/15277324362749/15277370065477.jpg" alt=""/></p>

<ul>
<li><p><strong>多数表决法</strong>: 每个邻近样本的权重是一样的,也就是说最终预测的结果为出现类别最多的那个类,比如上图中蓝色矩形的最终类别为红色圆;</p></li>
<li><p><strong>加权多数表决法</strong>:每个邻近样本的权重是不一样的,一般情况下采用权重和距离成反比的方式来计算,也就是说最终预测结果是出现权重最大的那个类别:比如上图中,假设三个红色圆点到预测样本点的距离均为2,两个黄色星到预测样本点距离为1,那么蓝色圆圈的最终类别为黄色.(\(\frac12*3 &lt; 1*2\))</p></li>
</ul>

<h2 id="toc_4">KNN回归预测规则</h2>

<p>在KNN回归应用中,一般采用平均值法或者加权平均值法.<br/>
<img src="media/15277324362749/15277369863901.jpg" alt=""/></p>

<ul>
<li><p><strong>平均值法</strong>: 每个邻近样本的权重是一样的,也就是说最终预测的结果为所有邻近样本的目标属性值的均值;比如上图中,蓝色方块的最终预测值为\((3+3+3+2+2)/5=2.6\).</p></li>
<li><p><strong>加权平均值法</strong>:每个邻近样本的权重是不一样的,一般情况下采用权重和距离成反比的方式来计算,也就是说在计算均值的时候进行加权操作;比如上图中,假设上面三个点到待预测样本点的距离均为2,下面两个点到待预测样本点距离为1.那么蓝色方块的最终预测值为2.43.</p></li>
</ul>

<h1 id="toc_5">KNN算法实现方式</h1>

<p>KNN算法的重点在于找出K个最近邻的点,主要方式有一下几种:</p>

<ul>
<li><strong>蛮力实现(brute)</strong>:计算预测样本到所有训练集样本的距离,然后选择最小的K个距离即可得到K个最近邻点.缺点在于当特征数比较多,样本数比较多的时候,算法的执行效率比较低;</li>
<li><strong>KD树(kd_tree)</strong>:KD树算法中,首先是对训练数据进行建模,构建KD树,然后再根据建好的模型来获取邻近样本数据.</li>
<li>除此之外,还有一些从kd_tree修改后的求解最邻近点的算法,比如:Ball Tree,BBF Tree,MVP tree等</li>
</ul>

<p>接下来重点说一下KD树的构建过程</p>

<h1 id="toc_6">KD Tree</h1>

<p>KD Tree是KNN算法中用于计算最近邻的快速,便捷构建方式.</p>

<p>当样本数据量少的时候,我们可以使用brute这种暴力的方式进行求解最近邻,即计算到所有样本的距离.但是当样本量比较大的时候,直接计算所有样本的距离,工作量有点大,所以在这种情况下,我们可以使用kd_tree来快速的计算.</p>

<h2 id="toc_7">KD Tree构建方式</h2>

<p>KD树采用从m个样本的n维特征中,分别计算n个特征取值的方差,用方差最大的第k维特征\(n_k\)作为根节点.对于这个特征,选择取值的中位数\(n_{kv}\)作为样本的划分点,对于小于该值的样本划分到左子树,对于大于等于该值的样本划分到右子树,对于左右子树采用同样的方式找方差最大的特征最为根节点,递归即可产生KD树.(可以认为方差越大,越具有区分能力)</p>

<p><img src="media/15277324362749/15277383424878.jpg" alt="KD树构建过程"/></p>

<p>比如我们有二维样本6个，{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，构建kd树的具体步骤如草稿所示<br/>
<img src="media/15277324362749/15277404253921.jpg" alt=""/></p>

<p>构建的完整子树就如图所示<br/>
<img src="media/15277324362749/15278193600353.jpg" alt=""/></p>

<h2 id="toc_8">KD Tree查找最近邻</h2>

<p>当我们生成KD树以后,就可以去预测测试集里面的样本目标点了.对于一个目标点,我们首先在KD树里面找到包含目标点的叶子节点.以目标点为圆心,以目标点到叶子节点样本实例的距离为半径,得到一个超球体,最近邻的点一定在这个超球体内部.然后返回叶子节点的父节点,检查另一个子节点包含的超矩形体是否和超球体相交,如果相交就到这个子节点寻找是否有更加近的近邻,有的话就更新最近邻.如果不相交那就简单了,我们直接返回父节点的父节点,在另一个子树继续搜索最近邻.当回溯到根节点时,算法结束,此时保存的最近邻节点就是最终的最近邻.</p>

<p>具体过程如图所示<br/>
<img src="media/15277324362749/15277426727922.jpg" alt=""/></p>

<p><img src="media/15277324362749/15278221391077.jpg" alt=""/></p>

<h1 id="toc_9">参考引用</h1>

<ul>
<li><a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E8%BF%91%E9%84%B0%E5%B1%85%E6%B3%95">KNN wiki</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/6061661.html">K近邻法(KNN)原理小结</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[逻辑回归]]></title>
    <link href="http://dmlcoding.com/15198753668603.html"/>
    <updated>2018-03-01T11:36:06+08:00</updated>
    <id>http://dmlcoding.com/15198753668603.html</id>
    <content type="html"><![CDATA[
<p>不知道什么是逻辑回归,但是也许看完接下来的文章,你会有个大概的印象吧</p>

<h1 id="toc_0">简单介绍Logistic回归</h1>

<h2 id="toc_1">Logistic回归用到的知识点</h2>

<ul>
<li>Sigmoid函数和Logistic回归分类器</li>
<li>最优化理论初步</li>
<li>梯度下降最优化算法</li>
<li>数据中的缺失项处理</li>
</ul>

<span id="more"></span><!-- more -->

<h2 id="toc_2">Logistic回归的一般过程</h2>

<ul>
<li>1.收集数据:采用任意方法收集数据</li>
<li>2.准备数据:由于需要进行距离计算,因此要求数据类型为数值型.另外,结构化数据格式则最佳.</li>
<li>3.分析数据:采用任意方法对数据进行分析.</li>
<li>4.训练算法:大部分时间将用于训练,训练的目的是为了找到最佳的分类回归系数.</li>
<li>5.使用算法:首先,我们需要一些输入数据,并将其转换成对应的结构化数值;接着,基于训练好的回归系数就可以对这些数值进行简单的回归计算,判定他们属于哪个类别;在这之后,我们就可以在输出的类别上做一些其他分析工作.</li>
</ul>

<h1 id="toc_3">基于Logistic回归和Sigmoid函数的分类</h1>

<h2 id="toc_4">Logistic回归</h2>

<ul>
<li>优点
<ul>
<li>计算代价不高</li>
<li>易于理解和实现</li>
</ul></li>
<li>缺点
<ul>
<li>容易欠拟合</li>
<li>分类精度可能不高</li>
</ul></li>
<li>适用类型
<ul>
<li>数值型(数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析))</li>
<li>标称型数据(标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类))</li>
</ul></li>
</ul>

<p>基本公式:<br/>
Sigmoid函数具体的计算公式</p>

<p>\[p=h_{\theta}(x)=g(\theta^Tx)=\dfrac{1}{1+e^{-\theta^Tx}}\]<br/>
\[g(z)=\dfrac{1}{1+e^{-z}}\]</p>

<p><img src="media/15198753668603/15253390268591.jpg" alt=""/></p>

<p>\[<br/>
\begin{aligned}<br/>
g^{&#39;}(X)&amp; =\left(\dfrac{1}{1+e^{-z}}\right)^{&#39;} \\\<br/>
&amp; = \dfrac{e^{-z}}{(1+e^{-z})^2} \\\<br/>
&amp; = \dfrac{1}{1+e^{-z}}\cdot \dfrac{e^{-z}}{1+e{-z}} \\\<br/>
&amp; = \dfrac{1}{1+e^{-z}}\cdot \left(1-\dfrac{1}{1+e^{-z}}\right) \\\<br/>
&amp; = g(z)\cdot (1-g(z))<br/>
\end{aligned}<br/>
\]</p>

<h2 id="toc_5">logistic 回归及似然函数</h2>

<p>假设<br/>
\[<br/>
\begin{aligned}<br/>
P(y=1|x;\theta)&amp; =h_{\theta}(x) \\\<br/>
P(y=0|x;\theta)&amp; =1-h_{\theta}(x) \\\<br/>
P(y|x;\theta)&amp; =(h_{\theta}(x))^y(1-h_{\theta}(x))^{(1-y)}<br/>
\end{aligned}<br/>
\]</p>

<p>似然函数:<br/>
\[<br/>
\begin{aligned}<br/>
L(\theta) &amp; =p(\vec{y}|X;\theta) \\\<br/>
&amp; =\prod_{i=1}^m p(y^{(i)}|x^{(i)};\theta) \\\<br/>
&amp; = \prod_{i=1}^m (h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{(1-y^{(i)})}<br/>
\end{aligned}<br/>
\]</p>

<p>对数似然函数:<br/>
\[<br/>
\begin{aligned}<br/>
\ell(\theta) &amp; =\log L(\theta) \\\<br/>
&amp; = \sum_{i=1}^m\left(y^{(i)}\log h_{\theta}(x^{(i)})+(1-y^{(i)})\log (1-h_{\theta}(x^{(i)}))\right)<br/>
\end{aligned}<br/>
\]</p>

<p>最大似然/极大似然函数的随机梯度</p>

<p><img src="media/15198753668603/15350010973220.jpg" alt="" style="width:809px;"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[梯度下降笔记]]></title>
    <link href="http://dmlcoding.com/15277385790025.html"/>
    <updated>2018-05-31T11:49:39+08:00</updated>
    <id>http://dmlcoding.com/15277385790025.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[什么是最小二乘法]]></title>
    <link href="http://dmlcoding.com/15299158833805.html"/>
    <updated>2018-06-25T16:38:03+08:00</updated>
    <id>http://dmlcoding.com/15299158833805.html</id>
    <content type="html"><![CDATA[
<p>最小二乘法(又称最小平方法)是一种数学优化技术.他通过最小化误差的平方和寻找数据的最佳函数匹配.</p>

<p>利用最小二乘法可以简便地求得未知的数据,并使得这些求得的数据与实际数据之间误差的平方和为最小.</p>

<p><img src="media/15294841337807/15296334116393.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[回归算法]]></title>
    <link href="http://dmlcoding.com/15248143200628.html"/>
    <updated>2018-04-27T15:32:00+08:00</updated>
    <id>http://dmlcoding.com/15248143200628.html</id>
    <content type="html"><![CDATA[
<p>总结一下,这一节主要记录了下面的内容</p>

<ol>
<li>什么是回归算法.</li>
<li>线性回归,单变量线性回归,多变量线性回归</li>
<li>代价函数</li>
<li>过拟合与欠拟合</li>
<li>正则项</li>
</ol>

<p>在这篇文章中,主要介绍了什么是回归算法,以及简单的线性回归.包括单变量线性回归和多变量线性回归.</p>

<p>同时介绍了用最小二乘法求解\(\theta\)值.接下介绍了防止过拟合问题,加入正则项.</p>

<p>最后介绍了如何评价模型的效果.</p>

<h1 id="toc_0">什么是回归算法</h1>

<ol>
<li>回归算法是一种有监督算法;</li>
<li>回归算法是一种比较常用的机器学习算法,用于建立解释变量(自变量X)和观测值(因变量Y)之间的关系;</li>
<li>从机器学习的角度来讲,用于构建一个算法模型(函数)来做属性(X)与标签(Y)之间的映射关系,在算法的学习过程中,试图寻找一个函数h:使得\(R^d\to R\)使得参数之间的关系拟合性最好;</li>
<li>回归算法中,算法(函数)的最终结果是一个<strong>连续</strong>的数据值,输入值(属性值)是一个d维度的属性/数值向量;</li>
<li>因此,回归算法是用于预测连续型数值输出的算法.</li>
</ol>

<p>常用的回归算法有几种:比如,线性回归,Logistic回归,Softmax回归等等.先说最简单的线性回归算法.</p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">线性回归</h2>

<p>一个很常见的例子,根据房屋面积来预测房价(不考虑其他的环境因素).</p>

<p>假如我们有一些关于房屋面积和租赁价格的数据,比如</p>

<table>
<thead>
<tr>
<th>房屋面积(m<sup>2)</sup></th>
<th>租赁价格(1000/元)</th>
</tr>
</thead>

<tbody>
<tr>
<td>10</td>
<td>0.8</td>
</tr>
<tr>
<td>15</td>
<td>1</td>
</tr>
<tr>
<td>20</td>
<td>1.8</td>
</tr>
<tr>
<td>30</td>
<td>2</td>
</tr>
<tr>
<td>50</td>
<td>3.2</td>
</tr>
<tr>
<td>60</td>
<td>3</td>
</tr>
<tr>
<td>60</td>
<td>3.1</td>
</tr>
<tr>
<td>70</td>
<td>3.5</td>
</tr>
</tbody>
</table>

<p>那么一个问题来了,请问,如果现在有一个房屋面积为55平,请问最终的租赁价格是多少比较合适呢?</p>

<p>这个数据里面只有1个条件.很显然,在生活中,房价是和很多因素相关,比如房屋的面积,房间的数量,地理位置等等.</p>

<p>那么第一个例子,其实就是单变量线性回归.第二个当然就是多变量线性回归了.</p>

<p>公式描述如下:</p>

<p>\[<br/>
\begin{aligned}<br/>
h_{\theta}(x)&amp; =\theta_0+\theta_1x_1+\cdots+\theta_nx_n \\\<br/>
&amp; = \theta_01+\theta_1x_1+\cdots+\theta_nx_n \\\<br/>
&amp; = \theta_0x_0+\theta_1x_1+\cdots+\theta_nx_n \\\<br/>
&amp; = \sum_{i=0}^n\theta_ix_i \\\<br/>
&amp; = \theta^Tx<br/>
\end{aligned}<br/>
\]</p>

<p>所以在线性回归中,最终的要求也就是计算出\(\theta\)值,并选择最优的\(\theta\)值构成算法公式.</p>

<p>这里就涉及到两个问题:</p>

<ol>
<li>计算出\(\theta\)值;</li>
<li>选择最优的\(\theta\)值.</li>
</ol>

<p>对于第一个问题,在机器学习中,一般都是用列向量表示数据,因此公式的最后两行,也就是将第一个列向量转置成为行向量.这样就可以转换成一个行向量与列向量相乘,也就是矩阵相乘了.公式描述就能直观了.</p>

<p>对于第二个问题,就是找代价函数的最小值.后面会说到.</p>

<h3 id="toc_2">单变量线性回归</h3>

<p>因为只含有一个特征/输入变量,因此这样的问题叫做单变量线性回归问题.</p>

<p>公式描述如下:</p>

<p>\[h_{\theta}=\theta_0+ \theta_1x\]</p>

<h3 id="toc_3">多变量线性回归</h3>

<p>公式如下:<br/>
\[h_{\theta}(x) =\theta_0+\theta_1x_1+\cdots+\theta_nx_n\]</p>

<h1 id="toc_4">代价函数(Cost Function)</h1>

<p>我们选择的参数\(\theta\)决定了我们得到的直线相对于我们的训练集的准确程度,模型所预测的值与训练集中实际值之间的差距就是建模误差(modeling error).</p>

<p>我们的目标便是选择出可以使得<strong>建模误差的平方和</strong>能够<strong>最小</strong>的<strong>模型参数</strong>.<strong>即使得代价函数最小</strong>.</p>

<p>\[J(\theta_0,\theta_1)=\dfrac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2\]</p>

<p>\(h_{\theta}(x^{(i)})\)也就是我们的回归函数,即预测值.\(y^{(i)}\)是数据给定的真实值.</p>

<p>接下来说一下公式的推导过程</p>

<h1 id="toc_5">公式推导</h1>

<h2 id="toc_6">原理说明</h2>

<p>很显然,不可能找到一个算法可以百分百的预测正确,而且在实际问题中,总会有一些因素会造成误差.因此我们给线性回归的公式加上一个误差函数\(\epsilon^{(i)}\),这样就更合理了.</p>

<p>\[y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}\]</p>

<p><strong>补充说明</strong>:\(\epsilon^{(i)}(1\leq i \leq n)\)是独立同分布的,服从均值为0,方差为某定值\(\sigma^2\)的高斯分布.实际问题中,很多随机现象可以看到看到众多因素的独立影响的综合反应,往往服从正态分布.原理是因为中心极限定理.</p>

<p>因为,误差函数服从高斯分布,因此\(p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}e^{\left(-\dfrac{(\epsilon^{(i)})^2}{2\sigma^2}\right)}\)</p>

<p>因此,咱们也可以说是在某个因素发生的条件下,y发生的概率.所以可以改写成\(p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}e^{\left(-\dfrac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)}\)</p>

<p>最后,根据极大似然函数方法,咱们来求解这个\(\theta\)值.</p>

<h2 id="toc_7">\(\theta\)值推导过程以及求解</h2>

<p>根据极大似然函数,来求解\(\theta\)值,那么将公式改写成</p>

<p>\[\begin{aligned}<br/>
L(\theta)&amp; =\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)\\\<br/>
&amp; = \prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}e^{\left(-\dfrac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)}<br/>
\end{aligned}\]</p>

<p>最大似然,也就是要使这个方程的结果最大.连乘不好求,一般都是对连乘取对数.</p>

<p>现在转成求对数似然</p>

<p>\[<br/>
\begin{align}<br/>
\ell(\theta)&amp; =\log L(\theta) \\\<br/>
&amp; = \log\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}e^{\Big(-\dfrac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\Big)} \\\<br/>
&amp; = \sum_{i=1}^m\log\frac{1}{\sqrt{2\pi}\sigma}+\sum_{i=1}^m \log e^{\Big(-\dfrac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\Big)} \\\<br/>
&amp; = m\log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma^2}\cdot\frac{1}{2}\sum_{i=1}^m\Big(y^{(i)}-\theta^Tx{(i)}\Big)^2<br/>
\end{align}<br/>
\]</p>

<p>因为\(m\log\frac{1}{\sqrt{2\pi}\sigma}\)是一个常数,要使\(\ell(\theta)\)最大,那么也就是使这个式子的结果最小.保留\(\dfrac 12\)是因为求导求最小值的时候会约去参数.</p>

<p>这样就得到了我们的损失函数:<br/>
\[loss(y_j,\hat{y_j})=J(\theta)=\frac{1}{2}\sum_{i=1}^{m}\left(h_\theta(x^{(i)}-y^{(i)})\right)^2\]</p>

<p>我们的目标是最小化损失函数.</p>

<p>注意:矩阵的平方也等于矩阵的转置乘以矩阵.</p>

<p>\[J(\theta)=\cfrac{1}{2}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2=\cfrac{1}{2}(X\theta-Y)^T(X\theta-Y) \to \min \limits_{\theta}J(\theta)\]</p>

<p>既然是求最小值,那么肯定是通过求导来计算.对这个损失函数进行求导运算.</p>

<p>\[<br/>
\begin{aligned}<br/>
\nabla J(\theta)&amp; =\nabla_{\theta}\Big(\cfrac{1}{2}(X\theta-Y)^T(X\theta-Y)\Big) \\\<br/>
&amp; = \nabla_{\theta}\Big(\cfrac{1}{2}(\theta^TX^T-Y^T)(X\theta-Y)\Big) \\\<br/>
&amp; = \nabla_{\theta}\Big(\cfrac{1}{2}(\theta^TX^TX\theta-\theta^TX^TY-Y^TX\theta+Y^TY)\Big) \\\<br/>
&amp; = \cfrac{1}{2}\Big(2X^TX\theta-X^T-(Y^TX)^T\Big) \\\<br/>
&amp; = X^TX\theta-X^TY<br/>
\end{aligned}<br/>
\]</p>

<p>因此求解\(\theta\)的公式就推导出来了(最小二乘法).</p>

<p>\[\theta=(X^TX)^{-1}X^TY\]</p>

<h2 id="toc_8">公式验证</h2>

<p>求解\(\theta\)值的公式已经推导出来了,那让我们来校验一下,是否正确呢?</p>

<p>用python的科学计算包可以很方便的来做科学计算,废话不多说看代码吧.</p>

<p>1.构造测试数据</p>

<pre class="line-numbers"><code class="language-python">import numpy as np
import pandas as pd

# 构造测试数据y=3x1+x2
df=pd.DataFrame({
    &quot;x1&quot;:[1,2,3,4,5,6],
    &quot;x2&quot;:[1,2,1,2,1,2],
    &quot;y&quot;:[3,6,7,10,11,14]
})

</code></pre>

<p>2.抽取x和y</p>

<pre class="line-numbers"><code class="language-python"># 抽取x和y
x=df[[&#39;x1&#39;,&#39;x2&#39;]]
# 将y转换成n行1列的列向量
y=df[&#39;y&#39;].values.reshape((-1,1))

</code></pre>

<p>3.求解\(\theta\) 值</p>

<p><em>根据公式计算\(\theta\)</em>值,推导过程上面已经详述了.<br/>
\[\theta=(X^TX)^{-1}X^TY\]</p>

<p><em>numpy</em> api说明:</p>

<ul>
<li>mat:将ndarray转成matrix</li>
<li>dot:矩阵乘法</li>
<li>T:求矩阵的转置</li>
<li>I:求矩阵的逆</li>
</ul>

<pre class="line-numbers"><code class="language-python"># 将公式转成python代码,计算theta值

np.dot(np.mat(x.T.dot(x)).I,x.T).dot(y)
</code></pre>

<p>4.结果输出</p>

<pre class="line-numbers"><code class="language-text">    matrix([[ 2.],
            [ 1.]])
</code></pre>

<p>很明显,计算结果正确.两个系数分别是2和1</p>

<h1 id="toc_9">最小二乘法的参数最优解</h1>

<p>求解\(\theta\)的公式为\(\theta=(X^TX)^{-1}X^TY\).但是最小二乘法的使用要求矩阵\(X^TX\)是可逆的.</p>

<p>为了防止不可逆或者过拟合的问题存在,可以<strong>增加额外数据影响</strong>,导致最终的矩阵是可逆的.</p>

<p>\[\theta=(X^TX+\lambda I)^{-1}X^TY\]</p>

<p>注意:最小二乘法直接求解的难点:矩阵逆的求解是一个难处.<br/>
因此,后面会说到另一种求解办法,即,梯度下降法来求解.</p>

<h1 id="toc_10">线性回归案例</h1>

<p><strong>数据来源</strong>:<a href="https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption">https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption</a></p>

<p>数据描述:现有一批描述家庭用电情况的数据,对数据进行算法模型预测,并最终得到预测模型(每天各个时间段和功率之间的关系,功率与电流之间的关系等)</p>

<p><img src="media/15248143200628/15264383529325.jpg" alt=""/></p>

<p>我们使用 <code>sklearn</code> 库中的 <code>linear_model</code> 中的 <code>LinerRegression</code>来获取算法,让我们来做一个成功的<code>调包侠</code>,哈哈哈哈.</p>

<h2 id="toc_11">线性回归预测时间和功率之间的关系</h2>

<p>虽然调api不用我们来实现算法的实现了,但是学习的过程还是应该掌握公式的推导比较好.</p>

<p>代码示例如下(机器学习的简单开发流程):</p>

<pre class="line-numbers"><code class="language-text"># -*- coding: utf-8 -*-
&#39;&#39;&#39;
    Created by hushiwei on 2018/4/27.

    1:时间与功率之间的关系
&#39;&#39;&#39;

import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 设置字符集,防止中文乱码
mpl.rcParams[&#39;font.sans-serif&#39;]=[u&#39;simHei&#39;]
mpl.rcParams[&#39;axes.unicode_minus&#39;]=False


# 加载数据
df = pd.read_csv(
    &quot;/Users/hushiwei/PycharmProjects/Python-AI/MachineLearning/LinearRegression/datas/household_power_consumption_1000.txt&quot;,
    sep=&quot;;&quot;)
print(df.head())

# 异常数据处理
new_df = df.replace(&#39;?&#39;, np.nan)
datas = new_df.dropna(axis=0, how=&#39;any&#39;)

# 观察数据

print(datas.describe())


# 处理时间

def date_format(dt):
    import time
    t = time.strptime(&#39; &#39;.join(dt), &#39;%d/%m/%Y %H:%M:%S&#39;)
    return (t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec)


# 获取X和Y,并将时间转换为数值型变量
# X:时间
# Y:用电量

X=datas.iloc[:,0:2]
X=X.apply(lambda x:pd.Series(date_format(x)),axis=1)
Y=datas[&#39;Global_active_power&#39;]


print(X.head())
print(Y.head())


# 划分数据集
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)

# 对数据进行标准化
ss=StandardScaler()

# 训练并转换
X_train=ss.fit_transform(X_train)
# 直接使用在模型构建数据上进行一个数据标准化操作
X_test=ss.transform(X_test)

# 这里,fit_transform和transform的区别?
# fit_tranform是计算并转换,在这个过程中会计算出均值和方差,然后将变量进行标准化去量纲
# transform是转换,它将用上面计算出的均值和方差来进行标准化去量纲.
# 因为训练集的数据相较于测试集更多 ,所以测试集也延用训练集算出来的均值和方差
# 因此fit_transform在transform上面调用

# 模型训练

liner=LinearRegression()
liner.fit(X_train,Y_train)


# 模型校验

y_predict=liner.predict(X_test)
time_name=[&#39;年&#39;,&#39;月&#39;,&#39;日&#39;,&#39;时&#39;,&#39;分&#39;,&#39;秒&#39;]
print(&quot;准确率:&quot;,liner.score(X_train,Y_train))
mse=np.average((y_predict-Y_test)**2)
rmse=np.sqrt(mse)
print(&#39;rmse:&#39;,rmse)
print(&quot;特征的系数为:&quot;,list(zip(time_name,liner.coef_)))

# 模型保存

# from sklearn.externals import joblib
#
# joblib.dump(ss,&quot;data_ss.model&quot;) ## 将标准化模型保存
# joblib.dump(liner,&quot;data_lr.model&quot;) ## 将模型保存
#
# joblib.load(&quot;data_ss.model&quot;) ## 加载模型
# joblib.load(&quot;data_lr.model&quot;) ## 加载模型

# 预测值和实际值可视化画图比较

t=np.arange(len(X_test))
plt.figure(facecolor=&#39;w&#39;)
plt.plot(t,Y_test,&#39;r-&#39;,linewidth=2,label=&#39;真实值&#39;)
plt.plot(t,y_predict,&#39;g-&#39;,linewidth=2,label=&#39;预测值&#39;)
plt.legend(loc=&#39;upper left&#39;)
plt.title(&quot;线性回归预测时间与功率之间的关系&quot;,fontsize=20)
plt.grid(b=True)
plt.show()

</code></pre>

<p>结果输出:</p>

<pre class="line-numbers"><code class="language-text">准确率: 0.24409311805909026
rmse: 1.164092345973625
特征的系数为: [(&#39;年&#39;, 0.0), (&#39;月&#39;, 1.1102230246251565e-16), (&#39;日&#39;, -1.415881661733238), (&#39;时&#39;, -0.9349532432495644), (&#39;分&#39;, -0.10214075598497), (&#39;秒&#39;, 0.0)]
</code></pre>

<p>可视化显示:</p>

<p><img src="media/15248143200628/15264444548035.jpg" alt=""/></p>

<p>从这个结果可以看出,预测效果不太好,可能是时间与功率没啥关系,我们现在换一下因变量与自变量</p>

<h2 id="toc_12">线性回归预测功率与电流之间的关系</h2>

<p>代码示例:</p>

<pre class="line-numbers"><code class="language-text"># -*- coding: utf-8 -*-
&#39;&#39;&#39;
    Created by hushiwei on 2018/4/27.

    desc: 功率与电流之间的关系
&#39;&#39;&#39;

import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 设置字符集,防止中文乱码
mpl.rcParams[&#39;font.sans-serif&#39;]=[u&#39;simHei&#39;]
mpl.rcParams[&#39;axes.unicode_minus&#39;]=False


# 加载数据
df = pd.read_csv(
    &quot;/Users/hushiwei/PycharmProjects/Python-AI/MachineLearning/LinearRegression/datas/household_power_consumption_1000.txt&quot;,
    sep=&quot;;&quot;)
print(df.head())

# 异常数据处理
new_df = df.replace(&#39;?&#39;, np.nan)
datas = new_df.dropna(axis=0, how=&#39;any&#39;)

# 观察数据

print(datas.describe())



X=datas.iloc[:,2:4]
Y=datas.iloc[:,5]


print(X.head())
print(Y.head())


# 划分数据集
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)

# 对数据进行标准化
ss=StandardScaler()

# 训练并转换
X_train=ss.fit_transform(X_train)
# 直接使用在模型构建数据上进行一个数据标准化操作
X_test=ss.transform(X_test)

# 这里,fit_transform和transform的区别?
# fit_tranform是计算并转换,在这个过程中会计算出均值和方差,然后将变量进行标准化去量纲
# transform是转换,它将用上面计算出的均值和方差来进行标准化去量纲.
# 因为训练集的数据相较于测试集更多 ,所以测试集也延用训练集算出来的均值和方差
# 因此fit_transform在transform上面调用

# 模型训练

liner=LinearRegression()
liner.fit(X_train,Y_train)


# 模型校验

y_predict=liner.predict(X_test)
print(&quot;电流训练准确率: &quot;,liner.score(X_train,Y_train))
print(&quot;电流预测准确率: &quot;,liner.score(X_test,Y_test))
print(&quot;电流参数: &quot;,liner.coef_)



# 预测值和实际值可视化画图比较

t=np.arange(len(X_test))
plt.figure(facecolor=&#39;w&#39;)
plt.plot(t,Y_test,&#39;r-&#39;,linewidth=2,label=&#39;真实值&#39;)
plt.plot(t,y_predict,&#39;g-&#39;,linewidth=2,label=&#39;预测值&#39;)
plt.legend(loc=&#39;upper left&#39;)
plt.title(&quot;线性回归预测时间与功率之间的关系&quot;,fontsize=20)
plt.grid(b=True)
plt.show()

</code></pre>

<p>模型预测结果</p>

<pre class="line-numbers"><code class="language-text">电流训练准确率:  0.9909657573073489
电流预测准确率:  0.9920420609708968
电流参数:  [5.07744316 0.07191391]
</code></pre>

<p><img src="media/15248143200628/15264449452427.jpg" alt=""/></p>

<p>从图中也可以看出,效果很明显,预测得非常好.</p>

<h1 id="toc_13">线性回归的过拟合和正则项</h1>

<p>我们用线性回归的时候,可能拟合的不是那么好,那么可以加大阶数,去拟合.</p>

<p>但是也不是阶数越高效果越好.为什么这么说呢?看例子分析.</p>

<p>我们的目标函数是(之前已经写过推导过程):<br/>
\[J(\theta)=\dfrac{1}{2}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2\]</p>

<p>为了防止数据过拟合,也就是\(\theta\)值在样本空间中不能过大/过小,可以在目标函数之上增加一个平方和损失:<br/>
\[J(\theta)=\dfrac{1}{2}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^n\theta_j^2\]</p>

<p>我们如果对这个函数进行求导计算会得出之前写的一个函数,即<br/>
\[\theta=(X^TX+\lambda I)^{-1}X^TY\]</p>

<p>注意:这里新加的\(\lambda\sum_{i=1}^n\theta_j^2\),也叫正则项(norm).这里的这个正则项叫做L2-norm.所以还有其他的正则项,且看后续说明.</p>

<h2 id="toc_14">正则项</h2>

<p><strong>L2正则(L2-norm)</strong>:\(\lambda\sum_{i=1}^n\theta_j^2 \quad \lambda &gt; 0\)</p>

<p><strong>L1正则(L1-norm)</strong>:\(\lambda\sum_{i=1}^n|\theta_j| \quad \lambda &gt; 0\)</p>

<h2 id="toc_15">Ridge回归</h2>

<p>使用L2正则的线性回归模型就称为Ridge回归(也叫岭回归)<br/>
\[J(\theta)=\dfrac{1}{2}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^n\theta_j^2 \qquad \lambda &gt; 0\]</p>

<h2 id="toc_16">Lasso回归</h2>

<p>使用L1正则的线性回归模型就称为LASSO回归(Least Absolute Shrinkage and Selection Operator)<br/>
\[J(\theta)=\dfrac{1}{2}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^n|\theta_j| \qquad \lambda &gt; 0\]</p>

<h2 id="toc_17">Ridge(L2-norm)和LASSO(L1-norm)比较</h2>

<p>L2-norm中,由于对于各个维度的参数缩放是在一个圆内缩放的,不可能导致有维度参数变为0的情况,那么也就不会产生稀疏解;实际应用中,数据的维度中是存在噪音和冗余的,稀疏的解可以找到有用的维度并且减少冗余,提高回归预测的准确性和鲁棒性(减少了overfitting)(L1-norm可以达到最终解的稀疏性的要求)</p>

<p>Ridge模型具有较高的准确性,鲁棒性以及稳定性;LASSO模型具有较高的求解速度.</p>

<p>如果既要考虑稳定性也考虑求解的速度,就使用Elastic Net<br/>
<img src="media/15248143200628/15265465368148.jpg" alt=""/></p>

<h2 id="toc_18">Elastic Net</h2>

<p>同时使用L1正则和L2正则的线性回归模型就称为Elastic Net算法(弹性网络算法)</p>

<p>\[J(\theta)=\dfrac{1}{2}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda \left(p\sum_{j=1}^n|\theta_j| +(1-p)\sum_{j=1}^n\theta_j^2 \right) \qquad \lambda &gt; 0 , p \in [0,1]\]</p>

<h1 id="toc_19">模型效果判断</h1>

<p>MSE:误差平方和,越趋近于0表示模型越拟合训练数据<br/>
\[MSE=\dfrac{1}{m}\sum_{i=1}^m(y_i-\hat y_i)^2\]</p>

<p>RMSE:MSE的平方根,作用同MSE<br/>
\[RMSE=\sqrt{MSE}=\sqrt{\dfrac{1}{m}\sum_{i=1}^m(y_i-\hat y_i)^2}\]</p>

<p>\(R^2\):取值范围(负无穷,1],值越大表示模型越拟合训练数据;最优解是1;当模型预测为随机值的时候,有可能为负;若预测值恒为样本期望,\(R^2\)为0</p>

<p>TSS:总平方和TSS(Total Sum Of Squares),表示样本之间的差异情况,是伪方差的m倍.</p>

<p>RSS:残差平方和RSS(Residual Sum Of Squares),表示预测值和样本值之间的差异情况,是MSE的m倍</p>

<p>\[R^2=1-\dfrac{RSS}{TSS}=1-\dfrac{\sum_{i=1}^m(y_i-\hat y_i)^2}{\sum_{i=1}^m(y_i-\overline y)^2} \qquad \overline y=\dfrac{1}{m}\sum_{i=1}^my_i\]</p>

<h1 id="toc_20">机器学习调参</h1>

<h2 id="toc_21">调参</h2>

<p>在实际工作中,对于各种算法模型(线性回归)来讲,我们需要获取\(\theta,\lambda,p\)的值;\(\theta\)的求解其实就是算法模型的求解,一般不需要开发人员参与(算法已经实现),主要需要求解的是\(\lambda\)和p的值,这个过程就叫做<strong>调参(超参)</strong></p>

<h2 id="toc_22">交叉验证</h2>

<p>将训练数据分为多份,其中一份进行数据验证并获取最优的超参:\(\lambda\)和p;比如:十折交叉验证,五折交叉验证(scikit-learn中默认)等</p>

<p><img src="media/15248143200628/15267171709516.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[学习机器学习]]></title>
    <link href="http://dmlcoding.com/15263722402424.html"/>
    <updated>2018-05-15T16:17:20+08:00</updated>
    <id>http://dmlcoding.com/15263722402424.html</id>
    <content type="html"><![CDATA[
<p>机器不能代替你学习,代替你成长,终归还得自己踏实学习!</p>

<p>对于下面的基础知识,不管是大学的时候没学好,还是由于时间太长了,导致自己忘记了.都无所谓了.既然打算学习机器学习,那么就不得不花时间去重拾这些知识.</p>

<p>至于如何学,很多说法,我在网上看了很多,比如:</p>

<ol>
<li>看视频学习;</li>
<li>看教程学习;</li>
<li>边学习边看.</li>
</ol>

<span id="more"></span><!-- more -->

<p>我个人是先回顾一遍,常用的知识点做到心里有数,用到的时候,知道用的什么知识点,去哪里查.然后在学习的过程中巩固,毕竟不是应试考试,没时间也没精力去抠知识点了.</p>

<p>之前总是想要先把数学之类的都复习好,再开始看机器学习算法的.但是每次翻开高数课本,每次第一章的极限没看完,就看不下去了.</p>

<p>所以,这是我自己的问题,我只能想办法,慢慢尝试,看看如何跳过这个坎.</p>

<p>这也是我目前学习中遇到的问题,我会都记录下来,一来是复习,二是帮助大家.</p>

<p>最后,学习缓慢.磕磕碰碰,在所难免,与诸君共勉!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[python-matplotlib画图中文显示不出来]]></title>
    <link href="http://dmlcoding.com/15242825563762.html"/>
    <updated>2018-04-21T11:49:16+08:00</updated>
    <id>http://dmlcoding.com/15242825563762.html</id>
    <content type="html"><![CDATA[
<pre class="line-numbers"><code class="language-text">import matplotlib as mpl
import matplotlib.pyplot as plt

## 设置字符集，防止中文乱码
mpl.rcParams[&#39;font.sans-serif&#39;]=[u&#39;simHei&#39;]
mpl.rcParams[&#39;axes.unicode_minus&#39;]=False
</code></pre>

<span id="more"></span><!-- more -->

<p><img src="media/15242825563762/15242826157154.jpg" alt=""/></p>

<p>已经在代码中设置了字符集,但是却仍然无法正确显示.这是因为系统中没有这个字体</p>

<h1 id="toc_0">解决方案</h1>

<h2 id="toc_1">1. 下载SimHei.ttf字体</h2>

<pre class="line-numbers"><code class="language-text">http://www.fontpalace.com/font-details/SimHei/
</code></pre>

<h2 id="toc_2">2. 查看matplotlib的字体存放目录</h2>

<pre class="line-numbers"><code class="language-text">/Users/hushiwei/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data
</code></pre>

<pre class="line-numbers"><code class="language-text">total 64
drwxr-xr-x   6 hushiwei  staff    192  1 18 12:03 ./
drwxr-xr-x  86 hushiwei  staff   2752  1 18 12:03 ../
drwxr-xr-x   5 hushiwei  staff    160  1 18 12:03 fonts/
drwxr-xr-x  58 hushiwei  staff   1856  1 18 12:03 images/
-rw-rw-r--   2 hushiwei  staff  31975 10 10  2017 matplotlibrc
drwxr-xr-x  27 hushiwei  staff    864  1 18 12:03 stylelib/
</code></pre>

<h2 id="toc_3">3. 导入字体,删除缓存</h2>

<p>将我们刚刚下载好的字体,<code>SimHei.ttf</code>文件​，放在matplotlib的font目录（./matplotlib/mpl-data/fonts/ttf/）或者系统的font目录下都行(/usr/share/fonts/)</p>

<p>接着:<br/>
​删除/Users/hushiwei/.matplotlib/*目录，重新运行你的画图脚本；此时程序会自动在~/.matplotlib目录下生成fontList.json文件；</p>

<h2 id="toc_4">4. 重启jupyter ,重新执行代码.</h2>

<p>OK,完成....<br/>
<img src="media/15242825563762/15242830696194.jpg" alt=""/></p>

<h1 id="toc_5">总结</h1>

<ol>
<li>需要在代码中引入字符集.</li>
<li>系统中需要有这个字体.</li>
<li>导入字体后,若要生效,需要先删除~/.matplotlib目录,让这个目录中的文件重新生成.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Markdown中用LaTex语法写数学公式]]></title>
    <link href="http://dmlcoding.com/15235878044126.html"/>
    <updated>2018-04-13T10:50:04+08:00</updated>
    <id>http://dmlcoding.com/15235878044126.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15235878044126/15249021210382.jpg" alt=""/></p>

<p>最近在学习机器学习和神经网络的一些知识, 在用Markdown做笔记的时候, 发现公式用到LaTeX十分方便, 写篇文章记录在学习过程中的常用的LaTeX公式和用法, 因为本博客支持MathJax, <code>MathJax</code>是一款运行在浏览器中的开源的数学符号渲染引擎，使用MathJax可以方便的在浏览器中显示数学公式，MathJax可以解析<code>Latex</code>、<code>MathML</code>和<code>ASCIIMathML</code>的标记语言。</p>

<h2 id="toc_0">关键字索引</h2>

<span id="more"></span><!-- more -->

<p><code>\</code> 或者<code>~</code>: 空格</p>

<p><code>\verb +text+</code> : 直接打印不执行任何LATEX命令。这里的+仅是分隔符的一个例子, 除了* 或空格，可以使用任意一个字符。</p>

<p><code>\url{www.baidu.com}</code> : 嵌入超链接, 可在文档中点击访问</p>

<p><code>\emph{text}</code> : 强调的内容</p>

<p><code>\section{...}</code> : 章</p>

<p><code>\subsection{...}</code> : 节</p>

<p><code>\subsubsection{...}</code> : 子节</p>

<p><code>\paragraph{...}</code> : 段落</p>

<p><code>\subparagraph{...}</code> : 子段落</p>

<p>`<code>text&#39;</code> : 单引号</p>

<p><code>text&#39;&#39;</code> : 双引号引起来的text</p>

<p><code>--</code> : 折线</p>

<h2 id="toc_1">基本概念</h2>

<ul>
<li><p>控制序列: 以<code>\</code>开头,参数：<code>必须参数{}</code>和<code>可选参数[]</code></p></li>
<li><p>环境: 以<code>bengin 环境名</code>开始，并以<code>end 环境名</code>结束</p></li>
<li><p>文本模式:如果你想要在公式中排版普通的文本，那么你必须要把这些文本放在<code>\textrm{...}</code> 命令中</p></li>
<li><p>数学模式</p></li>
</ul>

<ol>
<li><p>内嵌模式:公式直接放在文字之间，公式高度一般受文本高度限制: <code>$ latex $</code></p>
<blockquote>
<p>文字\(\sum_{i=0}^{n}i^2\) 文字</p>
</blockquote></li>
<li><p>独立模式:公式另起一行，高度可调整</p>
<blockquote>
<p>文字\[\sum_{i=0}^{n}i^2\]文字</p>
</blockquote></li>
</ol>

<blockquote>
<p>在LaTeX中，花括号是用于分组，即花括号内部文本为一组, 大括号能消除二义性, 一个组即单个字符或者使用{..}包裹起来的内容。</p>
</blockquote>

<h3 id="toc_2">上标与下标</h3>

<ul>
<li><p>上标<code>^{角标}</code>，下标<code>_{角标}</code>, 默认情况下，上下标符号仅仅对下一个组起作用。</p>
<p><code>x_1</code> : \(x_1\) </p>
<p><code>x_1^2</code> : \(x_1^2\) </p>
<p><code>x^{2_1}</code> : \(x^{2_1}\) </p>
<p><code>x_{(22)}^{(n)}</code> : \(x_{(22)}^{(n)}\)</p></li>
</ul>

<h3 id="toc_3">分式</h3>

<ul>
<li><p><code>\frac{分子}{分母}</code></p>
<p><code>\frac ab</code> : \(\frac ab\)</p>
<p><code>\frac{x+y}{2}</code> : \(\frac{x+y}{2}\)</p>
<p><code>\frac{1}{1+\frac{1}{2}}</code> : \(\frac{1}{1+\frac{1}{2}}\)</p></li>
</ul>

<h3 id="toc_4">根式</h3>

<ul>
<li><p>开平方：<code>\sqrt{表达式}</code>；开n次方：<code>\sqrt[n]{表达式}</code></p>
<p><code>\sqrt{2}&lt;\sqrt[3]{3}</code> : \(\sqrt{2}&lt;\sqrt[3]{3}\)</p>
<p><code>\sqrt[4]{\frac xy}</code> ：\(\sqrt[4]{\frac xy} \)</p>
<p><code>\sqrt{1+\sqrt[^p]{1+a^2}}</code> : \(\sqrt{1+\sqrt[^p]{1+a^2}}\)</p></li>
</ul>

<h3 id="toc_5">求和与积分</h3>

<ul>
<li><p>求和<code>\sum</code> ; 求积分<code>\int</code>; 上下限就是<code>上标和下标</code></p>
<p><code>\int_1^\infty</code> ：\(\int_1^\infty\)</p>
<p><code>\int_a^b f(x)dx</code> : \(\int_a^b f(x)dx\)</p>
<p><code>\sum_1^n</code> ：\(\sum_1^n\)</p>
<p><code>\sum_{k=1}^n\frac{1}{k}</code> : \(\sum_{k=1}^n\frac{1}{k}\)</p></li>
</ul>

<h3 id="toc_6">微分</h3>

<p><code>\frac{\partial E_w}{\partial w}</code> : \(\frac{\partial E_w}{\partial w}\)</p>

<p>\[\frac{\partial E_\hat{w}}{\partial \hat{w}}= 2X^T(X\vec{\hat{w}}-\vec{y}) = 0\]</p>

<h3 id="toc_7">加粗</h3>

<p><code>\mathbf{x}_i :</code>\(\mathbf{x}_i\)</p>

<h3 id="toc_8">多重积分</h3>

<ul>
<li>对于多重积分，不要使用<code>\int\int</code>此类的表达，应该使用<code>\iint</code>,<code>\iiint</code>等特殊形式。效果如下：</li>
</ul>

<p>\[<br/>
\begin{array}{cc}<br/>
\mathrm{Bad} &amp; \mathrm{Better} \\\\<br/>
\hline \\\\<br/>
\int\int_S f(x)\,dy\,dx &amp; \iint_S f(x)\,dy\,dx \\\\<br/>
\int\int\int_V f(x)\,dz\,dy\,dx &amp; \iiint_V f(x)\,dz\,dy\,dx<br/>
\end{array}<br/>
\]</p>

<ul>
<li>在微分前应该使用<code>\</code>,来增加些许空间，否则\(\TeX\)会将微分紧凑地排列在一起。如下：</li>
</ul>

<p>\[<br/>
\begin{array}{cc}<br/>
\mathrm{Bad} &amp; \mathrm{Better} \\\\<br/>
\hline \\\\<br/>
\iiint_V f(x)dz dy dx &amp; \iiint_V f(x)\,dz\,dy\,dx<br/>
\end{array}<br/>
\]</p>

<h2 id="toc_9">特殊函数与符号</h2>

<ol>
<li>常见的三角函数，求极限符号可直接使用<code>\+</code>缩写即可
<code>\sin x</code> , <code>\arctan x</code>, <code>\lim_{1\to\infty}</code> : \(\sin x\),\(\arctan x\),\(\lim_{1\to\infty}\)</li>
<li>比较运算符：
<code>\lt \gt \le \ge \neq</code> ： \(\lt \gt \le \ge \neq\)
<ul>
<li>在这些运算符前面加上<code>\not</code>
<code>\not\lt</code> ：\(\not\lt\)</li>
</ul></li>
<li><code>\times \div \pm \mp</code> ：\(\times \div \pm \mp\)
<code>\cdot</code>表示居中的点
<code>x \cdot y</code> : \(x \cdot y\)。</li>
<li>集合关系与运算
<code>\cup \cap \setminus \subset \subseteq \subsetneq \supset \in \notin \emptyset \varnothing</code> ：\(\cup 
\cap \setminus \subset \subseteq \subsetneq \supset \in \notin \emptyset \varnothing\)</li>
<li>排列
<code>{n+1 \choose 2k}</code> : \({n+1 \choose 2k}\) 
<code>\binom{n+1}{2k}</code> : \({n+1 \choose 2k}\)</li>
<li>箭头：
<code>\to \rightarrow \leftarrow \Rightarrow \Leftarrow \mapsto</code> : \(\to \rightarrow \leftarrow \Rightarrow \Leftarrow \mapsto\)</li>
<li>逻辑运算符
<code>\land \lor \lnot \forall \exists \top \bot \vdash \vDash</code> ：\(\land \lor \lnot \forall \exists \top \bot \vdash \vDash\)</li>
<li><code>\star \ast \oplus \circ \bullet</code> ： \(\star \ast \oplus \circ \bullet\)</li>
<li><code>\approx \sim \cong \equiv \prec</code> ： \(\approx \sim \cong \equiv \prec\)</li>
<li><code>\infty \aleph_0</code> : \(\infty  \aleph_0\) 
<code>\nabla \partial</code> : \(\nabla \partial\) \Im \Re \(Im \Re\)</li>
<li>模运算 <code>\pmode</code> 
<code>a\equiv b\pmod n</code> ：\(a\equiv b\pmod n\)</li>
<li><code>\ldots与\cdots</code>区别是dots的位置不同，ldots位置稍低，cdots位置居中
<code>a_1+a_2+\cdots+a_n</code> : \(a_1+a_2+\cdots+a_n\)
<code>a_1, a_2, \ldots ,a_n</code> : \(a_1, a_2, \ldots ,a_n\)。</li>
<li><p>一些希腊字母具有变体形式<br/>
<code>\epsilon \varepsilon</code> : \( \epsilon \varepsilon\)<br/>
<code>\phi \varphi</code> : \(\phi \varphi\)</p></li>
</ol>

<ul>
<li>使用<a href="http://detexify.kirelabs.org/classify.html">Detexify</a>在网页上画出符号，Detexify会给出相似的符号及其代码但是不能保证它给出的符号可以在MathJax中使用，你可以参考<a href="http://docs.mathjax.org/en/latest/tex.html#supported-latex-commands">supported-latex-commands</a>确定MathJax是否支持此符号。</li>
</ul>

<h2 id="toc_10">空格 : 美化公式</h2>

<p>紧贴<code>a!b</code> : \(a!b\)<br/>
  没有空格<code>ab</code> : \(ab\)<br/>
  小空格<code>a\,b</code> : \(a\,b\)<br/>
  中等空格<code>a\;b</code> : \(a\;b\)<br/>
  大空格<code>a\ b</code> : \(a\ b\)<br/>
  quad空格<code>a\quad b</code> : \(a\quad b\)<br/>
  两个quad空格<code>a\qquad b</code> : \(a\qquad b\)</p>

<p>原公式:<code>\int_a^b f(x)\mathrm{d}x</code><br/>
  \(\int_a^b f(x)\mathrm{d}x\)</p>

<p>插入空格:<code>\int_a^b f(x)\qquad \mathrm{d}x</code><br/>
  \(\int_a^b f(x)\qquad \mathrm{d}x\)</p>

<h2 id="toc_11">括号</h2>

<ol>
<li>小括号与方括号：使用原始的<code>( )</code>，<code>[ ]</code>即可，如<code>(2+3)[4+4]</code>：\((2+3)[4+4]\)</li>
<li>大括号：时由于大括号{}被用来分组，使用\lbrace 和\rbrace来表示
<code>\lbrace a*b \rbrace</code> ：\(\lbrace a*b \rbrace\)</li>
<li>尖括号：使用<code>\langle</code> 和 <code>\rangle</code>表示左尖括号和右尖括号
<code>\langle x \rangle</code> ：\(\langle x \rangle\)</li>
<li>上取整：使用<code>\lceil</code> 和 <code>\rceil</code> 表示 
<code>\lceil x \rceil</code> ：\(\lceil x \rceil\)</li>
<li>下取整：使用<code>\lfloor</code> 和 <code>\rfloor</code> 表示
<code>\lfloor x \rfloor</code> ：\(\lfloor x \rfloor\)</li>
<li><p>不可见括号：使用<code>.</code>表示</p></li>
</ol>

<ul>
<li><p>注意 : 原始符号并不会随着公式大小缩放。如，<code>(\frac12)</code> ：\((\frac12)\)。可以使用\left(…\right)来自适应的调整括号大小。<br/>
如1.1和1.2公式，公式1.2中的括号是经过缩放的。</p>
<p><code>\lbrace \sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6} \rbrace \tag{1.1}</code><br/>
\[\lbrace\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}\rbrace\tag{1.1}\]</p>
<p><code>\left \lbrace \sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6} \right\rbrace \tag{1.2}</code><br/>
\[\left \lbrace \sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6} \right\rbrace\tag{1.2}\]</p></li>
<li><p>定界符之前冠以 \left（修饰左定界符）或 \right（修饰右定界符），可以得到自适应缩放的定界符，它们会根据定界符所包围的公式大小自适应缩放</p>
<p><code>\left( \sum_{k=\frac{1}{2}}^{N^2}\frac{1}{k} \right)</code><br/>
\[ \left( \sum_{k=\frac{1}{2}}^{N^2}\frac{1}{k} \right) \]</p></li>
<li><p>诸如()、[]、{}、|等分割公式的称为定界符，前面加上\big，\Big，\bigg，\Bigg可以放大这些符号，我比较喜欢用自适应的放大命令，<code>\left...\right</code>，例如</p>
<p><code>$$\left. \frac{\partial f(x, y)}{\partial x}\right|_{x=0}$$</code></p>
<p>\[\left. \frac{\partial f(x, y)}{\partial x}\right|_{x=0}\]</p></li>
</ul>

<h2 id="toc_12">字体</h2>

<ol>
<li>使用<code>\mathbb</code>或<code>\Bbb</code>显示黑板粗体字，此字体经常用来表示代表实数、整数、有理数、复数的大写字母。
<code>\mathbb{CHNQRZ}</code> : \(\mathbb{CHNQRZ}\)。</li>
<li>使用<code>\mathbf</code>显示黑体字
<code>\mathbf{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$，$\mathbf{abcdefghijklmnopqrstuvwxyz}</code>
\(\mathbf{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\)，\(\mathbf{abcdefghijklmnopqrstuvwxyz}\)</li>
<li>使用<code>\mathtt</code>显示打印机字体
<code>\mathtt{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$，$\mathtt{abcdefghijklmnopqrstuvwxyz}</code>
\(\mathtt{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\)，\(\mathtt{abcdefghijklmnopqrstuvwxyz}\)</li>
<li>使用<code>\mathrm</code>显示罗马字体
<code>\mathrm{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$，$\mathrm{abcdefghijklmnopqrstuvwxyz}</code>
\(\mathrm{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\)，\(\mathrm{abcdefghijklmnopqrstuvwxyz}\)</li>
<li>使用<code>\mathscr</code>显示手写体, 无小写
<code>\mathscr{ABCDEFGHIJKLMNOPQRSTUVWXYZ}, $\mathscr{abcdefghijklmnopqrstuvwxyz}</code>
\(\mathscr{ABCDEFGHIJKLMNOPQRSTUVWXYZ}, \)\mathscr{abcdefghijklmnopqrstuvwxyz}$</li>
<li>使用<code>\mathfrak</code>显示Fraktur字母（一种德国字体）
<code>\mathfrak{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$, $\mathfrak{abcdefghijklmnopqrstuvwxyz}</code>
\(\mathfrak{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\),\(\mathfrak{abcdefghijklmnopqrstuvwxyz}\)</li>
</ol>

<h2 id="toc_13">方程组</h2>

<ul>
<li><p>使用<code>\begin{array} ... \end{array}</code>与<code>\left\{…\right.</code>配合，表示方程组</p></li>
<li><p>使用 <code>\being{aligned} .. \end{aligned}</code></p></li>
</ul>

<p>\[<br/>
\begin{aligned}<br/>
\dot{x} &amp; = \sigma(y-x) \\\<br/>
\dot{y} &amp; = \rho x - y - xz \\\<br/>
\dot{z} &amp; = -\beta z + xy<br/>
\end{aligned}<br/>
\]</p>

<ul>
<li>使用 <code>\being{case} .. \end{case}</code></li>
</ul>

<p>\[<br/>
\begin{cases}<br/>
a_1x+b_1y+c_1z=d_1 \\\<br/>
a_2x+b_2y+c_2z=d_2 \\\<br/>
a_3x+b_3y+c_3z=d_3<br/>
\end{cases}<br/>
\]</p>

<ul>
<li>对齐方程组中的<code>=</code>号, 注意<code>{</code>要转义,&#39;\{&#39;</li>
</ul>

<p>\[ \left\\{<br/>
     \begin{aligned} <br/>
       a_1x+b_1y+c_1z &amp;=d_1+e_1 \\\<br/>
       a_2x+b_2y&amp;=d_2 \\\ <br/>
       a_3x+b_3y+c_3z &amp;=d_3 <br/>
\end{aligned} <br/>
\right. <br/>
\]</p>

<ul>
<li>如果要对齐 <code>=</code>号和<code>项</code>，可以使用<code>\being{array}{列样式} .. \end{array}</code></li>
</ul>

<p>\[<br/>
\left\\{<br/>
\begin{array}{ll}<br/>
a_1x+b_1y+c_1z &amp;=d_1+e_1 \\\<br/>
a_2x+b_2y &amp;=d_2 \\\<br/>
a_3x+b_3y+c_3z &amp;=d_3 <br/>
\end{array} <br/>
\right.<br/>
\]</p>

<h3 id="toc_14">上下文字</h3>

<p>\[ \mathop {\arg min }\limits_{(w,b)}^{top} f(x)\]</p>

<h3 id="toc_15">公式标记与引用</h3>

<ul>
<li>使用<code>\tag{tag}</code>来标记公式，如果想在之后引用该公式，则还需要加上<code>\label{label}</code>在<code>\tag</code>之后，如：</li>
<li>需要<code>\*</code>对标签<code>*</code>进行转义</li>
</ul>

<p>\[<br/>
 a := x^2-y^3 \tag{\*}\label{\*}<br/>
\]</p>

<p>\[<br/>
 a := x^2-y^3 \tag{2.1}\label{2.1}<br/>
\]</p>

<ul>
<li>为了引用公式，可以使用<code>\eqref{rlabel}</code>，如：</li>
</ul>

<p>\[a+y^3 \stackrel{\eqref{\*}}= x^2\]</p>

<p>\[a+y^3 \stackrel{\eqref{2.1}}= x^2\]</p>

<ul>
<li>通过超链接可以跳转到被引用公式位置。</li>
</ul>

<h3 id="toc_16"><strong>矩阵</strong></h3>

<ul>
<li><p>使用<code>$$\begin{matrix}…\end{matrix}$$</code>表示矩阵，矩阵的行之间使用<code>\\\</code>分隔，列之间使用<code>&amp;</code>分隔。</p></li>
<li><p>效果如下 :</p></li>
</ul>

<p>\[<br/>
        \begin{matrix}<br/>
        1 &amp; x &amp; x^2 \\\<br/>
        1 &amp; y &amp; y^2 \\\<br/>
        1 &amp; z &amp; z^2 \\\<br/>
        \end{matrix}<br/>
\]</p>

<ul>
<li><strong>加括号</strong>
如果要对矩阵加括号，可以像上文中提到的一样，使用<code>\left</code>与<code>\right</code>配合表示括号符号。也可以使用特殊的<code>matrix</code>。即替换<code>\begin{matrix}...\end{matrix}</code>中的matrix为<code>pmatrix</code>，<code>bmatrix</code>，<code>Bmatrix</code>，<code>vmatrix</code>,<code>Vmatrix</code></li>
</ul>

<p><code>pmatrix</code> : \(\begin{pmatrix}1&amp;2\\\3&amp;4\\\ \end{pmatrix}\) <br/>
<code>bmatrix</code> : \(\begin{bmatrix}1&amp;2\\\3&amp;4\\\ \end{bmatrix}\) <br/>
<code>Bmatrix</code> : \(\begin{Bmatrix}1&amp;2\\\3&amp;4\\\ \end{Bmatrix}\) <br/>
<code>vmatrix</code> : \(\begin{vmatrix}1&amp;2\\\3&amp;4\\\ \end{vmatrix}\) <br/>
<code>Vmatrix</code> : \(\begin{Vmatrix}1&amp;2\\\3&amp;4\\\ \end{Vmatrix}\) </p>

<ul>
<li><strong>省略元素</strong>  使用<code>\cdots</code> : \(\cdots\) <code>\ddots</code> : \(\ddots\)  <code>\vdots</code> : \(\vdots\)来省略矩阵中的元素</li>
</ul>

<p>\[<br/>
    \begin{pmatrix}<br/>
         1 &amp; a_1 &amp; a_1^2 &amp; \cdots &amp; a_1^n \\\<br/>
         1 &amp; a_2 &amp; a_2^2 &amp; \cdots &amp; a_2^n \\\<br/>
         \vdots  &amp; \vdots&amp; \vdots &amp; \ddots &amp; \vdots \\\<br/>
         1 &amp; a_m &amp; a_m^2 &amp; \cdots &amp; a_m^n <br/>
    \end{pmatrix}<br/>
\]</p>

<h3 id="toc_17"><strong>增广矩阵</strong></h3>

<ul>
<li>增广矩阵需要使用前面的<code>array</code>来实现</li>
</ul>

<p>\[ \left[<br/>
      \begin{array}{cc|c}<br/>
        1&amp;2&amp;3\\\<br/>
        4&amp;5&amp;6<br/>
      \end{array}<br/>
    \right]<br/>
\]</p>

<h3 id="toc_18"><strong>对齐公式</strong></h3>

<ul>
<li>使用形如<code>\begin{align}…\end{align}</code>的格式，使用&amp;来指示需要对齐的位置</li>
</ul>

<p>\[<br/>
    \begin{align}<br/>
    \sqrt{37} &amp; = \sqrt{\frac{73^2-1}{12^2}} \\\<br/>
     &amp; = \sqrt{\frac{73^2}{12^2}\cdot\frac{73^2-1}{73^2}} \\\<br/>
     &amp; = \sqrt{\frac{73^2}{12^2}}\sqrt{\frac{73^2-1}{73^2}} \\\<br/>
     &amp; = \frac{73}{12}\sqrt{1 - \frac{1}{73^2}} \\\<br/>
     &amp; \approx \frac{73}{12}\left(1 - \frac{1}{2\cdot73^2}\right)<br/>
    \end{align}<br/>
\]</p>

<h3 id="toc_19"><strong>分类表达式</strong></h3>

<ul>
<li>定义函数时分情况给出表达式，使用<code>\begin{cases}…\end{cases}</code></li>
<li>使用<code>\\\</code>来分类，使用<code>&amp;</code>指示需要对齐的位置</li>
</ul>

<p>\[<br/>
 f(n) =<br/>
    \begin{cases}<br/>
    n/2,  &amp; \text{if $n$ is even} \\\\[2ex]<br/>
    3n+1, &amp; \text{if $n$ is odd}<br/>
    \end{cases}<br/>
\]</p>

<ul>
<li><code>[4ex]</code>控制分类之间的垂直间隔, 这里要用<code>\\\\</code>转义</li>
</ul>

<h3 id="toc_20"><strong>表格</strong></h3>

<ul>
<li>使用<code>$$\begin{array}{列样式}...\end{array}$$</code>创建表格</li>
<li>列样式使用<code>clr</code>表示居中，左，右对齐</li>
<li>使用<code>|</code>表示一条竖线</li>
<li>使用<code>\\\\</code>分隔行，使用<code>&amp;</code>分隔列</li>
<li>使用<code>\hline</code>在本行前加入一条直线</li>
</ul>

<p>\[<br/>
    \begin{array}{c|lcr}<br/>
    n &amp; \text{Left} &amp; \text{Center} &amp; \text{Right} \\\\<br/>
    \hline<br/>
    1 &amp; 0.24 &amp; 1 &amp; 125 \\\\<br/>
    2 &amp; -1 &amp; 189 &amp; -8 \\\\<br/>
    3 &amp; -20 &amp; 2000 &amp; 1+10i \\\\<br/>
    \end{array}<br/>
\]</p>

<ul>
<li>复杂列表</li>
</ul>

<h3 id="toc_21"><strong>希腊字母</strong></h3>

<table>
<thead>
<tr>
<th>名称</th>
<th>大写</th>
<th>Tex</th>
<th>小写</th>
<th>Tex</th>
</tr>
</thead>

<tbody>
<tr>
<td>alpha</td>
<td>\(A\)</td>
<td>A</td>
<td>\(\alpha\)</td>
<td>\alpha</td>
</tr>
<tr>
<td>beta</td>
<td>\(B\)</td>
<td>B</td>
<td>\(\beta\)</td>
<td>\beta</td>
</tr>
<tr>
<td>gamma</td>
<td>\(\Gamma\)</td>
<td>\Gamma</td>
<td>\(\gamma\)</td>
<td>\gamma</td>
</tr>
<tr>
<td>delta</td>
<td>\(\Delta\)</td>
<td>\Delta</td>
<td>\(\delta\)</td>
<td>\delta</td>
</tr>
<tr>
<td>epsilon</td>
<td>\(E\)</td>
<td>E</td>
<td>\(\epsilon\)</td>
<td>\epsilon</td>
</tr>
<tr>
<td>zeta</td>
<td>\(Z\)</td>
<td>Z</td>
<td>\(\zeta\)</td>
<td>\zeta</td>
</tr>
<tr>
<td>eta</td>
<td>\(H\)</td>
<td>H</td>
<td>\(\eta\)</td>
<td>\eta</td>
</tr>
<tr>
<td>theta</td>
<td>\(\Theta\)</td>
<td>\Theta</td>
<td>\(\theta\)</td>
<td>\theta</td>
</tr>
<tr>
<td>iota</td>
<td>\(I\)</td>
<td>I</td>
<td>\(\iota\)</td>
<td>\iota</td>
</tr>
<tr>
<td>kappa</td>
<td>\(K\)</td>
<td>K</td>
<td>\(\kappa\)</td>
<td>\kappa</td>
</tr>
<tr>
<td>lambda</td>
<td>\(\Lambda\)</td>
<td>\Lambda</td>
<td>\(\lambda\)</td>
<td>\lambda</td>
</tr>
<tr>
<td>mu</td>
<td>\(M\)</td>
<td>M</td>
<td>\(\mu\)</td>
<td>\mu</td>
</tr>
<tr>
<td>nu</td>
<td>\(N\)</td>
<td>N</td>
<td>\(\nu\)</td>
<td>\nu</td>
</tr>
<tr>
<td>xi</td>
<td>\(\Xi\)</td>
<td>\Xi</td>
<td>\(\xi\)</td>
<td>\xi</td>
</tr>
<tr>
<td>omicron</td>
<td>\(O\)</td>
<td>O</td>
<td>\(\omicron\)</td>
<td>\omicron</td>
</tr>
<tr>
<td>pi</td>
<td>\(\Pi\)</td>
<td>\Pi</td>
<td>\(\pi\)</td>
<td>\pi</td>
</tr>
<tr>
<td>rho</td>
<td>\(P\)</td>
<td>P</td>
<td>\(\rho\)</td>
<td>\rho</td>
</tr>
<tr>
<td>sigma</td>
<td>\(\Sigma\)</td>
<td>\Sigma</td>
<td>\(\sigma\)</td>
<td>\sigma</td>
</tr>
<tr>
<td>tau</td>
<td>\(T\)</td>
<td>T</td>
<td>\(\tau\)</td>
<td>\tau</td>
</tr>
<tr>
<td>upsilon</td>
<td>\(\Upsilon\)</td>
<td>\Upsilon</td>
<td>\(\upsilon\)</td>
<td>\upsilon</td>
</tr>
<tr>
<td>phi</td>
<td>\(\Phi\)</td>
<td>\Phi</td>
<td>\(\phi\)</td>
<td>\phi</td>
</tr>
<tr>
<td>chi</td>
<td>\(X\)</td>
<td>X</td>
<td>\(\chi\)</td>
<td>\chi</td>
</tr>
<tr>
<td>psi</td>
<td>\(\Psi\)</td>
<td>\Psi</td>
<td>\(\psi\)</td>
<td>\psi</td>
</tr>
<tr>
<td>omega</td>
<td>\(\Omega \)</td>
<td>\Omega</td>
<td>\(\omega\)</td>
<td>\omega</td>
</tr>
</tbody>
</table>

<h3 id="toc_22"><strong>重音符号</strong></h3>

<ul>
<li><p><code>\hat{A}</code> : \( \hat{A} \)</p></li>
<li><p>单字符<br/>
<code>\hat</code> ：\(\hat x\)</p></li>
<li><p>多字符<br/>
<code>\widehat</code> : \(\widehat {xy}\)</p></li>
</ul>

<p><img src="media/15235878044126/15235930010630.jpg" alt=""/></p>

<h3 id="toc_23"><strong>二元关系</strong></h3>

<ul>
<li><code>a\ll{b}</code> : \( a\ll{b} \)</li>
</ul>

<p><img src="media/15235878044126/15235930296134.jpg" alt=""/></p>

<h3 id="toc_24"><strong>二元运算符</strong></h3>

<ul>
<li><code>\pm</code> : \( \pm \)</li>
</ul>

<p><img src="media/15235878044126/15235930579186.jpg" alt=""/></p>

<h3 id="toc_25"><strong>&quot;大&quot;运算符</strong></h3>

<ul>
<li><code>\sum</code> : \( \sum \)</li>
</ul>

<p><img src="media/15235878044126/15235931068818.jpg" alt=""/></p>

<h3 id="toc_26"><strong>箭头</strong></h3>

<ul>
<li><code>\to</code> : \( \to \)</li>
</ul>

<p><img src="media/15235878044126/15235931161176.jpg" alt=""/></p>

<h3 id="toc_27"><strong>定界符</strong></h3>

<ul>
<li><code>\lbrack</code> : \( \lbrack \)</li>
</ul>

<p><img src="media/15235878044126/15235931287395.jpg" alt=""/></p>

<h3 id="toc_28"><strong>大定界符</strong></h3>

<ul>
<li><code>\lgroup</code> : \( \lgroup \)</li>
</ul>

<p><img src="media/15235878044126/15235931644817.jpg" alt=""/></p>

<h3 id="toc_29"><strong>其他符号</strong></h3>

<ul>
<li><code>\dots</code> : \( \dots \)</li>
</ul>

<p><img src="media/15235878044126/15235931801879.jpg" alt=""/></p>

<h3 id="toc_30"><strong>注意事项</strong></h3>

<ul>
<li><p>不要在再指数或者积分中使用 <code>\frac</code></p></li>
<li><p>在指数或者积分表达式中使用<code>\frac</code>会使表达式看起来不清晰，应该使用一个水平的<code>/</code>来代替</p></li>
<li><p>使用 \mid 代替 | 作为分隔符<br/>
符号|作为分隔符时有排版空间大小的问题，应该使用\mid代替。效果如下：</p></li>
</ul>

<p>\[<br/>
\begin{array}<br/>
\mathrm{Bad} &amp; \mathrm{Better} \\\<br/>
\hline \\\<br/>
{x|x^2\in\Bbb Z} &amp; {x\mid x^2\in\Bbb Z} \\\<br/>
\end{array}<br/>
\]</p>

<ul>
<li>连分数<br/>
书写连分数表达式时，请使用<code>\cfrac</code>代替<code>\frac</code>或者<code>\over</code>,两者效果对比如下：</li>
</ul>

<p>\[<br/>
x = a_0 + \cfrac{1^2}{a_1<br/>
          \+ \cfrac{2^2}{a_2<br/>
          \+ \cfrac{3^2}{a_3 + \cfrac{4^4}{a_4 + \cdots}}}} \tag{\cfrac}<br/>
\]<br/>
\[<br/>
x = a_0 + \frac{1^2}{a_1<br/>
         \+ \frac{2^2}{a_2<br/>
          \+ \frac{3^2}{a_3 + \frac{4^4}{a_4 + \cdots}}}} \tag{\frac}<br/>
\]</p>

<blockquote>
<p><strong>转义</strong><br/>
  一些MathJax使用的特殊字符，可以使用<code>\</code>或者<code>\\</code>转义为原来的含义。如<code>\$</code>表示<code>$</code>，<code>\\_</code>表示下划线。</p>

<ul>
<li>注意: 换行和方程组的开始<code>{</code>等, 常常需要转义, 用<code>\</code>或者<code>\\</code>或者<code>\\\</code>或者<code>\\\\</code>
<a href="https://github.com/Simshang/blog/blob/master/source/_posts/Demo-LaTeX.md">Markdown文件</a></li>
</ul>
</blockquote>

<h3 id="toc_31">转载参考:</h3>

<ul>
<li><a href="http://simtalk.cn/2015/12/21/Demo-LaTeX/">http://simtalk.cn/2015/12/21/Demo-LaTeX/</a></li>
<li><a href="https://www.zybuluo.com/codeep/note/163962">https://www.zybuluo.com/codeep/note/163962</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如何优雅的设计数据分层]]></title>
    <link href="http://dmlcoding.com/15228352753621.html"/>
    <updated>2018-04-04T17:47:55+08:00</updated>
    <id>http://dmlcoding.com/15228352753621.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">理论</h1>

<p><img src="media/15228352753621/15231557165878.jpg" alt=""/></p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">1.ODS 全称是 Operational Data Store，操作数据存储</h2>

<p>“面向主题的”，数据运营层，也叫ODS层，是最接近数据源中数据的一层，数据源中的数据，经过抽取、洗净、传输，也就说传说中的 ETL 之后，装入本层。本层的数据，总体上大多是按照源头业务系统的分类方式而分类的。</p>

<p>但是，这一层面的数据却不等同于原始数据。在源数据装入这一层时，要进行诸如去噪（例如有一条数据中人的年龄是 300 岁，这种属于异常数据，就需要提前做一些处理）、去重（例如在个人资料表中，同一 ID 却有两条重复数据，在接入的时候需要做一步去重）、字段命名规范等一系列操作。</p>

<h2 id="toc_2">2.数据仓库层(DW)，是数据仓库的主体</h2>

<p>在这里，从 ODS 层中获得的数据按照主题建立各种数据模型。这一层和维度建模会有比较深的联系.</p>

<h2 id="toc_3">3.数据产品层（APP），这一层是提供为数据产品使用的结果数据</h2>

<p>在这里，主要是提供给数据产品和数据分析使用的数据，一般会存放在 ES、Mysql 等系统中供线上系统使用，也可能会存在 Hive 或者 Druid 中供数据分析和数据挖掘使用。<br/>
比如我们经常说的报表数据，或者说那种大宽表，一般就放在这里</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[大数定理&中心极限定理]]></title>
    <link href="http://dmlcoding.com/15264345359411.html"/>
    <updated>2018-05-16T09:35:35+08:00</updated>
    <id>http://dmlcoding.com/15264345359411.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">大数定理</h1>

<h2 id="toc_1">概念阐述</h2>

<p>大数定律的意义:随着样本容量n的增加,样本平均数将接近于总体平均数(期望\(\mu\)),所以在统计推断中,一般都会使用样本平均数估计总体平均数的值.</p>

<span id="more"></span><!-- more -->

<p>也就是我们会使用一部分样本的平均值来代替整体样本的期望/均值,出现偏差的可能性是存在的,但是当n足够大的时候,偏差的可能性是非常小的,当n无限大的时候,这种可能性的概率基本为0.</p>

<p>大数定律的主要作用就是为使用频率来估计概率提供了理论支持;为使用部分数据来近似的模拟构建全部数据的特征提供了理论支持.</p>

<h2 id="toc_2">代码实验</h2>

<pre class="line-numbers"><code class="language-text"># -*- coding: utf-8 -*-
&#39;&#39;&#39;
    Created by hushiwei on 2018/4/1.

    desc: 大数定理展示理解
&#39;&#39;&#39;

import random
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

# 解决中文显示问题
mpl.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;]
mpl.rcParams[&#39;axes.unicode_minus&#39;]=False

random.seed(28)


def generate_random_int(n):
    &#39;&#39;&#39;
    产生n个1-9的随机数
    :param n:
    :return:
    &#39;&#39;&#39;
    return [random.randint(1,9) for i in range(n)]

if __name__ == &#39;__main__&#39;:
    number=8000
    x=[i for i in range(number+1) if i!=0]

    # 产生number个[1,9]的随机数
    total_random_int=generate_random_int(number)

    # 求n个[1,9]的随机数的均值,n=1,2,3,4,5...
    # 求这number个数的均值,比如1个元素的均值,2个元素的均值,3个元素的均值,4个元素的均值,一直到最后
    y=[np.mean(total_random_int[0:i+1]) for i in range(number)]


    plt.plot(x,y,&#39;b-&#39;)
    plt.xlim(0,number)
    plt.grid(True)
    plt.show()
</code></pre>

<h2 id="toc_3">可视化展示原理</h2>

<p><img src="media/15264345359411/15265238560825.jpg" alt=""/></p>

<h1 id="toc_4">中心极限定理</h1>

<h2 id="toc_5">概念阐述</h2>

<p>中心极限定理(Central Limit Theorem).假设\({X_n}\)为独立同分布的随机变量序列,并具有相同的期望\(\mu\)和方差为\(\sigma^2\),则\({X_n}\)服从中心极限定理,且\(Z_n\)为随机序列\({X_n}\)的规范和.</p>

<p>\[<br/>
\begin{aligned}<br/>
Y_n &amp; =X_1+X_2+ \cdots +X_n \\\<br/>
&amp; = \sum_{i=1}^nX_i \to N(n\mu,n\sigma^2)<br/>
\end{aligned}<br/>
\]</p>

<p>\[Z_n=\dfrac{Y_n-E(Y_n)}{\sqrt{D(Y_n)}}=\dfrac{Y_n-n\mu }{\sqrt{n}{\sigma}} \to N(0,1)\]</p>

<p>中心极限定理就是一般在同分布的情况下,抽样样本值的规范和在总体数量趋于无穷时的极限分布近似于正态分布.</p>

<h2 id="toc_6">代码实验</h2>

<pre class="line-numbers"><code class="language-text"># -*- coding: utf-8 -*-
&#39;&#39;&#39;
    Created by hushiwei on 2018/4/1.
    desc :中心极限定理

    随机的抛六面的骰子,计算三次的点数的和,三次点数的和其实就是一个事件A

    问题: 事件A的发生属于什么分布呢?

    分析: A=x1+x2+x3,其中x1,x2,x3是分别三次抛骰子的点数
    根据中心极限定理,由于x1,x2,x3属于独立同分布的,因此最终的事件A属于高斯分布
&#39;&#39;&#39;

import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


def generate_random_int():
    &#39;&#39;&#39;
    随机产生一个[1,6]的数字,模拟随机抛六面骰子的结果
    :return:
    &#39;&#39;&#39;
    return random.randint(1,6)


def generate_sum(n):
    &#39;&#39;&#39;
    计算返回n次抛六面骰子的和结果

    也既是模拟A事件
    :param n:
    :return:
    &#39;&#39;&#39;
    return np.sum([generate_random_int() for i in range(n)])


if __name__ == &#39;__main__&#39;:
    # 进行A事件多少次
    number1=10000000

    # 表示每次A事件抛几次骰子
    number2=3

    # 进行number1次事件A的操作,每次事件A都进行number2次抛骰子
    keys=[generate_sum(number2) for i in range(number1)]

    # 统计每个和数字出现的次数,eg:和为3的出现多少次,和为10出现多少次...
    result={}
    for key in keys:
        count=1
        if key in result:
            count+=result[key]
        result[key]=count

    # 获取x和y
    x=sorted(np.unique(list(result.keys())))
    y=[]
    for key in x:
        # 将出现的次数进行一个百分比的计算
        y.append(result[key]/number1)

    # 画图,可视化展示

    plt.plot(x,y,&#39;b-&#39;)
    plt.xlim(x[0]-1,x[-1]+1)
    plt.grid(True)
    plt.show()
</code></pre>

<h2 id="toc_7">可视化展示原理</h2>

<p><img src="media/15264345359411/15265238226274.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark-Metrics 实战]]></title>
    <link href="http://dmlcoding.com/15224029526508.html"/>
    <updated>2018-03-30T17:42:32+08:00</updated>
    <id>http://dmlcoding.com/15224029526508.html</id>
    <content type="html"><![CDATA[
<p><a href="http://metrics.dropwizard.io/3.1.0/getting-started/">http://metrics.dropwizard.io/3.1.0/getting-started/</a><br/>
<a href="http://metrics.dropwizard.io/3.1.0/manual/core/">http://metrics.dropwizard.io/3.1.0/manual/core/</a><br/>
<a href="http://wuchong.me/blog/2015/08/01/getting-started-with-metrics/">http://wuchong.me/blog/2015/08/01/getting-started-with-metrics/</a></p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">使用</h2>

<p>The –files flag will cause /path/to/metrics.properties to be sent to every executor,<br/>
and spark.metrics.conf=metrics.properties will tell all executors to load that file<br/>
when initializing their respective MetricsSystems.</p>

<p>直接添加到命令行后</p>

<pre class="line-numbers"><code class="language-text">--files=/yourPath/metrics.properties 
--conf spark.metrics.conf=metrics.properties

</code></pre>

<h2 id="toc_1">小示例</h2>

<p>新建<code>metrics.properties</code>文件,内容如下</p>

<pre class="line-numbers"><code class="language-text">
driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource
executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource

*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink
*.sink.console.period=10
*.sink.console.unit=seconds
</code></pre>

<p>全是driver端的信息</p>

<p><img src="media/15224029526508/15224033134708.jpg" alt=""/></p>

<h3 id="toc_2">jmx-sink</h3>

<pre class="line-numbers"><code class="language-text">master.source.jvm.class=org.apache.spark.metrics.source.JvmSource
worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource
driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource
executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink
</code></pre>

<p>spark on yarn 查看 </p>

<p><a href="http://u007:8089/proxy/application_1509616075703_478985/metrics/json">http://u007:8089/proxy/application_1509616075703_478985/metrics/json</a></p>

<h2 id="toc_3">总结</h2>

<p>source里面定义从哪些实例来监控</p>

<p>sink指定你要输出到哪里</p>

<p>刚才这个例子里面,就是sink输出到console</p>

]]></content>
  </entry>
  
</feed>
