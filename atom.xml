<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Time渐行渐远]]></title>
  <link href="http://dmlcoding.com/atom.xml" rel="self"/>
  <link href="http://dmlcoding.com/"/>
  <updated>2019-03-08T18:31:12+08:00</updated>
  <id>http://dmlcoding.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Neo4j导入方式]]></title>
    <link href="http://dmlcoding.com/15520132048516.html"/>
    <updated>2019-03-08T10:46:44+08:00</updated>
    <id>http://dmlcoding.com/15520132048516.html</id>
    <content type="html"><![CDATA[
<p>初次使用Neo4j图数据库时,我们可能需要将在关系数据库或其他存储形式的数据批量导入到Neo4j图数据库中.因此,这里我将介绍怎样使用Neo4j导入工具来批量导入数据.<br/>
目前,导入到Neo4j的工具有两种:</p>

<ol>
<li><code>load csv</code>指令.</li>
<li><code>neo4j-import</code>工具.</li>
</ol>

<span id="more"></span><!-- more -->

<p>然而,这两种工具都是基于CSV文件的,那么我们就需要先有CSV文件了.这里我不详细阐述如何获取CSV文件了.</p>

<h1 id="toc_0">关系数据库导出为CSV</h1>

<ol>
<li>借助GUI数据库管理工具,比如<code>DataGrip</code>,<code>Navicat</code>等等工具,都有将表数据导出为CSV文件的功能.</li>
<li>借助数据库查询语句导出指令.比如<code>mysql</code>的<code>into outfile &lt;导出的目录和文件名&gt;</code>.等等之类的了.</li>
</ol>

<h1 id="toc_1">CSV内容格式注意事项</h1>

<h1 id="toc_2">使用Cypher create语句,为每一条数据写一个create</h1>

<h1 id="toc_3">使用java api</h1>

<h1 id="toc_4">使用Load CSV指令导入到Neo4j</h1>

<p>以<code>权力的游戏</code>的数据作为测试数据<br/>
<a href="https://www.macalester.edu/%7Eabeverid/data/stormofswords.csv">下载地址</a></p>

<pre class="line-numbers"><code class="language-text">Source,Target,Weight
Aemon,Grenn,5
Aemon,Samwell,31
Aerys,Jaime,18
Aerys,Robert,6
Aerys,Tyrion,5
Aerys,Tywin,8
Alliser,Mance,5
Amory,Oberyn,5
...
</code></pre>

<h2 id="toc_5">Load CSV读取但不存入数据库</h2>

<p>注意:从本地加载文件的时候,相对路径(相对于)</p>

<pre class="line-numbers"><code class="language-text">bash-4.4# pwd
/var/lib/neo4j/import
bash-4.4# ls
relation.csv       stormofswords.csv
</code></pre>

<p>查询</p>

<pre class="line-numbers"><code class="language-text">// 查看CSV文件行数
load csv from &quot;file:///stormofswords.csv&quot; as line
 return count(*)
</code></pre>

<p>输出<br/>
<img src="https://raw.githubusercontent.com/Timehsw/gitnote-images/master/neo4j/import/import_read_csv.png" alt=""/></p>

<pre class="line-numbers"><code class="language-text">// 查看CSV文件前5行
load csv from &quot;file:///stormofswords.csv&quot; as line
with line
return line 
limit 5;
</code></pre>

<p><img src="https://raw.githubusercontent.com/Timehsw/gitnote-images/master/neo4j/import/read_csv_top5.png" alt=""/></p>

<pre class="line-numbers"><code class="language-text">// 查看CSV文件,并带有头部数据
load csv with headers from &quot;file:///stormofswords.csv&quot; as line
with line
return line 
limit 5;
</code></pre>

<p><img src="https://raw.githubusercontent.com/Timehsw/gitnote-images/master/neo4j/import/read_csv_with_headers.png" alt=""/></p>

<p>这几个例子仅仅是用来读取CSV文件,它并没有将数据存入到neo4j的数据库中.</p>

<h2 id="toc_6">LOAD CSV指令用法</h2>

<p><code>load csv from &quot;file-url&quot; as line</code> </p>

<ol>
<li>这条指令就是将指定路径下的CSV文件读取出来</li>
<li>其中的&quot;file-url&quot;就是文件的地址,可以是本地文件路径也可以是网址,只要能从地址中读取到CSV文件即可.</li>
<li>因此也可以这样写.</li>
</ol>

<pre class="line-numbers"><code class="language-text"> LOAD CSV FROM &quot;https://www.macalester.edu/~abeverid/data/stormofswords.csv&quot; as line
 return line
</code></pre>

<p>这样就可以读取网址指定的movie.csv文件.<br/>
或者可以使用本地文件路径.</p>

<pre class="line-numbers"><code class="language-text">// 由于配置文件
conf/neo4j.conf
这个配置项
dbms.directories.import=import
</code></pre>

<p>所以读取本地文件的时候,我们需要把文件放到import目录下</p>

<h2 id="toc_7">导入CSV时附带表头</h2>

<p>我们将使用一个简单的数据模型<code>(:Character {name})-[:INTERACTS {weight}]-&gt;(:Character {name})</code>。带有标签的节点Character表示文本中的角色，单个关系类型INTERACTS从该角色连接到另一个文本中交互的角色。我们将把角色名作为node属性name存储，把两个角色之间的交互数作为relationships属性weight存储。</p>

<p>首先，我们必须创建一个约束来断言我们架构的完整性：</p>

<pre class="line-numbers"><code class="language-text">CREATE CONSTRAINT ON (c:Character) ASSERT c.name IS UNIQUE;
</code></pre>

<p>一旦创建了约束（该语句也将构建一个索引，它将提高通过角色名查找的性能），我们可以使用Cypher的LOAD CSV语句来导入数据：</p>

<p>头部由<code>row.Source</code>,<code>tow.Target</code>指定</p>

<pre class="line-numbers"><code class="language-text">LOAD CSV WITH HEADERS FROM &quot;file:///stormofswords.csv&quot; AS row
MERGE (src:Character {name: row.Source})
MERGE (tgt:Character {name: row.Target})
MERGE (src)-[r:INTERACTS]-&gt;(tgt)
ON CREATE SET r.weight = toInt(row.Weight)
return *
</code></pre>

<p><img src="https://raw.githubusercontent.com/Timehsw/gitnote-images/master/neo4j/import/stormofswords.png" alt=""/></p>

<h2 id="toc_8">导入CSV大文件</h2>

<p>如果要导入包含大量数据的CSV文件,则可以使用<code>PERIODIC COMMIT字句</code>.</p>

<ol>
<li>使用<code>PERIODIC COMMIT</code>指示neo4j在执行完一定行数后提交数据再继续,这样减少了内存开销.</li>
<li>使用<code>PERIODIC COMMIT</code>默认值为1000行,因此数据将每一千行提交一次.</li>
<li>要使用<code>PERIODIC COMMIT</code>,只需要在LOAD CSV 语句之前插入<code>USING PERIODIC COMMIT</code>语句</li>
</ol>

<pre class="line-numbers"><code class="language-text">USING PERIODIC COMMIT
LOAD CSV WITH HEADERS FROM &quot;file:///stormofswords.csv&quot; AS row
MERGE (src:Character {name: row.Source})
MERGE (tgt:Character {name: row.Target})
MERGE (src)-[r:INTERACTS]-&gt;(tgt)
ON CREATE SET r.weight = toInt(row.Weight)
</code></pre>

<p>可以让其每800行提交一次,如下所示.</p>

<pre class="line-numbers"><code class="language-text">USING PERIODIC COMMIT 800
LOAD CSV WITH HEADERS FROM &quot;file:///stormofswords.csv&quot; AS row
MERGE (src:Character {name: row.Source})
MERGE (tgt:Character {name: row.Target})
MERGE (src)-[r:INTERACTS]-&gt;(tgt)
ON CREATE SET r.weight = toInt(row.Weight)
</code></pre>

<h1 id="toc_9">使用neo4j-import 工具导入到Neo4j</h1>

<p>从neo4j 2.2版本开始,系统就自带了一个大数据量的导入工具<code>neo4j-import</code>,可支持并行,可扩展的大规模CSV数据导入.</p>

<pre class="line-numbers"><code class="language-text">bash-4.4# pwd
/var/lib/neo4j/bin
bash-4.4# ls
cypher-shell  neo4j         neo4j-admin   neo4j-import  tools
</code></pre>

<h2 id="toc_10">neo4j-import用法</h2>

<p>然而却提示neo4j-import过时了,推荐用新的neo4j-admin了.</p>

<pre class="line-numbers"><code class="language-text">bash-4.4# bin/neo4j-import
WARNING: neo4j-import is deprecated and support for it will be removed in a future
version of Neo4j; please use neo4j-admin import instead.
Neo4j Import Tool
    neo4j-import is used to create a new Neo4j database from data in CSV files. See
    the chapter &quot;Import Tool&quot; in the Neo4j Manual for details on the CSV file format
    - a special kind of header is required.
</code></pre>

<p>看了一下neo4j-admin的帮助文档,其实import功能只是一个子集罢了.admin里面包含了更多的功能.</p>

<h2 id="toc_11">1. phones.csv 记录电话号列表，作为nodes结点</h2>

<p><img src="https://raw.githubusercontent.com/Timehsw/gitnote-images/master/neo4j/import/phone_nums.png" alt=""/></p>

<h2 id="toc_12">2. phone_header.csv 标题文件只有一行数据</h2>

<pre class="line-numbers"><code class="language-text">phone:ID
</code></pre>

<h2 id="toc_13">3. call.csv 该文件记录通话记录的信息，作为以后关系的建立和关系属性的添加</h2>

<p><img src="https://raw.githubusercontent.com/Timehsw/gitnote-images/master/neo4j/import/phone_call.png" alt=""/></p>

<h2 id="toc_14">4. call_header.csv 通话记录头信息</h2>

<pre class="line-numbers"><code class="language-text">:START_ID,:END_ID,duration,num,average
</code></pre>

<h2 id="toc_15">5. CSV文件准备好了,开始执行导入</h2>

<p>注意:执行之前,要打开conf/neo4j.conf文件,将#dbms.directories.import=import注释.然后文件写绝对路径.不然容易报错</p>

<pre class="line-numbers"><code class="language-text">neo4j-admin import \
    --database=graph_demo.db \
    --nodes:phone=&quot;/var/lib/neo4j/import/phone_header.csv,phones.csv&quot; \
    --ignore-duplicate-nodes=true \
    --ignore-missing-nodes=true \
    --relationships:call=&quot;/var/lib/neo4j/import/call_header.csv,call.csv&quot;
</code></pre>

<ul>
<li>这里以防我们新建的数据库已经存在，我们选择删除已有库再进行导入</li>
<li><p>记得要先关闭neo4j</p></li>
<li><p>--nodes子句开头的CSV文件是节点CSV文件</p></li>
<li><p>--relationships开头的是关系CSV文件</p></li>
<li><p>--into子句指明了导入的neo4j数据库名称,其中不能包含现有数据库</p></li>
<li><p>--id-type子句指明了生成节点,关系的主键类型为string类型</p></li>
</ul>

<p>注意:由于neo4j-import工具不能使用Cypher语句创建节点,关系,所以需要为节点和关系分别提供不同的csv文件</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[neo4j案例]]></title>
    <link href="http://dmlcoding.com/15517601129865.html"/>
    <updated>2019-03-05T12:28:32+08:00</updated>
    <id>http://dmlcoding.com/15517601129865.html</id>
    <content type="html"><![CDATA[
<p><a href="https://neo4j.com/graphgist/credit-card-fraud-detection">官网案例</a></p>

<h1 id="toc_0">GraphGist: Credit Card Fraud Detection(使用图数据库进行盗刷欺诈侦测)</h1>

<h1 id="toc_1">数据准备</h1>

<p>建立节点 </p>

<span id="more"></span><!-- more -->

<h2 id="toc_2">建立用户节点</h2>

<pre class="line-numbers"><code class="language-text">// Create customers
CREATE (Paul:Person {id:&#39;1&#39;, name:&#39;Paul&#39;, gender:&#39;man&#39;, age:&#39;50&#39;})
CREATE (Jean:Person {id:&#39;2&#39;, name:&#39;Jean&#39;, gender:&#39;man&#39;, age:&#39;48&#39;})
CREATE (Dan:Person {id:&#39;3&#39;, name:&#39;Dan&#39;, gender:&#39;man&#39;, age:&#39;23&#39;})
CREATE (Marc:Person {id:&#39;4&#39;, name:&#39;Marc&#39;, gender:&#39;man&#39;, age:&#39;30&#39;})
CREATE (John:Person {id:&#39;5&#39;, name:&#39;John&#39;, gender:&#39;man&#39;, age:&#39;31&#39;})
CREATE (Zoey:Person {id:&#39;6&#39;, name:&#39;Zoey&#39;, gender:&#39;woman&#39;, age:&#39;52&#39;})
CREATE (Ava:Person {id:&#39;7&#39;, name:&#39;Ava&#39;, gender:&#39;woman&#39;, age:&#39;23&#39;})
CREATE (Olivia:Person {id:&#39;8&#39;, name:&#39;Olivia&#39;, gender:&#39;woman&#39;, age:&#39;58&#39;})
CREATE (Mia:Person {id:&#39;9&#39;, name:&#39;Mia&#39;, gender:&#39;woman&#39;, age:&#39;51&#39;})
CREATE (Madison:Person {id:&#39;10&#39;, name:&#39;Madison&#39;, gender:&#39;woman&#39;, age:&#39;37&#39;})

</code></pre>

<h2 id="toc_3">建立商品节点</h2>

<pre class="line-numbers"><code class="language-text">// Create merchants
CREATE (Amazon:Merchant {id:&#39;11&#39;, name:&#39;Amazon&#39;, street:&#39;2626 Wilkinson Court&#39;, address:&#39;San Bernardino, CA 92410&#39;})
CREATE (Abercrombie:Merchant {id:&#39;12&#39;, name:&#39;Abercrombie&#39;, street:&#39;4355 Walnut Street&#39;, age:&#39;San Bernardino, CA 92410&#39;})
CREATE (Wallmart:Merchant {id:&#39;13&#39;, name:&#39;Wallmart&#39;, street:&#39;2092 Larry Street&#39;, age:&#39;San Bernardino, CA 92410&#39;})
CREATE (MacDonalds:Merchant {id:&#39;14&#39;, name:&#39;MacDonalds&#39;, street:&#39;1870 Caynor Circle&#39;, age:&#39;San Bernardino, CA 92410&#39;})
CREATE (American_Apparel:Merchant {id:&#39;15&#39;, name:&#39;American Apparel&#39;, street:&#39;1381 Spruce Drive&#39;, age:&#39;San Bernardino, CA 92410&#39;})
CREATE (Just_Brew_It:Merchant {id:&#39;16&#39;, name:&#39;Just Brew It&#39;, street:&#39;826 Anmoore Road&#39;, age:&#39;San Bernardino, CA 92410&#39;})
CREATE (Justice:Merchant {id:&#39;17&#39;, name:&#39;Justice&#39;, street:&#39;1925 Spring Street&#39;, age:&#39;San Bernardino, CA 92410&#39;})
CREATE (Sears:Merchant {id:&#39;18&#39;, name:&#39;Sears&#39;, street:&#39;4209 Elsie Drive&#39;, age:&#39;San Bernardino, CA 92410&#39;})
CREATE (Soccer_for_the_City:Merchant {id:&#39;19&#39;, name:&#39;Soccer for the City&#39;, street:&#39;86 D Street&#39;, age:&#39;San Bernardino, CA 92410&#39;})
CREATE (Sprint:Merchant {id:&#39;20&#39;, name:&#39;Sprint&#39;, street:&#39;945 Kinney Street&#39;, age:&#39;San Bernardino, CA 92410&#39;})
CREATE (Starbucks:Merchant {id:&#39;21&#39;, name:&#39;Starbucks&#39;, street:&#39;3810 Apple Lane&#39;, age:&#39;San Bernardino, CA 92410&#39;})
CREATE (Subway:Merchant {id:&#39;22&#39;, name:&#39;Subway&#39;, street:&#39;3778 Tenmile Road&#39;, age:&#39;San Bernardino, CA 92410&#39;})
CREATE (Apple_Store:Merchant {id:&#39;23&#39;, name:&#39;Apple Store&#39;, street:&#39;349 Bel Meadow Drive&#39;, age:&#39;Kansas City, MO 64105&#39;})
CREATE (Urban_Outfitters:Merchant {id:&#39;24&#39;, name:&#39;Urban Outfitters&#39;, street:&#39;99 Strother Street&#39;, age:&#39;Kansas City, MO 64105&#39;})
CREATE (RadioShack:Merchant {id:&#39;25&#39;, name:&#39;RadioShack&#39;, street:&#39;3306 Douglas Dairy Road&#39;, age:&#39;Kansas City, MO 64105&#39;})
CREATE (Macys:Merchant {id:&#39;26&#39;, name:&#39;Macys&#39;, street:&#39;2912 Nutter Street&#39;, age:&#39;Kansas City, MO 64105&#39;})
</code></pre>

<h2 id="toc_4">建立交易历史记录节点</h2>

<pre class="line-numbers"><code class="language-text">// Create transaction history
CREATE (Paul)-[:HAS_BOUGHT_AT {amount:&#39;986&#39;, time:&#39;4/17/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Just_Brew_It)
CREATE (Paul)-[:HAS_BOUGHT_AT {amount:&#39;239&#39;, time:&#39;5/15/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Starbucks)
CREATE (Paul)-[:HAS_BOUGHT_AT {amount:&#39;475&#39;, time:&#39;3/28/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Sears)
CREATE (Paul)-[:HAS_BOUGHT_AT {amount:&#39;654&#39;, time:&#39;3/20/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Wallmart)
CREATE (Jean)-[:HAS_BOUGHT_AT {amount:&#39;196&#39;, time:&#39;7/24/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Soccer_for_the_City)
CREATE (Jean)-[:HAS_BOUGHT_AT {amount:&#39;502&#39;, time:&#39;4/9/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Abercrombie)
CREATE (Jean)-[:HAS_BOUGHT_AT {amount:&#39;848&#39;, time:&#39;5/29/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Wallmart)
CREATE (Jean)-[:HAS_BOUGHT_AT {amount:&#39;802&#39;, time:&#39;3/11/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Amazon)
CREATE (Jean)-[:HAS_BOUGHT_AT {amount:&#39;203&#39;, time:&#39;3/27/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Subway)
CREATE (Dan)-[:HAS_BOUGHT_AT {amount:&#39;35&#39;, time:&#39;1/23/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(MacDonalds)
CREATE (Dan)-[:HAS_BOUGHT_AT {amount:&#39;605&#39;, time:&#39;1/27/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(MacDonalds)
CREATE (Dan)-[:HAS_BOUGHT_AT {amount:&#39;62&#39;, time:&#39;9/17/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Soccer_for_the_City)
CREATE (Dan)-[:HAS_BOUGHT_AT {amount:&#39;141&#39;, time:&#39;11/14/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Amazon)
CREATE (Marc)-[:HAS_BOUGHT_AT {amount:&#39;134&#39;, time:&#39;4/14/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Amazon)
CREATE (Marc)-[:HAS_BOUGHT_AT {amount:&#39;336&#39;, time:&#39;4/3/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(American_Apparel)
CREATE (Marc)-[:HAS_BOUGHT_AT {amount:&#39;964&#39;, time:&#39;3/22/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Wallmart)
CREATE (Marc)-[:HAS_BOUGHT_AT {amount:&#39;430&#39;, time:&#39;8/10/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Sears)
CREATE (Marc)-[:HAS_BOUGHT_AT {amount:&#39;11&#39;, time:&#39;9/4/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Soccer_for_the_City)
CREATE (John)-[:HAS_BOUGHT_AT {amount:&#39;545&#39;, time:&#39;10/6/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Soccer_for_the_City)
CREATE (John)-[:HAS_BOUGHT_AT {amount:&#39;457&#39;, time:&#39;10/15/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Sprint)
CREATE (John)-[:HAS_BOUGHT_AT {amount:&#39;468&#39;, time:&#39;7/29/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Justice)
CREATE (John)-[:HAS_BOUGHT_AT {amount:&#39;768&#39;, time:&#39;11/28/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(American_Apparel)
CREATE (John)-[:HAS_BOUGHT_AT {amount:&#39;921&#39;, time:&#39;3/12/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Just_Brew_It)
CREATE (Zoey)-[:HAS_BOUGHT_AT {amount:&#39;740&#39;, time:&#39;12/15/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(MacDonalds)
CREATE (Zoey)-[:HAS_BOUGHT_AT {amount:&#39;510&#39;, time:&#39;11/27/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Abercrombie)
CREATE (Zoey)-[:HAS_BOUGHT_AT {amount:&#39;414&#39;, time:&#39;1/20/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Just_Brew_It)
CREATE (Zoey)-[:HAS_BOUGHT_AT {amount:&#39;721&#39;, time:&#39;7/17/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Amazon)
CREATE (Zoey)-[:HAS_BOUGHT_AT {amount:&#39;353&#39;, time:&#39;10/25/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Subway)
CREATE (Ava)-[:HAS_BOUGHT_AT {amount:&#39;681&#39;, time:&#39;12/28/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Sears)
CREATE (Ava)-[:HAS_BOUGHT_AT {amount:&#39;87&#39;, time:&#39;2/19/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Wallmart)
CREATE (Ava)-[:HAS_BOUGHT_AT {amount:&#39;533&#39;, time:&#39;8/6/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(American_Apparel)
CREATE (Ava)-[:HAS_BOUGHT_AT {amount:&#39;723&#39;, time:&#39;1/8/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(American_Apparel)
CREATE (Ava)-[:HAS_BOUGHT_AT {amount:&#39;627&#39;, time:&#39;5/20/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Just_Brew_It)
CREATE (Olivia)-[:HAS_BOUGHT_AT {amount:&#39;74&#39;, time:&#39;9/4/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Soccer_for_the_City)
CREATE (Olivia)-[:HAS_BOUGHT_AT {amount:&#39;231&#39;, time:&#39;7/12/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Wallmart)
CREATE (Olivia)-[:HAS_BOUGHT_AT {amount:&#39;924&#39;, time:&#39;10/4/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Soccer_for_the_City)
CREATE (Olivia)-[:HAS_BOUGHT_AT {amount:&#39;742&#39;, time:&#39;8/12/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Just_Brew_It)
CREATE (Mia)-[:HAS_BOUGHT_AT {amount:&#39;276&#39;, time:&#39;12/24/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Soccer_for_the_City)
CREATE (Mia)-[:HAS_BOUGHT_AT {amount:&#39;66&#39;, time:&#39;4/16/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Starbucks)
CREATE (Mia)-[:HAS_BOUGHT_AT {amount:&#39;467&#39;, time:&#39;12/23/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(MacDonalds)
CREATE (Mia)-[:HAS_BOUGHT_AT {amount:&#39;830&#39;, time:&#39;3/13/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Sears)
CREATE (Mia)-[:HAS_BOUGHT_AT {amount:&#39;240&#39;, time:&#39;7/9/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Amazon)
CREATE (Mia)-[:HAS_BOUGHT_AT {amount:&#39;164&#39;, time:&#39;12/26/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Soccer_for_the_City)
CREATE (Madison)-[:HAS_BOUGHT_AT {amount:&#39;630&#39;, time:&#39;10/6/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(MacDonalds)
CREATE (Madison)-[:HAS_BOUGHT_AT {amount:&#39;19&#39;, time:&#39;7/29/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Abercrombie)
CREATE (Madison)-[:HAS_BOUGHT_AT {amount:&#39;352&#39;, time:&#39;12/16/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Subway)
CREATE (Madison)-[:HAS_BOUGHT_AT {amount:&#39;147&#39;, time:&#39;8/3/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Amazon)
CREATE (Madison)-[:HAS_BOUGHT_AT {amount:&#39;91&#39;, time:&#39;6/29/2014&#39;, status:&#39;Undisputed&#39;}]-&gt;(Wallmart)
CREATE (Paul)-[:HAS_BOUGHT_AT {amount:&#39;1021&#39;, time:&#39;7/18/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Apple_Store)
CREATE (Paul)-[:HAS_BOUGHT_AT {amount:&#39;1732&#39;, time:&#39;5/10/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Urban_Outfitters)
CREATE (Paul)-[:HAS_BOUGHT_AT {amount:&#39;1415&#39;, time:&#39;4/1/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(RadioShack)
CREATE (Paul)-[:HAS_BOUGHT_AT {amount:&#39;1849&#39;, time:&#39;12/20/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Macys)
CREATE (Marc)-[:HAS_BOUGHT_AT {amount:&#39;1914&#39;, time:&#39;7/18/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Apple_Store)
CREATE (Marc)-[:HAS_BOUGHT_AT {amount:&#39;1424&#39;, time:&#39;5/10/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Urban_Outfitters)
CREATE (Marc)-[:HAS_BOUGHT_AT {amount:&#39;1721&#39;, time:&#39;4/1/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(RadioShack)
CREATE (Marc)-[:HAS_BOUGHT_AT {amount:&#39;1003&#39;, time:&#39;12/20/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Macys)
CREATE (Olivia)-[:HAS_BOUGHT_AT {amount:&#39;1149&#39;, time:&#39;7/18/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Apple_Store)
CREATE (Olivia)-[:HAS_BOUGHT_AT {amount:&#39;1152&#39;, time:&#39;8/10/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Urban_Outfitters)
CREATE (Olivia)-[:HAS_BOUGHT_AT {amount:&#39;1884&#39;, time:&#39;8/1/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(RadioShack)
CREATE (Olivia)-[:HAS_BOUGHT_AT {amount:&#39;1790&#39;, time:&#39;12/20/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Macys)
CREATE (Madison)-[:HAS_BOUGHT_AT {amount:&#39;1925&#39;, time:&#39;7/18/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Apple_Store)
CREATE (Madison)-[:HAS_BOUGHT_AT {amount:&#39;1374&#39;, time:&#39;7/10/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Urban_Outfitters)
CREATE (Madison)-[:HAS_BOUGHT_AT {amount:&#39;1368&#39;, time:&#39;7/1/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(RadioShack)
CREATE (Madison)-[:HAS_BOUGHT_AT {amount:&#39;1816&#39;, time:&#39;12/20/2014&#39;, status:&#39;Disputed&#39;}]-&gt;(Macys)
RETURN *

</code></pre>

<p><img src="https://raw.githubusercontent.com/Timehsw/gitnote-images/master/neo4j/case/20190305150014.png" alt=""/></p>

<h1 id="toc_5">查询</h1>

<h2 id="toc_6">识别所有的欺诈性交易</h2>

<p>查看在交易的过程中发生争吵的人和商店.</p>

<pre class="line-numbers"><code class="language-text">MATCH (victim:Person)-[r:HAS_BOUGHT_AT]-&gt;(merchant)
WHERE r.status = &quot;Disputed&quot;
RETURN victim.name AS `Customer Name`, merchant.name AS `Store Name`, r.amount AS Amount, r.time AS `Transaction Time`
ORDER BY `Transaction Time` DESC
</code></pre>

<p><img src="https://raw.githubusercontent.com/Timehsw/gitnote-images/master/neo4j/case/20190305152050.png" alt=""/></p>

<p>查找了Disputed可疑交易对应的受害者</p>

<p>分析:<br/>
现在我们知道哪些客服和哪些商家参与了我们的欺诈案件.<br/>
但我们正在寻找的罪犯在哪里？<br/>
可以想象,这里有用的是每次欺诈交易的交易日期。</p>

<p>继续查</p>

<pre class="line-numbers"><code class="language-text">MATCH (victim:Person)-[r:HAS_BOUGHT_AT]-&gt;(merchant)
WHERE r.status = &quot;Disputed&quot;
MATCH (victim)-[t:HAS_BOUGHT_AT]-&gt;(othermerchants)
WHERE t.status = &quot;Undisputed&quot; AND t.time &lt; r.time
WITH victim, othermerchants, t ORDER BY t.time DESC
RETURN victim.name AS `Customer Name`, othermerchants.name AS `Store Name`, t.amount AS Amount, t.time AS `Transaction Time`
ORDER BY `Transaction Time` DESC
</code></pre>

<p>分析这个查询</p>

<pre class="line-numbers"><code class="language-text">MATCH (victim)-[t:HAS_BOUGHT_AT]-&gt;(othermerchants)
WHERE t.status = &quot;Undisputed&quot; AND t.time &lt; r.time
</code></pre>

<p>这段话表示查找所有受害者victim在可疑交易时间之前还在哪些商店有过正常购买记录。在这些正常购买的时候可能发生了卡号泄露。所以根据受害者之前的所有正常交易商店可能推断泄露地点。 </p>

<p>现在我们想要找到共同点.在所有这些看似无害的交易中是否有共同商人？</p>

<pre class="line-numbers"><code class="language-text">MATCH (victim:Person)-[r:HAS_BOUGHT_AT]-&gt;(merchant)
WHERE r.status = &quot;Disputed&quot;
MATCH (victim:Person)-[r:HAS_BOUGHT_AT]-&gt;(merchant)
WHERE r.status = &quot;Disputed&quot;
MATCH (victim)-[t:HAS_BOUGHT_AT]-&gt;(othermerchants)
WHERE t.status = &quot;Undisputed&quot; AND t.time &lt; r.time
WITH victim, othermerchants, t ORDER BY t.time DESC
RETURN DISTINCT othermerchants.name AS `Suspicious Store`, count(DISTINCT t) AS Count, collect(DISTINCT victim.name) AS Victims
ORDER BY Count DESC
</code></pre>

<p>这个查询的关键点在于<br/>
<strong>RETURN DISTINCT othermerchants.name AS <code>Suspicious Store</code>, count(DISTINCT t) AS Count, collect(DISTINCT victim.name) AS Victims</strong><br/>
看下表，查出了可疑的商店，比如Wallmart发生过4次可能被盗卡的交易场景。<br/>
<img src="https://raw.githubusercontent.com/Timehsw/gitnote-images/master/neo4j/case/20190305153514.png" alt=""/></p>

<h1 id="toc_7">最后小偷在哪呢?</h1>

<p><img src="https://raw.githubusercontent.com/Timehsw/gitnote-images/master/neo4j/case/20190305153828.png" alt=""/><br/>
在每次欺诈交易中，信用卡持有人都在前一天访问了沃尔玛。我们现在知道客户的信用卡号码被盗的位置和日期。</p>

<p>剩下的交给警察叔叔就行了!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[构建信用风险类型的特征]]></title>
    <link href="http://dmlcoding.com/15505602742016.html"/>
    <updated>2019-02-19T15:11:14+08:00</updated>
    <id>http://dmlcoding.com/15505602742016.html</id>
    <content type="html"><![CDATA[
<p><strong>简述建模过程</strong></p>

<ul>
<li>数据预处理
<ul>
<li>时间格式</li>
<li>缺失值</li>
<li>极值</li>
</ul></li>
<li>特征构造
<ul>
<li>计数</li>
<li>比例</li>
<li>距离</li>
</ul></li>
<li>特征选择
<ul>
<li>相关性</li>
<li>差异性</li>
<li>显著性</li>
</ul></li>
<li><p>模型参数估计</p>
<ul>
<li>回归系数</li>
<li>模型复杂度</li>
</ul>
<span id="more"></span><!-- more --></li>
</ul>

<h2 id="toc_0">常用数据格式的处理方式</h2>

<p>一般而言原始数据带有一定的格式,需要转换成正确的格式.<br/>
例如:</p>

<ul>
<li>利率<br/>
带%的百分比,需要转化成浮点数</li>
<li>日期<br/>
Nov-17,需要转化为Python的时间</li>
<li>工作年限<br/>
&quot;<1 year"转化为0,">10years&quot;转化为11</li>
<li>...</li>
</ul>

<h2 id="toc_1">文本类的数据的处理方式</h2>

<ul>
<li>主题提取<br/>
优点: 提取准确,详细的信息,对风险的评估非常有效<br/>
缺点: NPL的模型较为复杂,且需要足够多的训练样本</li>
<li>编码<br/>
优点: 简单<br/>
缺点: 信息丢失很高</li>
</ul>

<h2 id="toc_2">缺失值的处理方式</h2>

<p>缺失在数据分析的工作是频繁出现的</p>

<ul>
<li>缺失的种类
<ul>
<li>完全随机缺失</li>
<li>随机缺失</li>
<li>完全非随机缺失</li>
</ul></li>
<li>处理的方法
<ul>
<li>补缺</li>
<li>作为一种状态</li>
</ul></li>
</ul>

<h2 id="toc_3">常用的特征衍生方式</h2>

<ul>
<li>计数: 过去1年内申请贷款的总次数</li>
<li>求和: 过去1年内的网店消费总额</li>
<li>比例: 贷款申请额度与年收入的占比</li>
<li>时间差: 第一次开户距今时长</li>
<li>波动率: 过去3年内每份工作的时间的标准差</li>
</ul>

<h1 id="toc_4">特征的分箱</h1>

<p>对特征做分箱操作</p>

<h2 id="toc_5">分箱的一些描述</h2>

<ul>
<li>分箱的定义
<ul>
<li>将连续变量离散化</li>
<li>将多状态的离散变量合并成少状态</li>
</ul></li>
<li>分箱的重要性
<ul>
<li>稳定性: 避免特征中无意义的波动对评分带来的波动</li>
<li>健壮性: 避免了极端值的影响</li>
</ul></li>
<li>分箱的优势
<ul>
<li>可以将缺失作为独立的一个箱带入模型中</li>
<li>将所有变量变换到相似的尺度上</li>
</ul></li>
<li>分箱的限制
<ul>
<li>计算量大</li>
<li>分箱后需要编码</li>
</ul></li>
</ul>

<h2 id="toc_6">常用的分箱方法</h2>

<ul>
<li>有监督
<ul>
<li>Best-KS</li>
<li>ChiMerge</li>
</ul></li>
<li>无监督
<ul>
<li>等频</li>
<li>等距</li>
<li>聚类</li>
</ul></li>
</ul>

<h3 id="toc_7">Best-KS</h3>

<p>原理: 让分箱后组别的分布的差异最大化</p>

<p><strong>对于连续变量</strong></p>

<ol>
<li>排序, \(x=\lbrace X_1,X_2,...,X_k\rbrace \)</li>
<li>计算每一点的KS值</li>
<li>选择最大的KS对应的特征值</li>
</ol>

<h3 id="toc_8">卡方分箱法</h3>

<h3 id="toc_9">注意</h3>

<p>对于连续型变量</p>

<ol>
<li>使用ChiMerge进行分箱,默认分为5个箱.</li>
<li>检查分箱后的bad rate单调性;倘若不满足,需要进行相邻两箱的合并,直到bad rate为止.</li>
</ol>

<h1 id="toc_10">WOE编码</h1>

<blockquote>
<p>WOE(weight of evidence,证据权重)<br/>
一种有监督的编码方式,将预测类别的集中度的属性作为编码的数值</p>
</blockquote>

<p>优势</p>

<ol>
<li>将特征的值规范到相近的尺度上(经验上讲,WOE的绝对值波动范围在0.1~3之间)</li>
<li>具有业务含义</li>
</ol>

<p>缺点</p>

<ol>
<li>需要每箱中同时包含好,坏两个类别</li>
</ol>

<h2 id="toc_11">WOE计算公式</h2>

<p><img src="media/15505602742016/15505691302453.jpg" alt=""/></p>

<h2 id="toc_12">WOE编码的意义</h2>

<ol>
<li>符号和好样本比例相关</li>
<li>要求回归模型的系数为负</li>
</ol>

<h1 id="toc_13">特征信息度的计算和意义</h1>

<p>可以用信息度来进行变量挑选</p>

<p>在评分卡模型中,变量挑选是非常重要的工作.比如:<br/>
如果变量之间具有共线性,线性相关性,那么就会造成信息冗余,这就降低了显著性,甚至造成符号失真.<br/>
变量太多的话,也加剧了后期验证,部署,监控的负担.而且有些变量在业务上含义不充分,解释性比较低.</p>

<p>这里是进行变量挑选的一些依据</p>

<ol>
<li>带约束Lasso</li>
<li>特征重要性:随机森林</li>
<li>模型拟合优度和复杂度:基于AIC的逐步回归</li>
<li>变量信息度:IV</li>
</ol>

<p>IV(Information Value),衡量特征包含预测变量浓度的一种指标<br/>
<img src="media/15505602742016/15505713539693.jpg" alt=""/></p>

<p>特征信息度的结构<br/>
\[IV_i=(G_i-B_i)\times \log{\dfrac{G_i}{B_i}} = (G_i-B_i)\times WOE_i\]</p>

<p>其中,G_i,B_i代表箱i中好坏样本占全体好坏样本的比例.<br/>
WOE:衡量两类样本分布的差异性<br/>
(G_i,B_i):衡量差异的重要性<br/>
例如:<br/>
G_1=0.2,B_1=0.1与G_2=0.02,B_2=0.01<br/>
\[WOE_1=WOE_2=log(2) \\\<br/>
IV_1=(0.2-0.1)\times log(2)=0.1 \times log(2) \\\<br/>
IV_2=(0.02-0.01) \times log(2)=0.01 \times log(2)\]</p>

<h2 id="toc_14">特征信息度的作用</h2>

<p><strong>挑选变量</strong></p>

<ul>
<li>非负指标</li>
<li>高IV表示该特征和目标变量的关联度高</li>
<li>目标变量只能是二分类</li>
<li>过高的IV,可能有潜在的风险</li>
<li>特征分箱越细,IV越高</li>
<li>常用的阈值
<ul>
<li>&lt;=0.02 : 没有预测性,不可用</li>
<li>0.02 ~ 0.1 : 弱预测性</li>
<li>0.1 ~ 0.2 : 有一定的预测性</li>
<li>0.2+ : 高预测性</li>
</ul></li>
</ul>

<h1 id="toc_15">信用风险中的单变量分析和多变量分析</h1>

<h2 id="toc_16">单变量分析</h2>

<p>目的:根据变量某些属性,从初选名单(long list)中筛选出合适的变量进入缩减名单(short list)</p>

<p>那么需要分析变量属性</p>

<ol>
<li>变量的显著性(高IV)</li>
<li>变量的分布</li>
<li>变量的业务含义</li>
</ol>

<p>以分箱后的WOE为例</p>

<ol>
<li>用IV检验有效性</li>
<li>连续变量bad rate的单调性(可以放宽到U型)</li>
<li>单一区间的占比不宜过高</li>
</ol>

<h2 id="toc_17">多变量分析</h2>

<p>变量的两两相关性</p>

<p>当相关性高时,只能保留一个.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[XGBoost]]></title>
    <link href="http://dmlcoding.com/15488325444778.html"/>
    <updated>2019-01-30T15:15:44+08:00</updated>
    <id>http://dmlcoding.com/15488325444778.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">XGBoost概述</h1>

<ul>
<li>XGBoost是GBDT算法的一种变种，是一种常用的有监督集成学习算法；是一种伸缩性强、便捷的可并行构建模型的Gradient Boosting算法。
<ul>
<li>XGBoost官网：<a href="http://xgboost.readthedocs.io">http://xgboost.readthedocs.io</a> </li>
<li>XGBoost Github源码位置：<a href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a></li>
<li>XGBoost支持开发语言：Python、R、Java、Scala、C++等。</li>
</ul></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GBDT]]></title>
    <link href="http://dmlcoding.com/15488325386128.html"/>
    <updated>2019-01-30T15:15:38+08:00</updated>
    <id>http://dmlcoding.com/15488325386128.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Adaboost]]></title>
    <link href="http://dmlcoding.com/15488325277802.html"/>
    <updated>2019-01-30T15:15:27+08:00</updated>
    <id>http://dmlcoding.com/15488325277802.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">AdaBoost算法原理简述</h1>

<p><strong>Adaptive Boosting</strong> 是一种迭代算法.每一轮迭代中会在训练集上产生一个新的学习器,然后使用该学习器对所有样本进行预测,以评估每个样本的重要性.换句话来讲,算法会为每个样本赋予一个权重,每次用训练好的学习器标注/预测各个样本.如果某个样本点被预测的越正确,则将其权重降低;否则提高样本的权重.权重越高的样本在下一轮迭代训练中所占的比重就越大,也就是越难区分的样本在训练过程中会变得越重要.</p>

<span id="more"></span><!-- more -->

<p>整个迭代过程直到错误率足够小或者达到一定的迭代次数为止.</p>

<p>AdaBoost 算法的核心,样本加权.<br/>
<img src="media/15488325277802/15490045182494.jpg" alt="" style="width:664px;"/></p>

<h1 id="toc_1">AdaBoost算法推理</h1>

<p>AdaBoost算法将基分类器进行线性组合作为强分类器,同时给分类误差率较小的基分类器以大的权重,给分类误差率较大的基分类器以小的权重值;构建的线性组合</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pandas 中map, applymap and apply的区别]]></title>
    <link href="http://dmlcoding.com/15423390877662.html"/>
    <updated>2018-11-16T11:31:27+08:00</updated>
    <id>http://dmlcoding.com/15423390877662.html</id>
    <content type="html"><![CDATA[
<p>Summing up, apply works on a row / column basis of a DataFrame, applymap works element-wise on a DataFrame, and map works element-wise on a Series.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">1.apply()</h1>

<p>当想让方程作用在一维的向量上时，可以使用apply来完成，如下所示</p>

<pre class="line-numbers"><code class="language-text">In [116]: frame = DataFrame(np.random.randn(4, 3), columns=list(&#39;bde&#39;), index=[&#39;Utah&#39;, &#39;Ohio&#39;, &#39;Texas&#39;, &#39;Oregon&#39;])

In [117]: frame
Out[117]: 
               b         d         e
Utah   -0.029638  1.081563  1.280300
Ohio    0.647747  0.831136 -1.549481
Texas   0.513416 -0.884417  0.195343
Oregon -0.485454 -0.477388 -0.309548

In [118]: f = lambda x: x.max() - x.min()

In [119]: frame.apply(f)
Out[119]: 
b    1.133201
d    1.965980
e    2.829781
dtype: float64

</code></pre>

<p>但是因为大多数的列表统计方程 (比如 sum 和 mean)是DataFrame的函数，所以apply很多时候不是必须的</p>

<h1 id="toc_1">2.applymap()</h1>

<p>如果想让方程作用于DataFrame中的每一个元素，可以使用applymap().用法如下所示</p>

<pre class="line-numbers"><code class="language-text">In [120]: format = lambda x: &#39;%.2f&#39; % x

In [121]: frame.applymap(format)
Out[121]: 
            b      d      e
Utah    -0.03   1.08   1.28
Ohio     0.65   0.83  -1.55
Texas    0.51  -0.88   0.20
Oregon  -0.49  -0.48  -0.31

</code></pre>

<h1 id="toc_2">3.map()</h1>

<p>map()只要是作用将函数作用于一个Series的每一个元素，用法如下所示</p>

<pre class="line-numbers"><code class="language-text">In [122]: frame[&#39;e&#39;].map(format)
Out[122]: 
Utah       1.28
Ohio      -1.55
Texas      0.20
Oregon    -0.31
Name: e, dtype: object
</code></pre>

<p>总的来说就是apply()是一种让函数作用于列或者行操作，applymap()是一种让函数作用于DataFrame每一个元素的操作，而map是一种让函数作用于Series每一个元素的操作</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Objective vs. Cost vs. Loss vs. Error Function]]></title>
    <link href="http://dmlcoding.com/15420931314995.html"/>
    <updated>2018-11-13T15:12:11+08:00</updated>
    <id>http://dmlcoding.com/15420931314995.html</id>
    <content type="html"><![CDATA[
<p><strong><u>The function we want to minimize or maximize is called the objective function, or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function - these terms are synonymous. The cost function is used more in optimization problem and loss function is used in parameter estimation.</u></strong></p>

<span id="more"></span><!-- more -->

<p>The loss function (or error) is for a single training example, while the cost function is over the entire training set (or mini-batch for mini-batch gradient descent). Therefore, a loss function is a part of a cost function which is a type of an objective function.  Objective function, cost function, loss function: are they the same thing? | </p>

<p>StackExchange</p>

<ul>
<li> Loss function is usually a function defined on a data point, prediction and label, and measures the penalty. For example:
<ul>
<li> square loss l(f(xi|θ),yi)=(f(xi|θ)−yi)2, used in linear regression</li>
<li> hinge loss l(f(xi|θ),yi)=max(0,1−f(xi|θ)yi), used in SVM</li>
<li> 0/1 loss l(f(xi|θ),yi)=1⟺f(xi|θ)≠yi, used in theoretical analysis and definition of accuracy</li>
</ul></li>
<li> Cost function is usually more general. It might be a sum of loss functions over your training set plus some model complexity penalty (regularization). For example:
<ul>
<li> Mean Squared Error MSE(θ)=1N∑Ni=1(f(xi|θ)−yi)2</li>
<li> SVM cost function SVM(θ)=∥θ∥2+C∑Ni=1ξi (there are additional constraints connecting ξi with C and with training set)</li>
</ul></li>
<li> Objective function is the most general term for any function that you optimize during training. For example, a probability of generating training set in maximum likelihood approach is a well defined objective function, but it is not a loss function nor cost function (however you could define an equivalent cost function). For example:
<ul>
<li> MLE is a type of objective function (which you maximize)</li>
<li>Divergence between classes can be an objective function but it is barely a cost function, unless you define something artificial, like 1-Divergence, and name it a cost</li>
</ul></li>
<li> Error function - Backpropagation; or automatic differentiation, is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function. This technique is also sometimes called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers. Backpropagation | Wikipedia</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[将python生成的base64图片,在浏览器上展示出来]]></title>
    <link href="http://dmlcoding.com/15402913817590.html"/>
    <updated>2018-10-23T18:43:01+08:00</updated>
    <id>http://dmlcoding.com/15402913817590.html</id>
    <content type="html"><![CDATA[
<p>方便调试</p>

<pre class="line-numbers"><code class="language-text">&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;show base64 pic&lt;/title&gt;
    &lt;meta charset=&quot;UTF-8&quot;&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div id=&#39;root&#39;&gt;
        &lt;img src=&quot;data:image/png;base64,这里是base64的内容,替换掉即可(中文部分)&quot; /&gt;
    &lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<span id="more"></span><!-- more -->

<pre class="line-numbers"><code class="language-text">#!/usr/bin/env python 

import base64

import pandas as pd
import pydotplus
from IPython.display import Image
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier

data = pd.read_csv(&quot;./datas/demos.csv&quot;)
train_data = data.drop(&quot;label&quot;, axis=1)
target = data[&quot;label&quot;]

DTmodel = DecisionTreeClassifier(max_depth=4)
DTmodel = DTmodel.fit(train_data, target)
print(DTmodel)

# 将决策树的图片转成base64
dot_data = tree.export_graphviz(DTmodel, out_file=None)
graph = pydotplus.graph_from_dot_data(dot_data)
pic_base64 = base64.b64encode(Image(graph.create_png()).data).decode(&#39;utf8&#39;)

print(pic_base64)

</code></pre>

<p>将打印出来的pic_base64的内容替换到html里面的中文部分,然后用浏览器打开这个html文件即可看到图片</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Isolation Forest]]></title>
    <link href="http://dmlcoding.com/15398508250689.html"/>
    <updated>2018-10-18T16:20:25+08:00</updated>
    <id>http://dmlcoding.com/15398508250689.html</id>
    <content type="html"><![CDATA[
<p>scikit-learn返回:是异常点(-1)或者不是异常点(1)</p>

<p><img src="media/15398508250689/15398521143637.jpg" alt=""/></p>

<p>孤立森林--&gt;查看数据页面,如上所示<br/>
原始数据的所有列,预测出来是否是异常值,也即是是否偏离(偏移即是-1),偏移度也就是decision_function算出来的值,返回样本的异常评分，值越小表示越有可能是异常样本<br/>
data,model.predict(X_train),model.decision_function(X_train)</p>

<pre class="line-numbers"><code class="language-text">df=pd.concat([pd.DataFrame(X_train),pd.Series(clf.predict(X_train)), pd.Series(clf.decision_function(X_train))], axis=1)

df.columns = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[聚类算法评估指标]]></title>
    <link href="http://dmlcoding.com/15396891344806.html"/>
    <updated>2018-10-16T19:25:34+08:00</updated>
    <id>http://dmlcoding.com/15396891344806.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">Not Given Label</h1>

<h2 id="toc_1">1、Compactness(紧密性)(CP)</h2>

<span id="more"></span><!-- more -->

<h1 id="toc_2">代码实现</h1>

<pre class="line-numbers"><code class="language-text"># -*- coding: utf-8 -*-
&#39;&#39;&#39;
    Created by hushiwei on 2018/10/16
    Desc : 进行聚类算法的评估指标计算
    Note : CP指数,SP指数,DB,DVI
&#39;&#39;&#39;

# 导入包
from numpy import *
import numpy as np
import pandas as pd
from sklearn.datasets import make_blobs  # 导入产生模拟数据的方法
from sklearn.cluster import KMeans  # 导入kmeans 类
import time
from MachineLearning.cluster_evaluate.calculate_dvi import dunn

def distEclud(vecA, vecB, axis=None):
    &#39;&#39;&#39;
    求两个向量之间的距离,计算欧几里得距离
    same as : np.linalg.norm(np.asarray(datas[0].iloc[0].values) - np.asarray(datas[0].iloc[1].values))
    :param vecA: 中心点
    :param vecB: 数据点
    :param axis: np.sum的参数
    :return:
    &#39;&#39;&#39;
    return np.sqrt(np.sum(np.power(vecA - vecB, 2), axis=axis))


def calculate_cp_i(cluster_center, cluster_point):
    &#39;&#39;&#39;
    计算聚类类各点到聚类中心的平均距离
    :param cluster_center: 聚类中心点
    :param cluster_point: 聚类样本点
    :return:
    &#39;&#39;&#39;
    return np.average(distEclud(cluster_center, cluster_point, axis=1))


def calculate_cp(cluster_centers, cluster_points):
    &#39;&#39;&#39;
    CP,计算每一个类各点到聚类中心的平均距离
    CP越低意味着类内聚类距离越近
    :param cluster_centers: 聚类中心点
    :param cluster_points: 聚类样本点
    :return:
    &#39;&#39;&#39;

    cps = [calculate_cp_i(cluster_centers[i], cluster_points[i]) for i in arange(len(cluster_centers))]
    cp = np.average(cps)
    return cp


def calculate_sp(cluster_centers):
    &#39;&#39;&#39;
    SP,计算各聚类中心两两之间的平均距离
    SP越高意味类间聚类距离越远
    :param cluster_centers:
    :return:
    &#39;&#39;&#39;
    k = len(cluster_centers)
    res = []
    for i in arange(k):
        tmp = []
        for j in arange(i + 1, k):
            tmp.append(distEclud(cluster_centers[i], cluster_centers[j]))
        res.append(np.sum(tmp))

    sp = (2 / (k * k - k)) * np.sum(res)
    return sp


def calculate_db(cluster_centers, cluster_points):
    &#39;&#39;&#39;
    DB,计算任意两类别的类内距离平均距离(CP)之和除以两聚类中心距离求最大值
    DB越小意味着类内距离越小 同时类间距离越大
    :param cluster_centers: 聚类中心点
    :param cluster_points: 聚类样本点
    :return:
    &#39;&#39;&#39;
    n_cluster = len(cluster_centers)
    cps = [calculate_cp_i(cluster_centers[i], cluster_points[i]) for i in arange(n_cluster)]
    db = []

    for i in range(n_cluster):
        for j in range(n_cluster):
            if j != i:
                db.append((cps[i] + cps[j]) / distEclud(cluster_centers[i], cluster_centers[j]))
    db = (np.max(db) / n_cluster)
    return db


def calculate_dvi(cluster_points):
    &#39;&#39;&#39;
    DVI计算,任意两个簇元素的最短距离(类间)除以任意簇中的最大距离(类内).
    DVI越大意味着类间距离越大 同时类内距离越小。
    :param cluster_points:
    :return:
    &#39;&#39;&#39;
    # calcuation of maximum distance
    d1 = []
    d2 = []
    d3 = []
    # 类内最大距离
    for k, cluster_point in cluster_points.items():
        # 遍历每一个簇中每一个元素
        for i in range(len(cluster_point)):
            temp1 = cluster_point.iloc[i].values
            for j in range(len(cluster_point)):
                temp2 = cluster_point.iloc[j].values

                # 求簇内两元素的距离
                dist = distEclud(temp1, temp2)
                d1.insert(j, dist)

            d2.insert(i, max(d1))
            d1 = []

        d3.insert(k, max(d2))
        d2 = []
    xmax = max(d3)

    # calcuation of minimun distance
    d1 = []
    d2 = []
    d3 = []
    d4 = []
    # 类间最小距离
    for k, cluster_point in cluster_points.items():
        # 遍历每一个簇中每一个元素
        for j in range(len(cluster_point)):
            temp1 = cluster_point.iloc[j].values
            for key in cluster_points.keys():
                if not key == k:
                    other_cluster_df = cluster_points[key]
                    for index in range(len(other_cluster_df)):
                        temp2 = other_cluster_df.iloc[index].values
                        dist = distEclud(temp1, temp2)
                        d1.insert(index, dist)

                    d2.insert(key, min(d1))
                    d1 = []
            d3.insert(j, min(d2))
            d2 = []
        d4.insert(k, min(d3))
    xmin = min(d4)

    dunn_index = xmin / xmax
    return dunn_index


if __name__ == &#39;__main__&#39;:
    # 1. 产生模拟数据
    N = 1000
    centers = 4
    X, Y = make_blobs(n_samples=N, n_features=2, centers=centers, random_state=28)
    # df = pd.DataFrame(X, columns=list(&#39;abcdefghij&#39;))
    # df.to_csv(
    #     &#39;/Users/hushiwei/PycharmProjects/gai_platform/data/data_spliter/local_debug_data/cluster_data/cluster_data.csv&#39;,
    #     sep=&#39;,&#39;, index=False)
    # sys.exit()

    # 2. 模型构建
    km = KMeans(n_clusters=centers, init=&#39;random&#39;, random_state=28)
    km.fit(X)

    # 模型的预测
    y_hat = km.predict(X[:10])
    print(y_hat)

    print(&quot;所有样本距离所属簇中心点的总距离和为:%.5f&quot; % km.inertia_)
    print(&quot;所有样本距离所属簇中心点的平均距离为:%.5f&quot; % (km.inertia_ / N))

    print(&quot;所有的中心点聚类中心坐标:&quot;)
    cluster_centers = km.cluster_centers_
    print(cluster_centers)

    print(&quot;score其实就是所有样本点离所属簇中心点距离和的相反数:&quot;)
    print(km.score(X))

    # 统计各个聚簇点的类别数目
    r1 = pd.Series(km.labels_).value_counts()

    kmlabels = km.labels_  # 得到类别，label_ 是内部变量
    X = pd.DataFrame(X, columns=list(&#39;ab&#39;))
    r = pd.concat([X, pd.Series(kmlabels, index=X.index)], axis=1)  # 详细输出每个样本对应的类别，横向连接（0是纵向），得到聚类中心对应的类别下的数目

    r.columns = list(X.columns) + [&#39;class&#39;]  # 重命名表头  加一列的表头
    print(r.head())

    # 根据聚类信息,将数据进行分类{聚类id:聚类点}
    cluster_centers = {k: value for k, value in enumerate(cluster_centers)}
    cluster_points = {label: r[r[&#39;class&#39;] == label].drop(columns=[&#39;class&#39;]) for label in np.unique(km.labels_)}

    print(&#39;~&#39; * 10, &#39;evaluate kmeans&#39;, &#39;~&#39; * 10)

    cp = calculate_cp(cluster_centers, cluster_points)
    print(&#39;cp : &#39;, cp)

    sp = calculate_sp(cluster_centers)
    print(&#39;sp : &#39;, sp)

    dp = calculate_db(cluster_centers, cluster_points)
    print(&#39;dp : &#39;, dp)

    start=time.time()
    # dvi = calculate_dvi(cluster_points)
    # print(&#39;dvi : &#39;, dvi)


    a = [point.values for point in cluster_points.values()]
    di=dunn(a)
    print(&#39;di : &#39;,di)

    print(&quot;dvi cost &quot;,time.time()-start)

</code></pre>

<p>dvi的计算,代码2</p>

<pre class="line-numbers"><code class="language-text"># -*- coding: utf-8 -*-
&#39;&#39;&#39;
    Created by hushiwei on 2018/10/23
    Desc : 计算dvi
    Note : 这个和那个,有区别
&#39;&#39;&#39;

import numpy as np
from sklearn.metrics.pairwise import euclidean_distances


def delta(ck, cl):
    values = np.ones([len(ck), len(cl)]) * 10000

    for i in range(0, len(ck)):
        for j in range(0, len(cl)):
            values[i, j] = np.linalg.norm(ck[i] - cl[j])

    return np.min(values)


def big_delta(ci):
    values = np.zeros([len(ci), len(ci)])

    for i in range(0, len(ci)):
        for j in range(0, len(ci)):
            values[i, j] = np.linalg.norm(ci[i] - ci[j])

    return np.max(values)


def dunn(k_list):
    &quot;&quot;&quot; Dunn index [CVI]

    Parameters
    ----------
    k_list : list of np.arrays
        A list containing a numpy array for each cluster |c| = number of clusters
        c[K] is np.array([N, p]) (N : number of samples in cluster K, p : sample dimension)
    &quot;&quot;&quot;
    deltas = np.ones([len(k_list), len(k_list)]) * 1000000
    big_deltas = np.zeros([len(k_list), 1])
    l_range = list(range(0, len(k_list)))

    for k in l_range:
        for l in (l_range[0:k] + l_range[k + 1:]):
            deltas[k, l] = delta(k_list[k], k_list[l])

        big_deltas[k] = big_delta(k_list[k])

    di = np.min(deltas) / np.max(big_deltas)
    return di


def delta_fast(ck, cl, distances):
    values = distances[np.where(ck)][:, np.where(cl)]
    values = values[np.nonzero(values)]

    return np.min(values)


def big_delta_fast(ci, distances):
    values = distances[np.where(ci)][:, np.where(ci)]
    values = values[np.nonzero(values)]

    return np.max(values)


def dunn_fast(points, labels):
    &quot;&quot;&quot; Dunn index - FAST (using sklearn pairwise euclidean_distance function)

    Parameters
    ----------
    points : np.array
        np.array([N, p]) of all points
    labels: np.array
        np.array([N]) labels of all points
    &quot;&quot;&quot;
    distances = euclidean_distances(points)
    ks = np.sort(np.unique(labels))

    deltas = np.ones([len(ks), len(ks)]) * 1000000
    big_deltas = np.zeros([len(ks), 1])

    l_range = list(range(0, len(ks)))

    for k in l_range:
        for l in (l_range[0:k] + l_range[k + 1:]):
            deltas[k, l] = delta_fast((labels == ks[k]), (labels == ks[l]), distances)

        big_deltas[k] = big_delta_fast((labels == ks[k]), distances)

    di = np.min(deltas) / np.max(big_deltas)
    return di


def big_s(x, center):
    len_x = len(x)
    total = 0

    for i in range(len_x):
        total += np.linalg.norm(x[i] - center)

    return total / len_x


def davisbouldin(k_list, k_centers):
    &quot;&quot;&quot; Davis Bouldin Index

    Parameters
    ----------
    k_list : list of np.arrays
        A list containing a numpy array for each cluster |c| = number of clusters
        c[K] is np.array([N, p]) (N : number of samples in cluster K, p : sample dimension)
    k_centers : np.array
        The array of the cluster centers (prototypes) of type np.array([K, p])
    &quot;&quot;&quot;
    len_k_list = len(k_list)
    big_ss = np.zeros([len_k_list], dtype=np.float64)
    d_eucs = np.zeros([len_k_list, len_k_list], dtype=np.float64)
    db = 0

    for k in range(len_k_list):
        big_ss[k] = big_s(k_list[k], k_centers[k])

    for k in range(len_k_list):
        for l in range(0, len_k_list):
            d_eucs[k, l] = np.linalg.norm(k_centers[k] - k_centers[l])

    for k in range(len_k_list):
        values = np.zeros([len_k_list - 1], dtype=np.float64)
        for l in range(0, k):
            values[l] = (big_ss[k] + big_ss[l]) / d_eucs[k, l]
        for l in range(k + 1, len_k_list):
            values[l - 1] = (big_ss[k] + big_ss[l]) / d_eucs[k, l]

        db += np.max(values)
    res = db / len_k_list
    return res
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[模型评估 K-S值和AUC的区别]]></title>
    <link href="http://dmlcoding.com/15395734385439.html"/>
    <updated>2018-10-15T11:17:18+08:00</updated>
    <id>http://dmlcoding.com/15395734385439.html</id>
    <content type="html"><![CDATA[
<p>在模型建立之后，必须对模型的效果进行评估，因为数据挖掘是一个探索的过程，评估-优化是一个永恒的过程。在分类模型评估中，最常用的两种评估标准就是K-S值和AUC值。</p>

<p>可能有人会问了，为什么不直接看正确率呢？你可以这么想，如果一批样本中，正样本占到90%，负样本只占10%，那么我即使模型什么也不做，把样本全部判定为正，也能有90%的正确率咯？所以，用AUC值够保证你在样本不均衡的情况下也能准确评估模型的好坏，而K-S值不仅能告诉你准确与否，还能告诉你模型对好坏客户是否有足够的区分度。</p>

<p>下面分别看两种评估指标的概念。</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">一、ROC曲线和AUC值</h1>

<p>在逻辑回归、随机森林、GBDT、XGBoost这些模型中，模型训练完成之后，每个样本都会获得对应的两个概率值，一个是样本为正样本的概率，一个是样本为负样本的概率。把每个样本为正样本的概率取出来，进行排序，然后选定一个阈值，将大于这个阈值的样本判定为正样本，小于阈值的样本判定为负样本，然后可以得到两个值，<strong>一个是真正率，一个是假正率。</strong></p>

<p>真正率即判定为正样本且实际为正样本的样本数/所有的正样本数，假正率为判定为正样本实际为负样本的样本数/所有的负样本数。每选定一个阈值，就能得到一对真正率和假正率，由于判定为正样本的概率值区间为[0,1]，那么阈值必然在这个区间内选择，因此在此区间内不停地选择不同的阈值，重复这个过程，就能得到一系列的真正率和假正率，以这两个序列作为横纵坐标，即可得到ROC曲线了。<strong>而ROC曲线下方的面积，即为AUC值。</strong></p>

<p>对于AUC值，也许有一个更直观的理解，那就是，在按照正样本概率值对所有样本排序后，任意选取一对正负样本，正样本排在负样本之前的概率值，即为AUC值。也就是说，当所有的正样本在排序后都能排在负样本之前时，就证明所有的样本都被正确分类了，此时的AUC值也会为1。那么AUC值也就很好算了，如果有N个负样本，其中正样本有M个，那么可取的所有带正样本的样本对数对于排在第一位的正样本来说，有M+N-1个，但其中包含M-1对（正，正）的样本，而对于后面所有的正样本而言，能够取到的正样本概率大于负样本对数肯定小于其位置-1。</p>

<h1 id="toc_1">K-S曲线</h1>

<p>KS(Kolmogorov-Smirnov)值越大，表示模型能够将正、负客户区分开的程度越大。KS值的取值范围是[0，1] </p>

<p>通常来讲，KS&gt;0.2即表示模型有较好的预测准确性。</p>

<p>K-S曲线其实数据来源和本质和ROC曲线是一致的，只是ROC曲线是把真正率和假正率当作横纵轴，而K-S曲线是把真正率和假正率都当作是纵轴，横轴则由选定的阈值来充当。</p>

<p>下面这一段解释得更详细的K-S和AUC的区别是参考的这篇博客：</p>

<p><a href="https://blog.csdn.net/sinat_30316741/article/details/80018932">https://blog.csdn.net/sinat_30316741/article/details/80018932</a></p>

<blockquote>
<p>由于ks值能找出模型中差异最大的一个分段，因此适合用于cut_off，像评分卡这种就很适合用ks值来评估。但是ks值只能反映出哪个分段是区分最大的，而不能总体反映出所有分段的效果，因果AUC值更能胜任。 <br/>
ROC值一般在0.5-1.0之间。值越大表示模型判断准确性越高，即越接近1越好。ROC=0.5表示模型的预测能力与随机结果没有差别。 <br/>
KS值表示了模型将+和-区分开来的能力。值越大，模型的预测准确性越好。一般，KS&gt;0.2即可认为模型有比较好的预测准确性。</p>
</blockquote>

<p>好了，引用结束。 <br/>
K-S值一般是很难达到0.6的，在0.2~0.6之间都不错。一般如果是如果负样本对业务影响极大，那么区分度肯定就很重要，此时K-S比AUC更合适用作模型评估，如果没什么特别的影响，那么用AUC就很好了。</p>

<h1 id="toc_2">三、KS的计算和曲线绘制</h1>

<p>代码如果要用，最好自己改改，因为这个数据格式是我自己用的，计算方式也是跟数据格式有关系的，工作中用的可能不一样，所以最好是自己根据自己的写一个，这个也只是我根据自己的数据格式来写的，重要的是思路，不是代码本身。而且我画的曲线是折线，最好是自己考虑写一个平滑曲线的。</p>

<pre class="line-numbers"><code class="language-text">import matplotlib.pyplot as plt
#第一个参数是模型的预测值，第二个参数是模型的真实值
def draw_ks_curve(predict_result,true_result):
    tpr_list = []  #存放真正率数据
    fpr_list = []  #存放假正率数据
    dif_list = []  #存放真假正率差值
    max_ks_dot = []

    for i in np.arange(0,1.1,0.1):
        tpr = 0
        fpr = 0
        for j in range(len(predict_result)):
            if list(predict_result[j])[0]&gt;i and true_result[j]==1:
               tpr = tpr+1
               tpr_list.append(tpr)
            if list(predict_result[j])[0]&gt;i and true_result[j]==0:
               fpr = fpr+1
               fpr_list.append(fpr)
        tpr = tpr/sum(true_result)
        fpr = fpr/(len(true_result)-sum(true_result))
    fig = plt.figure(num=1, figsize=(15, 8),dpi=80)     #开启一个窗口，同时设置大小，分辨率
    plt.plot(np.arange(0,1,0.1),tpr_list)
    plt.plot(np.arange(0,1,0.1),fpr_list)

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mac 下高速免费下载百度云文件]]></title>
    <link href="http://dmlcoding.com/15343116796404.html"/>
    <updated>2018-08-15T13:41:19+08:00</updated>
    <id>http://dmlcoding.com/15343116796404.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">下载安装Aira2客户端</h1>

<p>地址: <a href="https://github.com/yangshun1029/aria2gui/releases">https://github.com/yangshun1029/aria2gui/releases</a></p>

<span id="more"></span><!-- more -->

<p><img src="media/15343116796404/15343175848538.jpg" alt="" style="width:900px;"/></p>

<h1 id="toc_1">安装Ghrome插件 BaiduExporter</h1>

<p>下载地址: <a href="https://github.com/acgotaku/BaiduExporter/archive/master.zip">https://github.com/acgotaku/BaiduExporter/archive/master.zip</a></p>

<p>这个只能这样安装<br/>
<img src="media/15343116796404/15343119219617.jpg" alt="" style="width:941px;"/></p>

<p><img src="media/15343116796404/15343119745283.jpg" alt="" style="width:1343px;"/><br/>
<img src="media/15343116796404/15343122583040.jpg" alt="" style="width:614px;"/></p>

<p><img src="media/15343116796404/15343121945795.jpg" alt="" style="width:743px;"/></p>

<p>ok,搞定了</p>

<h1 id="toc_2">总结</h1>

<p>所有的包上传百度云备份一份</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[windows10和mac共享文件]]></title>
    <link href="http://dmlcoding.com/15317970391028.html"/>
    <updated>2018-07-17T11:10:39+08:00</updated>
    <id>http://dmlcoding.com/15317970391028.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">一,windows10访问mac电脑</h1>

<h2 id="toc_1">1,先新建一个用于共享的登录账户</h2>

<p>在系统偏好设置-&gt;用户与群组-&gt;点击黄色的锁-&gt;输入当前账户密码-&gt;点击左侧➕ ,新建一个<em>仅限共享</em>的用户(注意:是将普通账户改为仅限共享,记住密码)。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_2">2,开启共享</h2>

<p>系统偏好设置 -&gt; 共享 -&gt; 点开黄色的锁输入密码 -&gt; 勾选文件共享和远程登录 -&gt; 选中共享文件 -&gt; 点击共享文件夹的➕，添加要共享的文件夹 -&gt; 点击用户➕添加新建的第1步中添加的帐号 -&gt; 点击选项勾选使用SMB -&gt; 勾选windows文件共享中的新添加的账户点击完成.</p>

<h2 id="toc_3">3,将mac加入到windows的工作组中</h2>

<p>系统偏好设置-&gt;网络-&gt;选择你所要设置的网络-右侧下方的高级按钮-wins选项卡.注意工作组要在windows上查找，wins服务器也要在windows上查找匹配。</p>

<h2 id="toc_4">4,windows中访问共享文件</h2>

<p>windows+R -&gt; 输入:&quot;\mac ip地址&quot; ,如 \192.168.1.102 -&gt; 在弹出的对话框中输入mac中添加的账户和密码就能访问了.</p>

<h1 id="toc_5">二，Mac访问windows10:</h1>

<h2 id="toc_6">1,打开共享设置</h2>

<p>1，控制面板-&gt; 网络和Internet -&gt; 网络和共享中心 -&gt; 高级共享设置 中专用和所有网络的都开启共享的设置.<br/>
2，文件资源管理器（默认选中快速访问）-&gt; 查看 -&gt; 选项 -&gt; 查看 -&gt; 勾选 使用共享向导(推荐).</p>

<h2 id="toc_7">2,在windows上新建用于mac登录的账户</h2>

<p>windows+R -&gt; 输入:netplwiz -&gt; 点击添加 -&gt; 选择不使用Microsoft帐户 -&gt; 本地账户 -&gt; 输入用户名，密码等信息（注意记住用于mac登录）.</p>

<h2 id="toc_8">3,设置要共享的文件夹</h2>

<p>1,选中文件夹右键 -&gt; 属性 -&gt; 共享 -&gt; 高级共享 -&gt; 勾选共享此文件夹 -&gt; 选中权限 进行相应的权限设置 -&gt; 点击应用，确定.<br/>
2,再次右键要共享的文件夹 -&gt; 属性 -&gt; 共享 -&gt; 共享(s) -&gt; 下拉箭头，添加之前第2部中增加的账户 -&gt; 设置权限级别.</p>

<h2 id="toc_9">4,mac中访问windows中的共享文件夹</h2>

<p>点击Finder -&gt; 左侧会有共享的设备 -&gt; 点击 连接身份 -&gt;输入第2步中创建的账户名和密码就可以访问了.</p>

<h2 id="toc_10">注意:</h2>

<p>2.3.4步就可以了</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HMM(摘自:统计学习方法)]]></title>
    <link href="http://dmlcoding.com/15299155104492.html"/>
    <updated>2018-06-25T16:31:50+08:00</updated>
    <id>http://dmlcoding.com/15299155104492.html</id>
    <content type="html"><![CDATA[
<p>HMM(隐马尔可夫模型的定义):隐马尔可夫模型是关于时序的概率模型,描述由一个隐藏的马尔可夫链随机生成不可观察的状态随机序列,再由各个状态生成一个观测而产生观测随机序列的过程.隐藏的马尔可夫链随机生成的状态的序列,称为状态序列(state sequence);每个状态生成一个观测,而由此产生的观测的随机序列,称为观测序列(observation sequence).序列的每一个位置又可以看作是一个时刻.</p>

<span id="more"></span><!-- more -->

<p><img src="media/15299155104492/15299160364725.jpg" alt=""/></p>

<h1 id="toc_0">隐马尔可夫模型的相关定义</h1>

<p>隐马尔可夫模型由初始概率分布,状态转移概率分布以及观测概率分布确定.隐马尔可夫模型的形式定义如下:<br/>
   设Q是所有可能的状态集合,V是所有可能的观测的集合,其中,N是可能的状态数,M是可能的观测数.</p>

<p>\[\begin{aligned}<br/>
   Q &amp; = {q_1,q_2,\cdots,q_N}  &amp;\text{Q是所有可能的状态集合} \\<br/>
   V &amp; = {v_1,v_2,\cdots,v_M} &amp; \text{V是所有可能的观测的集合} <br/>
\end{aligned}\]</p>

<p>I是长度为T的状态序列,O是对应的观测序列<br/>
  \[\begin{aligned}<br/>
   I &amp; = {i_1,i_2,\cdots,i_T}  &amp;\text{I是长度为T的状态序列} \\<br/>
   O &amp; = {o_1,o_2,\cdots,o_T} &amp; \text{O是对应的长度为T的观测序列} <br/>
\end{aligned}\]</p>

<p>A是状态转移概率矩阵:<br/>
\[A=[a_{ij}]_{N*N} \\\<br/>
其中,a_{ij}=P(i_{t+1}=q_j|i_t=q_i),\qquad i=1,2,\cdots,N;j=1,2,\cdots,N \\\<br/>
表示在时刻t处于状态q_i的条件下在时刻t+1转移到状态q_j的概率\]</p>

<p>B是观测概率矩阵:<br/>
\[B=[b_j(k)]_{N*M} \\\<br/>
其中,b_j(k)=P(o_t=v_k|i_t=q_j),\qquad k=1,2,\cdots,M;j=1,2,\cdots,N \\\<br/>
表示在时刻t处于状态q_j的条件下生成观测v_k的概率\]</p>

<p>\(\pi\)是初始状态概率向量<br/>
\[\pi=(\pi_i) \\\<br/>
其中,\pi_i=P(i_1=q_i),\qquad i=1,2,\cdots,N \\\<br/>
表示在时刻t=1处于状态q_i的概率\]</p>

<p>隐马尔可夫模型由初始状态概率向量\(\pi\),状态转移概率矩阵A和观测概率矩阵B决定,\(\pi\)和A决定状态序列,B决定观测序列.因此,隐马尔可夫模型\(\lambda\)可以用三元符合表示,即<br/>
\[\lambda=(\pi,A,B) \\\<br/>
A,B,\pi 称为隐马尔可夫模型的三要素\]</p>

<p>状态转移概率矩阵A与初始状态概率向量\(\pi\)确定了隐藏的马尔可夫链,生成不可观测的状态序列.观测概率矩阵B确定了如何从状态生成观测,与状态序列综合确定了如何产生观测序列.</p>

<p>从定义可知,隐马尔可夫模型作了两个基本假设:<br/>
(1)齐次马尔可夫假设,即假设隐藏的马尔可夫链在任意时刻t的状态只依赖于前一时刻的状态,与其他时刻的状态及观测无关,也与时刻t无关.<br/>
   \[P(i_t|i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(i_t|i_{t-1}),t=1,2,\cdots,T\]<br/>
(2)观测独立性假设,即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态,与其他观测及状态无关.<br/>
\[P(o_t|i_T,o_T,i_{T-1},o_{T-1},\cdots,i_{t+1},o_{t+1},i_t,o_t,i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(o_t|i_t)\]</p>

<p>观测序列的生成过程</p>

<p>根据隐马尔可夫模型定义,可以将一个长度为T的观测序列\(O=(o_1,o_2,\cdots,o_T)\)的生成过程描述如下:<br/>
输入:隐马尔可夫模型\(\lambda=(A,B,\pi)\),观测序列长度T;<br/>
输出:观测序列\(O=(o_1,o_2,\cdots,o_T)\)<br/>
(1) 按照初始状态分布\(\pi\)产生状态\(i_1\)<br/>
(2) 令t=1<br/>
(3) 按照状态\(i_t\)的观测概率分布\(b_{i_t}(k)生成o_t\)<br/>
(4) 按照状态\(i_t\)的状态转移概率分布\({a_{i_t j_{t+1}}}产生状态i_{t+1},i_{t+1}=1,2,\cdots,N\)<br/>
(5) 令t=t+1;如果t&lt;T,转步(3);否则,终止</p>

<h1 id="toc_1">前向算法</h1>

<p>首先定义前向概率:给定隐马尔可夫模型\(\lambda\),定义到时刻t部分观测序列\(o_1,o_2,\cdots,o_t\)且状态为\(q_i\)的概率为前向概率,记作<br/>
\[\alpha_t(i)=P(o_1,o_2,\cdots,o_t,i_t=q_i|\lambda)\]<br/>
可以递推地求得前向概率\( \alpha_t(i)\) 及观测序列概率 \(P(O|\lambda)\)</p>

<p><strong>观测序列概率的前向算法:</strong><br/>
输入:隐马尔可夫模型\(\lambda\),观测序列O;<br/>
输出:观测序列概率\(P(O|\lambda)\)<br/>
(1) 初值<br/>
\[\alpha_1(i)=\pi_ib_i(o_1),\qquad  i=1,2,\cdots,N\]<br/>
(2) 递推 <br/>
\[对t=1,2,\cdots,T-1, \\\<br/>
\alpha_{t+1}(i)= \bigg[\sum_{j=1}^N\alpha_t(j)\alpha_{ji} \bigg] b_i(\alpha_{t+1}),\qquad i=1,2,\cdots,N\]<br/>
(3) 终止<br/>
\[P(O|\lambda)=\sum_{i=1}^N\alpha_T(i)\]</p>

<p>前向算法,步骤(1)初始化前向概率,是初始时刻的状态\(i_1=q_i\)和观测\(o_1\)的联合概率.步骤(2)是前向概率的递推公式,计算到时刻t+1部分观测序列为\(o_1,o_2,\cdots,o_t,o_{t+1}\)且在时刻t+1处于状态\(q_i\)的前向概率,</p>

<p><img src="media/15299155104492/15299161632092.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[EM算法]]></title>
    <link href="http://dmlcoding.com/15299156181001.html"/>
    <updated>2018-06-25T16:33:38+08:00</updated>
    <id>http://dmlcoding.com/15299156181001.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[拉格朗日乘子法&KKT条件]]></title>
    <link href="http://dmlcoding.com/15289333499679.html"/>
    <updated>2018-06-14T07:42:29+08:00</updated>
    <id>http://dmlcoding.com/15289333499679.html</id>
    <content type="html"><![CDATA[
<p>如果不理解拉格朗日乘子法和KKT条件的相关原理,则不可能真正理解SVM的计算方法.</p>

<p>这是我们学习SVM的大前提</p>

<h1 id="toc_0">优化问题</h1>

<p>通常,我们要求解的函数优化问题,大致可分为以下3类</p>

<ul>
<li>无约束条件的优化问题:<br/>
\[min f(X)\]</li>
<li>只有等式约束的优化问题:<br/>
\[min f(X) \\\<br/>
s.t: h_i(X)=0,i \in {1,2,...,n}\]</li>
<li><p>含有不等式约束的优化问题:<br/>
\[min f(X) \\\<br/>
s.t: h_i(X)=0,i \in {1,2,...,n} \\\<br/>
     g_j(X) \leq 0, j \in {1,2,...,n} \]</p>
<span id="more"></span><!-- more -->        </li>
</ul>

<p>其中大写的X表示所有自变量的集合(也可以说是所有样本点的集合),\(h_i(X)\)表示等式约束条件,\(g_i(X)\)表示不等式约束条件.这里用\(min f(X)\)表示求取最优值的目标函数,其实不一定是求取最小值,求最大值也是可以的(取相反数即可).但是一般最优化都是取小值.</p>

<p>注意:默认所有解决的最优问题都是凸优化问题(即求取凸函数的最优解).拉格朗日乘子法一定是适用于凸优化问题,而不一定适用于其他非凸问题.</p>

<p>回到最上面的三类优化问题,解决方法是明确的,参考如下(注意:默认都是针对凸优化问题):</p>

<ul>
<li>对于无约束条件的优化问题,直接对其各个自变量求导,令导数为0,得全局最优解.</li>
<li>对于只有等式约束的优化问题,利用拉格朗日乘子法,得到全局最优解.</li>
<li>对于含不等式约束的优化问题,利用KKT条件,得全局最优解.</li>
</ul>

<h1 id="toc_1">拉格朗日乘子法</h1>

<p>举个例子来描述拉格朗日乘子法的基本思路:<br/>
\[min f(X)=2x_1^2+3x_2^2+7x_3^2 \\\<br/>
s.t : 2x_1+x_2=1,\quad 2x_2+3x_3=2\]</p>

<p>这个例子正式只有等式约束条件的优化问题,求解思路如下:<br/>
首先想到直接对3个自变量分别求偏导,令其偏导数都为0,则此时\(x_1,x_2,x_3\)都是0,看函数也是这样,当3个自变量都是0时,函数取得最小值0.但是这显然不符合约束条件的.</p>

<p>怎么把约束条件考虑进去呢?用拉格朗日乘子法,把改写后的约束条件乘以一个系数,然后以加的形式带入目标函数,该函数称之为拉格朗日函数.先看看改写后的约束条件:<br/>
\[s.t : 2x_1+x_2-1=0,\quad 2x_2+3x_3-2=0\]<br/>
左右移项,实际上没发生任何变化.其后,将改写后的约束条件带入目标函数,构造拉格朗日函数:<br/>
\[L(X,\alpha)=2x_1^2+3x_2^2+7x_3^2+\alpha_1(2x_1+x_2-1)+\alpha_2(2x_2+3x_3-2)\]</p>

<p>可见,当求得的最优解满足约束条件时,\(L(x,\alpha)\)与原始的目标函数并没有差别(后面都是0).这样,我们再对这个拉格朗日函数求关于各个自变量的偏导,并令这些偏导数为0.<br/>
\[<br/>
\begin{aligned}<br/>
\dfrac{\partial f(X)}{\partial x_1} &amp; =4x_1+2\alpha_1=0 \implies  x_1=-\frac12\alpha_1 \\<br/>
 \dfrac{\partial f(X)}{\partial x_2} &amp; =6x_2+\alpha_1+2\alpha_2=0 \implies  x_2=-\frac16\alpha_1 - \frac13\alpha_2 \\ <br/>
\dfrac{\partial f(X)}{\partial x_3} &amp; =14x_3+3\alpha_2=0 \implies  x_3=-\frac{3}{14}\alpha_2<br/>
\end{aligned}\]</p>

<p>现在,将用系数\(\alpha_1,\alpha_2\)表示的3个自变量带入约束条件,可得如下方程:<br/>
\[<br/>
\begin{aligned}<br/>
-\dfrac{7}{6}\alpha1 - \dfrac{1}{3}\alpha_2 -1 &amp; =0 \\<br/>
-\dfrac{1}{3}\alpha1 - \dfrac{55}{44}\alpha_2 -2 &amp; =0 <br/>
\end{aligned}\]</p>

<p>两个未知数,两个方程,可以求解得到\(\alpha_1=-0.45,\alpha_2=-1.41\),再带入自变量的表达式,即求得最优解.</p>

<h1 id="toc_2">拉格朗日乘子法的形式化描述</h1>

<p>用拉格朗日乘子法解决只有等式约束条件的优化问题,可以被如下形式化的描述:<br/>
现有优化问题:<br/>
    \[min f(X) \\<br/>
    s.t: h_i(X)=0,i \in {1,2,...,n}\]<br/>
则需要先得到拉格朗日函数\(L(X,\alpha)=f(X)+\sum_{i=1}^n \alpha_i \cdot h_i(X)\),对该函数的各个自变量求偏导,令偏导数为0,则可以求出各个自变量的含系数\(\alpha\)的代数式,再带入约束条件,解得\(\alpha\)后,可最终得到最优解.</p>

<h1 id="toc_3">KKT条件</h1>

<p>上面说的拉格朗日乘子法,解决的是只有等式约束条件的优化问题,而现实中更普遍的情况是求解含有不等式约束条件的问题.</p>

<p>还是通过一个例子讲解,优化问题如下:<br/>
\[min f(X)=x_1^2-2x_1+1+x_2^2+4x_2+4 \\\<br/>
s.t : x_1+10x_2 &gt; 10 \\\<br/>
10x_1-x_2 &lt; 10\]</p>

<p>第一步:把约束条件改写如下:<br/>
\[s.t : 10-x_1-10x_2&lt;0 \\\<br/>
10x_1-x_2-10&lt;0\]</p>

<p>改写的目的是方便后面的计算,且这种改写没有改变约束条件本身,所以不影响.</p>

<p>第二步:与拉格朗日乘子法相同,给约束条件乘以系数后加入目标函数,构成拉格朗日函数\(L(X,\beta)\)<br/>
\[L(X,\beta)=x_1^2-2x_1+1+x_2^2+4x_2+4+\beta_1(10-x_1-10x_2)+\beta_2(10x_1-x_2-10)\]</p>

<p>第三步:对拉格朗日函数的各个自变量求偏导:<br/>
\[<br/>
\begin{aligned}<br/>
\dfrac{\partial f(X)}{\partial x_1} &amp; =2x_1-\beta_1+10\beta_2 -2 \\<br/>
 \dfrac{\partial f(X)}{\partial x_2} &amp; =2x_2-10\beta_1-10\beta_2+4<br/>
\end{aligned}\]</p>

<p>做完以上3步,先不往下进行了.先给出KKT条件的形式化描述如下.</p>

<h1 id="toc_4">KKT条件的形式化描述</h1>

<p>KKT条件用于解决带有不等式约束条件的优化问题,可以被如下形式化描述:<br/>
现有优化问题:<br/>
\[min f(X) \\\<br/>
    s.t: h_i(X)=0,i \in {1,2,...,n} \\\<br/>
         g_j(X) \leq 0, j \in {1,2,...,n} \]<br/>
根据这个优化问题可以得到拉格朗日函数:<br/>
\[L(X,\alpha,\beta)=f(X)+\sum_{i=1}^m\alpha_i \cdot h_i(X)+\sum_{j=1}^m\beta_j \cdot g_j(X)\]</p>

<p>那么KKT条件就是函数的最优值必定满足下面3个条件(这3个条件是重点中的重点)</p>

<ul>
<li>\(L(X,\alpha,\beta)\)对各个自变量求导为0</li>
<li>h(X)=0</li>
<li>\(\sum_{i=1}^m\beta_j \cdot g_j(X)=0,\beta_j \geq 0\)</li>
</ul>

<p>3个条件中,前两个很好理解,不说了.关键在于第3个条件,这也是这篇文章中最难理解的部分.我尝试解释一下这个条件的原理.</p>

<p>首先,我们已知\(\beta_j \geq0,而g_j(X)\leq 0\)(这样设计的目的是要求目标函数的梯度和约束条件的梯度必须反向).也就是说,想要条件3成立,则每一项\(\beta_j \cdot g_j(X)都等于0\),再换个说法,对于每一项来说,要么\(\beta_j=0\),要么\(g_j(X)=0\).这就是条件3所要表达的真实含义.</p>

<p>下面解释为什么这样的条件可以用来寻找最优解.</p>

<p>假设X由2个自变量\(x_1,x_2\)组成,那么我们很容易想象出目标函数f(X)的图像,它就是一个三维空间中的曲面;再假设此时优化问题有3个不等式约束条件\(g_1(X),g_2(X),g_3(X)\).那现在可以把优化问题的图像画出来,如下图所示.画的是该问题再\(x_1 x_2\)平面上的投影,虚线为f(X)的等高线,那现在就有左图和右图两种不同情况了.<br/>
<img src="media/15289333499679/15289408757643.jpg" alt=""/></p>

<p>左图表示的是min f(X)在不等式的约束范围内,且不在不等式的约束面上.这时,f(X)的最优解与不等式的约束实际上没关系了,我们就令\(\beta_1=\beta_2=\beta3=0\)成立,而且此时,\(L(X,\alpha,\beta)\)对各个自变量求导为0的解就是最优解.换句话说,这种情况需要条件1,3同时成立.</p>

<h1 id="toc_5">参考</h1>

<p><a href="https://blog.csdn.net/on2way/article/details/47729419">https://blog.csdn.net/on2way/article/details/47729419</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SVM(支持向量机)]]></title>
    <link href="http://dmlcoding.com/15285262100303.html"/>
    <updated>2018-06-09T14:36:50+08:00</updated>
    <id>http://dmlcoding.com/15285262100303.html</id>
    <content type="html"><![CDATA[
<p>支持向量机(Support Vecor Machine,SVM)本身是一个<strong>二元分类算法</strong>,是对感知器算法模型的一种扩展,现在的SVM算法支持线性分类和非线性分类的分类应用,并且也能够直接将SVM应用于回归应用中.同时通过OvR或者OvO的方式我们也可以将SVM应用在多元分类领域中.在不考虑集成算法,不考虑特定的数据集的时候,在分类算法中SVM可以说是特别优秀的.</p>

<p><strong>线性可分(Linearly Separable)</strong>:在数据集中,如果可以找出一个超平面,将两组数据分开,那么这个数据集叫做线性可分数据.</p>

<p><strong>线性不可分(Linear Inseparable)</strong>:在数据集中,没法找出一个超平面,能够将两组数据分开,那么这个数据集就叫做线性不可分数据.</p>

<p><strong>分割超平面(Separating Hyperplane)</strong>:将数据集分割开来的直线/平面叫做分割超平面.</p>

<p><strong>间隔(Margin)</strong>:数据点到分割超平面的距离称为间隔.</p>

<p><strong>支持向量(Support Vector)</strong>:离分割超平面最近的那些点叫做支持向量.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">线性可分SVM</h1>

<p>在感知器模型中,算法是在数据中找出一个划分超平面,让尽可能多的数据分布在这个平面的两侧,从而达到分类的效果,但是在实际数据中这个符合我们要求的超平面是可能存在多个的.<br/>
<img src="media/15285262100303/15285276077823.jpg" alt=""/></p>

<p>在感知器模型中,我们可以找到多个可以分类的超平面将数据分开,并且优化时希望所有的点都离超平面尽可能的远,但是实际上离超平面足够远的点基本上都是被正确分类的,所以这个是没有意义的;反而比较关心那些离超平面很近的点,这些点比较容易分错.所以说我们只要让离超平面比较近的点尽可能的远离这个超平面,那么我们的模型分类效果应该就会比较不错了.(这个就是SVM的思想)<br/>
<img src="media/15285262100303/15285279858682.jpg" alt=""/></p>

<h2 id="toc_1">超平面和法向量</h2>

<p>先回顾一下同济高数上的一个例题,求解点到平面的距离.<br/>
<img src="media/15285262100303/15285475518085.jpg" alt=""/></p>

<p>常见的平面概念是在三维空间中定义的:\(Ax+By+Cz+D=0\).<br/>
而d维空间中的超平面由下面的方程确定:\(w^Tx+b=0\),其中w和x都是d维列向量,\(x=(x_1,x_2,...,x_d)^T\)为平面上的点,\(w=(w_1,w_2,...,w_d)^T\)为平面的法向量.b是一个实数,代表平面与原点之间的距离.</p>

<p><strong>点到超平面的距离</strong></p>

<p>假设点\(x^{&#39;}为超平面A:w^Tx+b=0上的任意一点,则点x到A的距离为x-x^{&#39;}在超平面法向量w上的投影长度:\)</p>

<p>\[d=\dfrac{|w^T(x-x^{&#39;})|}{||w||}=\dfrac{|w^Tx+b|}{||w||} \qquad 注意:w^Tx^{&#39;}=-b,x^{&#39;}为超平面上一点\]</p>

<p>这里,\(||w||是w的L_2范数\)</p>

<p><strong>超平面的正面和反面</strong><br/>
一个超平面可以将它所在的空间分为两半,它的法向量指向的那一半对应的一面是它的正面,另一面则是它的反面.</p>

<p><strong>法向量的意义</strong><br/>
法向量的大小是坐标原点到分离超平面的距离,垂直于分离超平面,方向由分离超平面决定.</p>

<h2 id="toc_2">线性可分SVM</h2>

<p>支持向量到超平面的距离为:<br/>
\[\because w^Tx+b=\pm1 \\\<br/>
\because y \in \{+1,-1\} \\\<br/>
\therefore \dfrac{|y(w^Tx+b)|}{||w||}=\dfrac{1}{||w||}\]</p>

<p>备注:在SVM中支持向量到超平面的函数距离一般设置为1<br/>
<img src="media/15285262100303/15285491659569.jpg" alt=""/></p>

<p>SVM模型是让所有的分类点在各自类别的支持向量的两边,同时要求支持向量尽可能的远离这个超平面,用数学公式表示如下:</p>

<p>注意:<br/>
\[<br/>
\begin{cases}<br/>
w^T=(w_1,w_2,\cdots,w_n)\\\ <br/>
||w||=\sqrt{w_1^2+w_2^2+\cdots+w_n^2}<br/>
\end{cases}<br/>
\]</p>

<p>\[<br/>
\begin{cases}<br/>
\max \limits_{w,b}\dfrac{1}{||w||} &amp;\text {目标}\\\ <br/>
s.t:y^{(i)}\left(w^Tx^{(i)}+b\right) \geq 1,i=1,2,...,m &amp;\text {限制条件}<br/>
\end{cases}<br/>
\]</p>

<p>一般求最优化问题,都是求最小值,因此稍微将这个优化问题转换一下<br/>
\[<br/>
\begin{cases}<br/>
\min \limits_{w,b}\dfrac12||w||^2 &amp;\text {目标}\\\ <br/>
s.t:y^{(i)}\left(w^Tx^{(i)}+b\right) \geq 1,i=1,2,...,m &amp;\text {限制条件}<br/>
\end{cases}<br/>
\]</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[聚类算法]]></title>
    <link href="http://dmlcoding.com/15284222434956.html"/>
    <updated>2018-06-08T09:44:03+08:00</updated>
    <id>http://dmlcoding.com/15284222434956.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->

<h1 id="toc_0">什么是聚类算法</h1>

<h1 id="toc_1">聚类算法的度量</h1>

<h1 id="toc_2">聚类的思想</h1>

<h1 id="toc_3">K-Means算法</h1>

<h1 id="toc_4">K-Means++算法</h1>

<h1 id="toc_5">K-Means||算法</h1>

<h1 id="toc_6">Canopy算法</h1>

<h1 id="toc_7">Mini Batch K-Means算法</h1>

<h1 id="toc_8">聚类算法的衡量指标</h1>

<h1 id="toc_9">层次聚类方法</h1>

<p>层次聚类方法对给定的数据集进行层次的分解,直到满足某种条件为止,传统的层次聚类算法主要分为两大类算法:<br/>
凝聚的层次聚类(AGNES算法:AGglomerative NESting):采用自底向上的策略.最初将每个对象作为一个簇,然后这些簇根据某些准则被一步一步合并,两个簇间的距离可以由这两个不同簇中距离最近的数据点的相似度来确定;聚类的合并过程反复进行直到所有的对象满足簇数目.</p>

<p>分裂的层次聚类(DIANA算法:DIvisive ANALysis):采用自顶向下的策略.首先将所有对象置于一个簇中,然后按照某种既定的规则逐渐细分为越来越小的簇(比如最大的欧式距离),直到达到某个终结条件(簇数目或者簇距离达到阈值).</p>

<p>AGNES和DIANA算法优缺点:</p>

<ul>
<li>简单,理解容易</li>
<li>合并点/分裂点选择不太容易</li>
<li>合并/分类的操作不能进行撤销</li>
<li>大数据集不太适合</li>
<li>执行效率较低</li>
</ul>

<p>AGNES算法中簇间距离</p>

<ul>
<li>最小距离(SL聚类)
<ul>
<li>两个聚簇中最近的两个样本之间的距离(single/word-linkage聚类法)</li>
<li>最终得到模型容易形成链式结构</li>
</ul></li>
<li>最大距离(CL聚类)
<ul>
<li>两个聚簇中最远的两个样本的距离(complete-linkage聚类法)</li>
<li>如果存在异常值,那么构建可能不太稳定</li>
</ul></li>
<li>平均距离(AL聚类)
<ul>
<li>两个聚类中样本间两两距离的平均值(average-linkage聚类法)</li>
<li>两个聚簇中样本间两两距离的中值(median-linkage聚类法)</li>
</ul></li>
</ul>

<h1 id="toc_10">密度聚类方法</h1>

<h1 id="toc_11">谱聚类</h1>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[集成学习]]></title>
    <link href="http://dmlcoding.com/15255701784620.html"/>
    <updated>2018-05-06T09:29:38+08:00</updated>
    <id>http://dmlcoding.com/15255701784620.html</id>
    <content type="html"><![CDATA[
<p>集成学习的思想是将若干个学习器(分类器&amp;回归器)组合之后产生一个新学习器.弱分类器(weak learner)指那些分类准确率只稍微好于随机猜测的分类器(error rate&lt; 0.5).</p>

<p>集成算法的成功在于保证弱分类器的多样性(Diversity).而且集成不稳定的算法也能够得到一个比较明显的性能提升.</p>

<p>常见的集成学习思想有:</p>

<ul>
<li>Bagging</li>
<li>Boosting</li>
<li>Stacking</li>
</ul>

<span id="more"></span><!-- more -->

<h2 id="toc_0">为什么需要集成学习</h2>

<ol>
<li>弱分类器间存在一定的差异性,这会导致分类的边界不同,也就是说可能存在错误.那么将多个弱分类器合并后,就可以得到更加合理的边界,减少整体的错误率,实现更好的效果.</li>
<li>对于数据集过大或者过小,可以分别进行划分和有放回的操作产生不同的数据子集,然后使用数据子集训练不同的分类器,最终再合并成为一个大的分类器.</li>
<li>如果数据的划分边界过于复杂,使用线性模型很难描述情况,那么可以训练多个模型,然后再进行模型的融合.</li>
<li>对于多个异构的特征集的时候,很难进行融合,那么可以考虑每个数据集构建一个分类模型,然后将多个模型融合.</li>
</ol>

<h1 id="toc_1">Bagging方法</h1>

<p>Bagging方法又叫自举汇聚法(Bootstrap Aggregating),思想是:在原始数据集上通过有放回的抽样的方式,重新选择出S个新数据集来分别训练S个分类器的集成技术.也就是说这些模型的训练数据中允许存在重复数据.</p>

<p>Bagging方法训练出来的模型在预测新样本分类的时候,会使用<strong>多数投票</strong>或者求<strong>均值</strong>的方式来统计最终的分类结果.</p>

<p>Bagging方法的弱学习器可以是基本的算法模型,eg:Linear,Ridge,Lasso,Logistic,Softmax,ID3,C4.5,CART,SVM,KNN等</p>

<p>备注:Bagging方式是有放回的抽样,并且每个子集的样本数量必须和原始样本数量一致,但是子集中允许存在重复数据.</p>

<p><img src="media/15255701784620/15259417755837.jpg" alt="Bagging方法-训练过程"/></p>

<h2 id="toc_2">随机森林(Random Forest)</h2>

<p>在Bagging策略的基础上进行修改后的一种算法</p>

<ol>
<li>从原始样本集(n个样本)中用Bootstrap采样(有放回重采样)选出n个样本;</li>
<li>从所有属性中随机选择k个属性,选择出最佳分割属性作为节点创建决策树;</li>
<li>重复以上两步m次,即建立m棵决策树;</li>
<li>这m个决策树形成随机森林,通过投票表决结果决定数据属于哪一类.</li>
</ol>

<h2 id="toc_3">随机森林的推广算法</h2>

<p>RF(随机森林)算法在实际应用中具有比较好的特性,应用也比较广泛,主要应用在:分类,回归,特征转换,异常点检测.常见的RF变种算法如下:</p>

<ul>
<li>Extra Tree</li>
<li>Totally Random Trees Embedding(TRTE)</li>
<li>lsolation Forest</li>
</ul>

<h2 id="toc_4">Extra Tree</h2>

<p>Extra Tree是RF的一个变种,原理基本和RF一样,区别如下:</p>

<ol>
<li>RF会随机采样作为子决策树的训练集,而Extra Tree每个子决策树采样原始数据集训练;</li>
<li>RF在选择划分特征点的时候会和传统决策树一样,会基于信息增益,信息增益率,基尼系数,均方差等原则来选择最优特征值;而Extra Tree会随机的选择一个特征值来划分决策树,</li>
</ol>

<p>Extra Tree因为是随机选择特征值的划分点,这样会导致决策树的规模一般大于RF所生成的决策树.也就是说Extra Tree模型的方差相对于RF进一步减少.在某些情况下,Extra Tree的泛华能力比RF的强.</p>

<h2 id="toc_5">Totally Random Trees Embedding(TRTE)</h2>

<p>TRTE是一种非监督的数据转化方式.将低维的数据集映射到高维,从而让映射到高维的数据更好的应用于分类回归模型.</p>

<p>TRTE算法的转换过程类似RF算法的方法,建立T个决策树来拟合数据.当决策树构建完成后,数据集里的每个数据在T个决策树叶子节点的位置就定下来了,将位置信息转换为向量就完成了特征转换操作.</p>

<h2 id="toc_6">Isolation Forest(IForest)</h2>

<p>IForest是一种异常点检测算法,使用类似RF的方式来检测异常点;IForest算法和RF算法的区别在于:</p>

<ol>
<li>在随机采样的过程中,一般只需要少量数据即可;</li>
<li>在进行决策树构建过程中,IForest算法会随机选择一个划分特征,并对划分特征随机选择一个划分阈值.</li>
<li>IForest算法构建的决策树一般深度max_depth是比较小的</li>
</ol>

<p>区别原因:目的是异常点检测,所以只要能够区分异常的即可,不需要大量数据;另外在异常点检测的过程中,一般不需要太大规模的决策树.</p>

<p>对于异常点的判断,则是将测试样本x拟合到T棵决策树上.计算在每棵树上该样本的叶子节点的深度\(h_t(x)\).从而计算出平均深度\(h(x)\);然后就可以使用下列公式计算样本点x的异常概率值,p(s,m)的取值范围为[0,1],越接近于1,则是异常点的概率越大.</p>

<p>\[p(x,m)=2^{-\dfrac{h(x)}{c(m)}}\]<br/>
\[c(m)=2\ln(m-1)+\xi-2\dfrac{m-1}{m} : m为样本个数,\xi为欧拉常数\]</p>

<h2 id="toc_7">RF随机森林总结</h2>

<ul>
<li><p>RF的主要优点</p>
<ul>
<li>1. 训练可以并行化,对于大规模样本的训练具有速度的优势;</li>
<li>2. 由于进行随机选择决策树划分特征列表,这样在样本维度比较高的时候,仍然具有比较高的训练性能;</li>
<li>3. 可以给出各个特征的重要性列表;</li>
<li>4. 由于存在随机抽样,训练出来的模型方差小,泛化能力强;</li>
<li>5. RF实现简单;</li>
<li>6. 对于部分特征的缺失不敏感.</li>
</ul></li>
<li><p>RF的主要缺点</p>
<ul>
<li>1. 在某些噪音比较大的特征上,RF模型容易陷入过拟合;</li>
<li>2. 取值比较多的划分特征对RF的决策会产生更大的影响,从而有可能影响模型的效果.</li>
</ul></li>
</ul>

<h2 id="toc_8">随机森林算法案例</h2>

<pre class="line-numbers"><code class="language-text">```
```
</code></pre>

<h2 id="toc_9">RF scikit-learn相关参数</h2>

<p><img src="media/15255701784620/15260264540993.jpg" alt=""/></p>

<h2 id="toc_10">随机森林的思考</h2>

<p>在随机森林的构建过程中,由于各棵树之间是没有关系的,相对独立的;在构建的过程中,构建第m棵子树的时候,不会考虑前面的m-1棵树.</p>

<p>思考:</p>

<ol>
<li>如果在构建第m棵子树的时候,考虑到前m-1棵子树的结果,会不会对最终结果产生有益的影响?</li>
<li>各个决策树组成随机森林后,在形成最终结果的时候能不能给定一种既定的决策顺序呢?</li>
</ol>

<h1 id="toc_11">Boosting</h1>

<p>提升学习(Boosting)是一种机器学习技术,可以用于回归和分类的问题,它每一步产生弱预测模型(如决策树),并加权累加到总模型中;如果每一步的弱项模型的生成都是依据损失函数的梯度方式的,那么就称为梯度提升(Gradient boosting).</p>

<p>提升技术的意义:如果一个问题存在弱预测模型,那么可以通过提升技术的方法得到一个强预测模型.</p>

<p>常见的模型有:</p>

<ul>
<li>Adaboost</li>
<li>Gradient Boosting(GBT/GBDT/GBRT)</li>
</ul>

<p><img src="media/15255701784620/15260302457703.jpg" alt=""/></p>

<h2 id="toc_12">Adaboost算法公式</h2>

<p>Adaboost算法将基分类器的线性组合作为强分类器,同时给分类误差率较小的基本分类器以大的权重,给分类误差率较大的基分类器以小的权重值;构建的线性组合为:<br/>
\[f(x)=\sum_{m=1}^M\alpha_mG_m(x)\]</p>

<p>最终分类器是在线性组合的基础上进行Sign函数转换:<br/>
\[G(x)=sign(f(x))=sign \left[\sum_{m=1}^M\alpha_mG_m(x) \right]\]</p>

<p>Sign函数<br/>
<img src="media/15255701784620/15260310974094.jpg" alt="Sign函数"/></p>

<h2 id="toc_13">AdaBoost算法原理</h2>

<p>Adaptive Boosting是一种迭代算法.每轮迭代中会在训练集上产生一个新的学习器,然后使用该学习器对所有样本进行预测,以评估每个样本的重要性(Informative).换句话来讲就是,算法会为每个样本赋予一个权重,每次用训练好的学习器标注/预测各个样本,如果某个样本点被预测的越正确,则将其权重降低;否则提高样本的权重.权重越高的样本在下一个迭代训练中所占的比重就越大,也就是说越难区分的样本在训练过程中会变得越重要.</p>

<p>整个迭代过程直到错误率足够小或者达到一定的迭代次数为止.</p>

<p>样本加权</p>

<p><img src="media/15255701784620/15260307178045.jpg" alt="样本加权"/></p>

<p>最终的强学习器<br/>
\[G(x)=sign(f(x))=sign \left[\sum_{m=1}^M\alpha_mG_m(x) \right]\]</p>

<p>损失函数<br/>
\[loss=\dfrac{1}{n}\sum_{i=1}^nI(G(x_i)\neq y_i)\]</p>

<p>损失函数<br/>
\[loss=\dfrac{1}{n}\sum_{i=1}^nI(G(x_i)\neq y_i) \leq \dfrac{1}{n}\sum_{i=1}^ne^{(-y_if(x))}\]</p>

<p>第k-1轮的强学习器<br/>
\[f_{k-1}(x)=\sum_{j=1}^{k-1}\alpha_jG_j(x)\]</p>

<p>第k轮的强学习器<br/>
\[f_{k}(x)=\sum_{j=1}^{k}\alpha_jG_j(x) \qquad<br/>
f_k(x)=f_{k-1}(x)+\alpha_kG_k(x)<br/>
\]</p>

<p>损失函数<br/>
\[loss(\alpha_m,G_m(x))=\dfrac{1}{n}\sum_{i=1}^ne^{(-y_i(f_{n-1}(x)+\alpha_mG_m(x)))}\]</p>

<p>未完待续</p>

<h2 id="toc_14">...</h2>

<h2 id="toc_15">AdaBoost总结</h2>

<ul>
<li><p>daBoost的优点如下:</p>
<ul>
<li>可以处理连续值和离散值;</li>
<li>模型的鲁棒性比较强;</li>
<li>解释强,结构简单.</li>
</ul></li>
<li><p>AdaBoost的缺点如下:</p>
<ul>
<li>对异常样本敏感,异常样本可能会在迭代过程中获得较高的权重值,最终影响模型效果.</li>
</ul></li>
</ul>

<h1 id="toc_16">Stacking</h1>

<p>Stacking是指训练一个模型用于组合(combine)其他模型(基模型/基学习器)的技术.即首先训练出多个不同的模型,然后再以之前训练的各个模型的输出作为输入来新训练一个新的模型,从而得到一个最终的模型.一般情况下使用单层的Logistic回归作为组合模型<br/>
<img src="media/15255701784620/15260322012383.jpg" alt=""/></p>

]]></content>
  </entry>
  
</feed>
