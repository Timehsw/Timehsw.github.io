<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Time渐行渐远]]></title>
  <link href="http://dmlcoding.com/atom.xml" rel="self"/>
  <link href="http://dmlcoding.com/"/>
  <updated>2018-03-01T14:51:08+08:00</updated>
  <id>http://dmlcoding.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch：用Curator辅助Marvel，实现自动删除旧marvel索引]]></title>
    <link href="http://dmlcoding.com/15198768495613.html"/>
    <updated>2018-03-01T12:00:49+08:00</updated>
    <id>http://dmlcoding.com/15198768495613.html</id>
    <content type="html"><![CDATA[
<p>Marvel几乎是所有Elasticsearch用户的标配。Marvel保留观测数据的代价是，<br/>
它默认每天会新建一个index，命名规律像是这样：.marvel-2017-12-10。<br/>
marvel自建的索引一天可以产生大概500M的数据，而且将会越来越多，占的容量也将越来越大。<br/>
有没有什么办法能让它自动过期？比如说只保留最近两天的观测数据，其他的都抛弃掉。</p>

<p>当然有办法,curator就可以帮你实现.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">curator是什么？</h1>

<p>它是一个命令，可以帮助你管理你在Elasticsearch中的索引，帮你删除，关闭（close），<br/>
打开（open）它们。当然这是比较片面的说法，更完整的说明见：<br/>
<a href="https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html">https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html</a></p>

<h1 id="toc_1">实践</h1>

<p>我们集群里面安装的Elasticsearch的版本是2.1.1.<br/>
按照官网,我装了最新的5.x版本,显示版本不对.<br/>
按照 <a href="http://blog.csdn.net/hereiskxm/article/details/47423715">http://blog.csdn.net/hereiskxm/article/details/47423715</a>  这个博客,我装了3.3.0版本.<br/>
显示也不对.</p>

<p>然后我搜了一下,感觉应该装一个中间的版本,因此我安装了4.0.0版本<br/>
<code><br/>
pip install elasticsearch-curator (4.0.0)<br/>
</code></p>

<p>然后我看了一下这个版本提供的参数</p>

<pre><code> curator --help
Usage: curator [OPTIONS] ACTION_FILE

  Curator for Elasticsearch indices.

  See http://elastic.co/guide/en/elasticsearch/client/curator/current

Options:
  --config PATH  Path to configuration file. Default: ~/.curator/curator.yml
  --dry-run      Do not perform any changes.
  --version      Show the version and exit.
  --help         Show this message and exit.
</code></pre>

<p>和我安装最新的5.X的版本看起来是一致的.正好在这个站点看到配置的办法<br/>
<a href="https://stackoverflow.com/questions/33430055/removing-old-indices-in-elasticsearch/42268400#42268400">https://stackoverflow.com/questions/33430055/removing-old-indices-in-elasticsearch/42268400#42268400</a></p>

<p>之前在博客里面看到的那个3.3.0版本,还不兼容呢.</p>

<h2 id="toc_2">用法</h2>

<blockquote>
<p>目的是删除2天前以.marvel开头的索引</p>
</blockquote>

<p>新建目录 /opt/curator</p>

<pre><code>~ pwd
/opt/curator
~ ll
total 12
-rw-r--r-- 1 root root  184 Dec 12 10:48 config_file.yml
-rw-r--r-- 1 root root 1311 Dec 12 10:37 delete_marvel_indices.yml
drwxr-xr-x 2 root root 4096 Dec 12 10:49 logs
</code></pre>

<h2 id="toc_3">config_file.yml</h2>

<pre><code># 记住,这个logfile得提前新建好.不然会启动报错.
vim config_file.yml

---
client:
  hosts:
   - 10.10.25.217
  port: 9200
logging:
  loglevel: INFO
  logfile: &quot;/opt/curator/logs/actions.log&quot;
  logformat: default
  blacklist: [&#39;elasticsearch&#39;, &#39;urllib3&#39;]

</code></pre>

<h2 id="toc_4">delete_marvel_indices.yml</h2>

<p>删除以.marvel前缀且是2天之前的索引</p>

<h2 id="toc_5">```</h2>

<h1 id="toc_6">Remember, leave a key empty if there is no value.  None will be a string,</h1>

<h1 id="toc_7">not a Python &quot;NoneType&quot;</h1>

<h1 id="toc_8">Also remember that all examples have &#39;disable_action&#39; set to True.  If you</h1>

<h1 id="toc_9">want to use this action as a template, be sure to set this to False after</h1>

<h1 id="toc_10">copying it.</h1>

<p>actions:<br/>
  1:<br/>
    action: delete_indices<br/>
    description: &gt;-<br/>
      Delete indices older than 30 days (based on index name), for rc- prefixed indices.<br/>
    options:<br/>
      ignore_empty_list: True<br/>
      timeout_override:<br/>
      continue_if_exception: False<br/>
      disable_action: False<br/>
    filters:<br/>
    - filtertype: pattern<br/>
      kind: prefix<br/>
      value: rc-<br/>
      exclude:<br/>
    - filtertype: age<br/>
      source: name<br/>
      direction: older<br/>
      timestring: &#39;%Y.%m.%d&#39;<br/>
      unit: days<br/>
      unit_count: 30<br/>
      exclude:<br/>
  2:<br/>
    action: delete_indices<br/>
    description: &gt;-</p>

<pre><code>  Delete indices older than 2 days (based on index name), for .marvel prefixed indices.
options:
  ignore_empty_list: True
  timeout_override:
  continue_if_exception: False
  disable_action: False
filters:
- filtertype: pattern
  kind: prefix
  value: .marvel
  exclude:
- filtertype: age
  source: name
  direction: older
  timestring: &#39;%Y.%m.%d&#39;
  unit: days
  unit_count: 2
  exclude:
</code></pre>

<pre><code>
配置完成.

## 执行命令

</code></pre>

<p>curator --config config_file.yml [--dry-run] delete_marvel_indices.yml<br/>
```</p>

<p>注意:<br/>
1. --dry-run 是可选参数,加上后不会真的删除,只会执行逻辑.你可以通过看日志来判断是否正确.<br/>
确认正确后,去掉--dry-run参数,再执行命令,既是真正的执行删除了.<br/>
2. 如果没有在config_file.yml里面配置logfile参数,那么日志会在console打印出来.</p>

<h1 id="toc_11">配置日常任务</h1>

<p>很明显,我们需要自动化这个过程,让它每天自动执行,因此写一个脚本,让crontab每天自动调用即可</p>

<pre><code>#!/bin/bash

curator --config /opt/curator/config_file.yml  /opt/curator/delete_marvel_indices.yml

echo &quot;delete success&quot;

</code></pre>

<p>配置crontab</p>

<pre><code># 每天2点执行删除脚本
0 2 * * * source /etc/profile;bash /opt/curator/delete_marvel_daily.sh &gt; /opt/curator/delete.log 2&gt;&amp;1

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PythonVirtualenv总结]]></title>
    <link href="http://dmlcoding.com/15198743666304.html"/>
    <updated>2018-03-01T11:19:26+08:00</updated>
    <id>http://dmlcoding.com/15198743666304.html</id>
    <content type="html"><![CDATA[
<p>我的电脑是macbookpro.我在电脑里面分别装了python2.7和python3.6.</p>

<p>当我用pip安装了virtual后,我如果想要对应版本的python</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">virtualenv</h1>

<h2 id="toc_1">安装</h2>

<pre><code>pip install virtualenv
</code></pre>

<h2 id="toc_2">virtualenv的参数</h2>

<pre><code>hushiwei@hsw  ~/virtual  virtualenv
You must provide a DEST_DIR
Usage: virtualenv [OPTIONS] DEST_DIR

Options:
  --version             show program&#39;s version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Increase verbosity.
  -q, --quiet           Decrease verbosity.
  -p PYTHON_EXE, --python=PYTHON_EXE
                        The Python interpreter to use, e.g.,
                        --python=python2.5 will use the python2.5 interpreter
                        to create the new environment.  The default is the
                        interpreter that virtualenv was installed with
                        (/usr/local/opt/python3/bin/python3.6)
  --clear               Clear out the non-root install and start from scratch.
  --no-site-packages    DEPRECATED. Retained only for backward compatibility.
                        Not having access to global site-packages is now the
                        default behavior.
  --system-site-packages
                        Give the virtual environment access to the global
                        site-packages.
  --always-copy         Always copy files rather than symlinking.
  --unzip-setuptools    Unzip Setuptools when installing it.
  --relocatable         Make an EXISTING virtualenv environment relocatable.
                        This fixes up scripts and makes all .pth files
                        relative.
  --no-setuptools       Do not install setuptools in the new virtualenv.
  --no-pip              Do not install pip in the new virtualenv.
  --no-wheel            Do not install wheel in the new virtualenv.
  --extra-search-dir=DIR
                        Directory to look for setuptools/pip distributions in.
                        This option can be used multiple times.
  --download            Download preinstalled packages from PyPI.
  --no-download, --never-download
                        Do not download preinstalled packages from PyPI.
  --prompt=PROMPT       Provides an alternative prompt prefix for this
                        environment.
  --distribute          DEPRECATED. Retained only for backward compatibility.
                        This option has no effect.
</code></pre>

<p>可以看到里面的--python参数可以指定虚拟环境的python版本.并且说明了默认是python3.6</p>

<h2 id="toc_3">创建虚拟环境</h2>

<h3 id="toc_4">virtual安装Python2.7</h3>

<pre><code>virtualenv --python=python python2env
</code></pre>

<h3 id="toc_5">virtual安装Python3.6环境</h3>

<pre><code>virtualenv --python=python3 python3env
</code></pre>

<h2 id="toc_6">激活进入虚拟环境</h2>

<pre><code>source python2env/bin/activate
source python3env/bin/activate
</code></pre>

<h2 id="toc_7">退出虚拟环境</h2>

<pre><code>deactivate
</code></pre>

<h2 id="toc_8">删除虚拟环境</h2>

<pre><code># 直接用rm删除目录就是删除虚拟环境了
rm -rf 虚拟环境的目录名称
</code></pre>

<h2 id="toc_9">激活虚拟环境,将全部依赖写入文件</h2>

<pre><code>pip freeze &gt; requirements.txt
</code></pre>

<p>进入项目内,安装全部依赖</p>

<pre><code>pip install -r requirements.txt
</code></pre>

<h1 id="toc_10">virtualenvwrapper</h1>

<blockquote>
<p>virtualenvwrapper是virtualenv的扩展管理包，用于更方便管理虚拟环境.</p>
</blockquote>

<p>他可以做;</p>

<ol>
<li>将所有虚拟环境整合在一个目录下</li>
<li>管理（新增，删除，复制）虚拟环境</li>
<li>切换虚拟环境</li>
</ol>

<h2 id="toc_11">安装</h2>

<pre><code>pip install virtualenvwrapper
</code></pre>

<h2 id="toc_12">使用方法</h2>

<p>1.初始化配置</p>

<p>默认virtualenvwrapper安装在/usr/local/bin下面，实际上需要运行virtualenvwrapper.sh文件才行；</p>

<p>所以需要先进行配置一下：</p>

<p>1.1 创建虚拟环境管理目录:</p>

<pre><code>mkdir $HOME/.local/virtualenvs
</code></pre>

<p>1.2 在~/.bash_profile中添加行</p>

<pre><code>export VIRTUALENV_USE_DISTRIBUTE=1        #  总是使用 pip/distribute
export WORKON_HOME=$HOME/.local/virtualenvs       # 所有虚拟环境存储的目录
if [ -e $HOME/.local/bin/virtualenvwrapper.sh ];then
    source $HOME/.local/bin/virtualenvwrapper.sh
else if [ -e /usr/local/bin/virtualenvwrapper.sh ];then
    source /usr/local/bin/virtualenvwrapper.sh
fi
  fi
export PIP_VIRTUALENV_BASE=$WORKON_HOME
export PIP_RESPECT_VIRTUALENV=true
</code></pre>

<p>2.使用方法</p>

<p>所有的命令可使用：<code>virtualenvwrapper --help</code> 进行查看，这里列出几个常用的：</p>

<ul>
<li><p>创建基本环境：mkvirtualenv [环境名]</p></li>
<li><p>删除环境：rmvirtualenv [环境名]</p></li>
<li><p>激活环境：workon [环境名]</p></li>
<li><p>退出环境：deactivate</p></li>
<li><p>列出所有环境：workon 或者 lsvirtualenv -b</p></li>
<li><p>在使用mkvirtualenv命令的时候,-p选项可以指定使用哪一个python环境</p></li>
</ul>

<p>3.举例</p>

<p>安装python2.7</p>

<pre><code>mkvirtualenv -p python python2env
</code></pre>

<p>安装python3.6</p>

<pre><code>mkvirtualenv -p python3 python3env
</code></pre>

<p>查看现在装了几个虚拟环境</p>

<pre><code>hushiwei@hsw  ~  workon
python2env
python3env
</code></pre>

<p>所有命令都可在后面使用<code>--help</code>参数查看具体用法！Enjoy it !</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[机器学习实战之朴素贝叶斯]]></title>
    <link href="http://dmlcoding.com/15198746079468.html"/>
    <updated>2018-03-01T11:23:27+08:00</updated>
    <id>http://dmlcoding.com/15198746079468.html</id>
    <content type="html"><![CDATA[
<p>朴素贝叶斯就是利用先验知识来解决后验概率，因为训练集中我们已经知道了每个单词在类别0和1中的概率，即p(w|c),<br/>
我们就是要利用这个知识去解决在出现这些单词的组合情况下，类别更可能是0还是1,即p(c|w)。<br/>
如果说之前的训练样本少，那么这个p(w|c)就更可能不准确，所以样本越多我们会觉得这个p(w|c)越可信。</p>

<span id="more"></span><!-- more -->

<pre><code class="language-python">import os
import sys
from numpy import *
sys.path.append(os.getcwd())
</code></pre>

<pre><code class="language-python">import bayes
</code></pre>

<pre><code class="language-python"># 返回实验样本和类别标签(侮辱类和非侮辱类)
listOPosts,listClasses=bayes.loadDataSet()
</code></pre>

<pre><code class="language-python"># 创建词汇表
# 将实验样本里面的词汇进行去重
def createVocabList(dataSet):
    vocabSet=set([])
    for document in dataSet:
        vocabSet=vocabSet|set(document)
    return list(vocabSet)
</code></pre>

<pre><code class="language-python">myVocabList=createVocabList(listOPosts)
</code></pre>

<pre><code class="language-python"># 将词汇转成特征向量
# 也就是将每一行样本转成特征向量
# 向量的每一元素为1或者0,分别表示词汇表中的单词在输入文档中是否出现
def setOfWordsVec(vocabList,inputSet):
    returnVec=[0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)]=1
        else:print &quot;the word:%s is not in my Vocabulary!&quot; % word
    return returnVec
</code></pre>

<h1 id="toc_0">训练算法:从词向量计算概率</h1>

<p>朴素贝叶斯分类器训练函数</p>

<pre><code class="language-python"># 输入为文档矩阵以及由每篇文档类别标签所构成的向量
def trainNB0(trainMatrix,trainCategory):
    # 文档总数,有几篇文档,在这里也就是有几个一维数组
    numTrainDocs=len(trainMatrix)
    # 每篇文档里面的单词数,也就是一维数组的长度
    numWords=len(trainMatrix[0])
    # 因为就只有0和1两个分类,将类别列表求和后,就是其中一个类别的个数
    # 然后numTrainDocs也就是文档总数,这样相除后就是这个类别的概率了
    pAbusive=sum(trainCategory)/float(numTrainDocs)
    # 以下两行,初始化概率
    p0Num=ones(numWords);p1Num=ones(numWords)
    p0Denom=2.0;p1Denom=2.0

    # 依次遍历所有的文档
    for i in range(numTrainDocs):
        # 判断这个文档所属类别
        if trainCategory[i]==1:
            # 数组与数组相加,这里就是统计每个词在这个分类里面出现的次数
            p1Num+=trainMatrix[i]
            # 统计该类别下,这些词语一共出现了多少次
            p1Denom+=sum(trainMatrix[i])
        else:
            p0Num+=trainMatrix[i]
            p0Denom+=sum(trainMatrix[i])
    # 通过求对数避免数据下溢出
    p1Vect=log(p1Num/p1Denom)
    p0Vect=log(p0Num/p0Denom)
    return p0Vect,p1Vect,pAbusive
</code></pre>

<pre><code class="language-python"># 所有文档的特征向量
trainMat=[]
</code></pre>

<pre><code class="language-python"># 将文档的每一行,转成词向量,然后追加到trainMat中
for postinDoc in listOPosts:
    trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc))
</code></pre>

<pre><code class="language-python">p0V,p1V,pAb=trainNB0(trainMat,listClasses)
</code></pre>

<pre><code class="language-python">pAb
</code></pre>

<pre><code>0.5
</code></pre>

<pre><code class="language-python">p0V
</code></pre>

<pre><code>array([-2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,
       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -2.56494936,
       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,
       -2.15948425, -3.25809654, -3.25809654, -2.56494936, -3.25809654,
       -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,
       -2.56494936, -3.25809654, -2.56494936, -3.25809654, -2.56494936,
       -2.56494936, -1.87180218])
</code></pre>

<pre><code class="language-python">p1V
</code></pre>

<pre><code>array([-3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -2.35137526, -2.35137526, -2.35137526, -3.04452244, -1.94591015,
       -3.04452244, -2.35137526, -2.35137526, -3.04452244, -1.94591015,
       -3.04452244, -1.65822808, -3.04452244, -2.35137526, -3.04452244,
       -3.04452244, -3.04452244])
</code></pre>

<pre><code class="language-python"># 朴素贝叶斯分类函数
def classifyNB(vec2classify,p0Vec,p1Vec,pClass1):
    #元素相乘
    p1=sum(vec2classify*p1Vec)+log(pClass1)
    p0=sum(vec2classify*p0Vec)+log(pClass1)
    if p1&gt;p0:
        return 1
    else:
        return 0
</code></pre>

<pre><code class="language-python">def testingNB():
    listOPosts,listClasses=bayes.loadDataSet()
    myVocabList=createVocabList(listOPosts)
    trainMat=[]
    for postinDoc in listOPosts:
        trainMat.append(setOfWordsVec(myVocabList,postinDoc))

    p0V,p1V,pAb=trainNB0(array(trainMat),array(listClasses))

    testEntry=[&#39;love&#39;,&#39;my&#39;,&#39;dalmation&#39;]
    thisDoc=array(setOfWordsVec(myVocabList,testEntry))
    print testEntry,&#39;classified as : &#39;,classifyNB(thisDoc,p0V,p1V,pAb)

    testEntry=[&#39;stupid&#39;,&#39;garbage&#39;]
    thisDoc=array(setOfWordsVec(myVocabList,testEntry))
    print testEntry,&#39;classified as : &#39;,classifyNB(thisDoc,p0V,p1V,pAb)
</code></pre>

<pre><code class="language-python">testingNB()
</code></pre>

<pre><code>[&#39;love&#39;, &#39;my&#39;, &#39;dalmation&#39;] classified as :  0
[&#39;stupid&#39;, &#39;garbage&#39;] classified as :  1
</code></pre>

<h1 id="toc_1">使用朴素贝叶斯过滤垃圾邮件</h1>

<ul>
<li>收集数据：提供文本文件</li>
<li>准备数据：将文本文件解析成词条向量</li>
<li>分析数据：检查词条确保解析的正确性</li>
<li>训练算法：使用我们之前建立的trainNB0()函数</li>
<li>测试算法：使用classifyNB()</li>
</ul>

<pre><code class="language-python">mySent=&#39;This book is the best book on Python or M.L. I have ever laid eyes upon.&#39;
</code></pre>

<pre><code class="language-python"># 文件解析及完整的垃圾邮件测试函数
def textParse(bigString):
    import re
    listOfTokens=re.split(r&#39;\W*&#39;,bigString)
    return [tok.lower() for tok in listOfTokens if len(tok)&gt;2]
</code></pre>

<pre><code class="language-python">def spamTest():
    docList=[];classList=[];fullText=[]
    for i in range(1,26):
        # 导入邮件文本，并解析成词条
        wordList=textParse(open(&#39;email/spam/%d.txt&#39; %i).read())
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(1)

        wordList=textParse(open(&#39;email/ham/%d.txt&#39; %i).read())
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(0)
    # 生成词汇表
    vocabList=createVocabList(docList)

    # 随机构建训练集、测试集
    trainingSet=range(50);testSet=[]
    for i in range(10):
        randIndex=int(random.uniform(0,len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])

    # 生成测试集的特征向量
    trainMat=[];trainClasses=[]
    for docIndex in trainingSet:
        trainMat.append(setOfWordsVec(vocabList,docList[docIndex]))
        trainClasses.append(classList[docIndex])

    p0V,p1V,pSpam=trainNB0(trainMat,trainClasses)

    # 测试集，测试错误率
    errorCount=0
    for docIndex in testSet:
        wordVector=setOfWordsVec(vocabList,docList[docIndex])
        if classifyNB(wordVector,p0V,p1V,pSpam)!=classList[docIndex]:
            errorCount+=1
    print &#39;the error rate is : &#39;,float(errorCount)/len(testSet)
</code></pre>

<pre><code class="language-python">spamTest()
</code></pre>

<pre><code>the error rate is :  0.2
</code></pre>

<h1 id="toc_2">使用朴素贝叶斯分类器从个人广告中获取区域倾向</h1>

<ul>
<li>收集数据:从RSS源收集内容,这里需要对RSS源构建一个接口</li>
<li>准备数据:将文本文件解析成词条向量</li>
<li>分析数据:检查词条确保解析的正确性</li>
<li>训练算法:使用我们之前建立的trainNB0()函数</li>
<li>测试算法:观察错误率,确保分类器可用.可以修改切分程序,以降低错误率,提高分类结果.</li>
<li>使用算法:构建一个完整的程序,封装所有内容.给定两个RSS源,该程序会显示最常用的公共词.</li>
</ul>

<p>下面将使用来自不同城市的广告训练一个分类器，然后观察分类器的效果。我们的目的并不是使用该分类器进行分类，而是通过观察单词和条件概率值来发现与特定城市相关的内容。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Presto的学习笔记]]></title>
    <link href="http://dmlcoding.com/15198745641385.html"/>
    <updated>2018-03-01T11:22:44+08:00</updated>
    <id>http://dmlcoding.com/15198745641385.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">是什么?可以做什么?</h1>

<ol>
<li>Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节。</li>
<li>Presto支持在线数据查询，包括Hive, Cassandra, 关系数据库以及专有数据存储。 一条Presto查询可以将多个数据源的数据进行合并，可以跨越整个组织进行分析。</li>
<li>作为Hive和Pig（Hive和Pig都是通过MapReduce的管道流来完成HDFS数据的查询）的替代者，Presto不仅可以访问HDFS，也可以操作不同的数据源，包括：RDBMS和其他的数据源（例如：Cassandra）。</li>
<li>查询后的数据自动分页,这个很不错.</li>
</ol>

<span id="more"></span><!-- more -->

<h1 id="toc_1">源码编译</h1>

<p>下载<strong>presto</strong>源码包地址:<a href="https://github.com/prestodb/presto/releases">https://github.com/prestodb/presto/releases</a></p>

<p>安装文档地址(注意这个中文文档的版本是0.100):</p>

<p>注意:</p>

<ul>
<li>jdk得是1.8以上</li>
<li>我是用的presto0.161</li>
</ul>

<pre><code>tar -xzvf presto-0.161.tar.gz

# 编译
./mvnw clean install -DskipTests
</code></pre>

<p>在pom.xml文件中加入阿里云的仓库,加速下载依赖</p>

<pre><code>&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;nexus-aliyun&lt;/id&gt;
        &lt;name&gt;Nexus aliyun&lt;/name&gt;
        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
</code></pre>

<p>编译报错</p>

<pre><code>[ERROR] Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.13:revision (default) on project presto-spi: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml -&gt; [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.13:revision (default) on project presto-spi: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:185)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:181)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.maven.plugin.MojoExecutionException: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml
    at pl.project13.maven.git.GitCommitIdMojo.throwWhenRequiredDirectoryNotFound(GitCommitIdMojo.java:432)
    at pl.project13.maven.git.GitCommitIdMojo.execute(GitCommitIdMojo.java:337)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
</code></pre>

<p>解决办法</p>

<pre><code>pom文件中加入这个插件
&lt;pluginManagement&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;pl.project13.maven&lt;/groupId&gt;
                &lt;artifactId&gt;git-commit-id-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;skip&gt;true&lt;/skip&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/pluginManagement&gt;
</code></pre>

<h1 id="toc_2">部署包安装</h1>

<h2 id="toc_3">下载地址</h2>

<p>下载presto-cli(记住要下载的presto-cli-xxxx-executable.jar):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli">https://repo1.maven.org/maven2/com/facebook/presto/presto-cli</a></p>

<p>下载部署包的地址:<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-server/">https://repo1.maven.org/maven2/com/facebook/presto/presto-server/</a></p>

<h2 id="toc_4">服务器说明</h2>

<p>机器共有三台U006,U007,U008</p>

<ul>
<li>coordinator

<ul>
<li>U007</li>
</ul></li>
<li>discovery

<ul>
<li>U007</li>
</ul></li>
<li>worker

<ul>
<li>U006</li>
<li>U008</li>
</ul></li>
</ul>

<p>coordinator和worker的其他配置都是一样的,除了config.properties不一样.具体哪里不一样,看下面的配置说明.</p>

<h1 id="toc_5">配置说明</h1>

<p>以下配置均是presto0.161版本的.如果你的版本不一样,启动后如果报错了,那么可能是配置文件里面的参数和版本对应不上,请找相应版本的配置.</p>

<p>在presto-server-0.161目录下新建etc目录,下面的配置文件均在此etc目录下</p>

<pre><code>[druid@U007 presto-server-0.161]$ tree etc/
etc/
├── catalog
│   ├── hive.properties
│   └── jmx.properties
├── config.properties
├── jvm.config
├── log.properties
└── node.properties
</code></pre>

<h2 id="toc_6">jvm.config</h2>

<blockquote>
<p>包含一系列在启动JVM的时候需要使用的命令行选项。这份配置文件的格式是：一系列的选项，每行配置一个单独的选项。由于这些选项不在shell命令中使用。 因此即使将每个选项通过空格或者其他的分隔符分开，java程序也不会将这些选项分开，而是作为一个命令行选项处理,信息如下：</p>
</blockquote>

<p>VM 系统属性 <code>HADOOP_USER_NAME</code> 来指定用户名</p>

<pre><code>-server
-Xmx16G
-XX:+UseG1GC
-XX:G1HeapRegionSize=32M
-XX:+UseGCOverheadLimit
-XX:+ExplicitGCInvokesConcurrent
-XX:+HeapDumpOnOutOfMemoryError
-XX:OnOutOfMemoryError=kill -9 %p
-DHADOOP_USER_NAME=hdfs
</code></pre>

<h2 id="toc_7">log.properties</h2>

<blockquote>
<p>这个配置文件中允许你根据不同的日志结构设置不同的日志级别。每个logger都有一个名字（通常是使用logger的类的全标示类名）. Loggers通过名字中的“.“来表示层级和集成关系，信息如下：</p>
</blockquote>

<pre><code>com.facebook.presto=INFO
</code></pre>

<ul>
<li>配置日志等级，类似于log4j。四个等级：DEBUG,INFO,WARN,ERROR</li>
</ul>

<h2 id="toc_8">node.properties</h2>

<blockquote>
<p>包含针对于每个节点的特定的配置信息。 一个节点就是在一台机器上安装的Presto实例，<strong>etc/node.properties</strong>配置文件至少包含如下配置信息</p>
</blockquote>

<pre><code>node.environment=production
node.id=ffffffff-ffff-ffff-ffff-ffffffffffff # 每个节点的node.id一定要不一样
node.data-dir=/home/druid/data/presto # 计算临时存储目录,presto得有读写权限
</code></pre>

<p>说明:</p>

<ol>
<li>node.environment： 集群名称, 所有在同一个集群中的Presto节点必须拥有相同的集群名称.</li>
<li>node.id： 每个Presto节点的唯一标示。每个节点的node.id都必须是唯一的。在Presto进行重启或者升级过程中每个节点的node.id必须保持不变。如果在一个节点上安装多个Presto实例（例如：在同一台机器上安装多个Presto节点），那么每个Presto节点必须拥有唯一的node.id.</li>
<li>node.data-dir： 数据存储目录的位置（操作系统上的路径）, Presto将会把日期和数据存储在这个目录下</li>
</ol>

<h2 id="toc_9">config.properties</h2>

<h3 id="toc_10">coordinator 主节点配置</h3>

<pre><code>coordinator=true
node-scheduler.include-coordinator=false
http-server.http.port=8585
query.max-memory=10GB
discovery-server.enabled=true
discovery.uri=http://U007:8585
</code></pre>

<p>说明:</p>

<ol>
<li><p>coordinator表示此节点是否作为一个coordinator。每个节点可以是一个worker，也可以同时是一个coordinator，但作为性能考虑，一般大型机群最好将两者分开。</p></li>
<li><p>若coordinator设置成true，则此节点成为一个coordinator。</p></li>
<li><p>若node-scheduler.include-coordinator设置成true，则成为一个worker，两者可以同时设置成true，此节点拥有两种身份。在一个节点上的Presto server即作为coordinator又作为worke将会降低查询性能。因为如果一个服务器作为worker使用，那么大部分的资源都会被worker占用，那么就不会有足够的资源进行关键任务调度、管理和监控查询执行.</p></li>
<li><p>http-server.http.port：指定HTTP server的端口。Presto 使用 HTTP进行内部和外部的所有通讯.</p></li>
<li><p>query.max-memory=10GB：一个单独的任务使用的最大内存 (一个查询计划的某个执行部分会在一个特定的节点上执行)。 这个配置参数限制的GROUP BY语句中的Group的数目、JOIN关联中的右关联表的大小、ORDER BY语句中的行数和一个窗口函数中处理的行数。 该参数应该根据并发查询的数量和查询的复杂度进行调整。如果该参数设置的太低，很多查询将不能执行；但是如果设置的太高将会导致JVM把内存耗光.</p></li>
<li><p>discovery-server.enabled：Presto 通过Discovery 服务来找到集群中所有的节点。为了能够找到集群中所有的节点，每一个Presto实例都会在启动的时候将自己注册到discovery服务。Presto为了简化部署，并且也不想再增加一个新的服务进程，Presto coordinator 可以运行一个内嵌在coordinator 里面的Discovery 服务。这个内嵌的Discovery 服务和Presto共享HTTP server并且使用同样的端口.</p></li>
<li><p>discovery.uri：Discovery server的URI。由于启用了Presto coordinator内嵌的Discovery 服务，因此这个uri就是Presto coordinator的uri。注意：这个URI一定不能以“/“结尾</p></li>
</ol>

<h3 id="toc_11">worker节点配置</h3>

<pre><code>coordinator=false
node-scheduler.include-coordinator=true
http-server.http.port=8585
query.max-memory=5GB
query.max-memory-per-node=1GB
discovery.uri=http://U007:8585
</code></pre>

<h2 id="toc_12">catalog</h2>

<p>hive.properties(hive连接器的配置)</p>

<p>连接hive</p>

<pre><code># 在etc/catalog目录下,新建hive.properties文件,配置上hive的一些信息
[druid@U006 catalog]$ pwd
/home/druid/presto-server-0.161/etc/catalog
[druid@U006 catalog]$ more hive.properties
connector.name=hive-cdh5
hive.metastore.uri=thrift://U006:9083
hive.config.resources=/etc/hadoop/conf.cloudera.yarn/core-site.xml,/etc/hadoop/conf.clouder
a.yarn/hdfs-site.xml
</code></pre>

<p>保证每个节点presto对core-site.xml,hdfs-site.xml两个文件有读权限</p>

<h1 id="toc_13">启动停止presto</h1>

<h2 id="toc_14">单节点启动</h2>

<p>在每个节点依次执行启动脚本</p>

<pre><code># 后台运行
bin/launcher start
# 前台运行
bin/launcher run

# 重启presto
bin/launcher restart

# 停止presto
bin/launcher stop
</code></pre>

<h2 id="toc_15">批量启动停止脚本</h2>

<pre><code>ssh -t ${i} -C &#39;. /usr/local/bin/env.sh &amp;&amp; /usr/local/presto-server-0.161/bin/launcher restart&#39;
</code></pre>

<h1 id="toc_16">监控presto</h1>

<p>启动完成后,在浏览器输入:</p>

<pre><code>http://U007:8585
</code></pre>

<p>这个地址也就是coordinator的discovery.uri</p>

<h1 id="toc_17">cli连接</h1>

<h2 id="toc_18">连接器注意说明</h2>

<ul>
<li><p>cli下载地址(找到自己的版本下载):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli">https://repo1.maven.org/maven2/com/facebook/presto/presto-cli</a></p></li>
<li><p>连接器的配置文件必须是以<code>.properties</code>后缀结尾的,前面的名字就是连接器的catalog名字</p></li>
<li><p>每次新加连接器配置文件后,都需要在presto的所有机器上加上相同的配置文件,然后重启</p></li>
<li><p>要下载对应版本的cli连接器,不然可能不好使.名字类似<code>presto-cli-0.161-executable.jar</code></p></li>
</ul>

<h2 id="toc_19">hive连接器</h2>

<p>配置说明(hive连接器的配置在说catalog的时候已经配置好了,你可以回头看看):</p>

<ul>
<li>connector.name=hive-cdh5(根据你的hive版本来选择)</li>
<li>hive.metastore.uri=thrift://U006:9083(hive的metastore地址)</li>
<li>hive.config.resources=/etc/hadoop/conf.cloudera.yarn/core-(配置文件的地址)</li>
</ul>

<pre><code>chmod +x presto-cli-0.161-executable.jar
./presto-cli-0.161-executable.jar --server U007:8585 --catalog hive --schema default
或者 mv presto-cli-0.161-executable.jar presto都可以
./presto --server U007:8585 --catalog hive --schema default
执行该语句后在 presto shell 中执行: show tables 查看 hive 中的 default 库下的表。如果出现对应的表，表安装验证成功
</code></pre>

<h2 id="toc_20">jmx连接器</h2>

<ul>
<li>JMX提供了有关JVM中运行的Java虚拟机和软件的信息</li>
<li>jmx连接器用于在presto服务器中查询JMX信息</li>
</ul>

<p>在etc/catalog目录下新建<strong>jmx.properties</strong></p>

<pre><code>connector.name=jmx
</code></pre>

<p>现在连接presto cli以启用JMX插件</p>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog jmx --schema jmx
presto:jmx&gt; show schemas from jmx;
       Schema
--------------------
 current
 history
 information_schema
(3 rows)

Query 20171012_063601_00020_yuhat, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:00 [3 rows, 47B] [39 rows/s, 614B/s]
</code></pre>

<h2 id="toc_21">MySQL连接器</h2>

<p>vim etc/catalog/mysql.properties</p>

<pre><code>connector.name=mysql
connection-url=jdbc:mysql://10.10.25.13:3306
connection-user=root
connection-password=wankatest***
</code></pre>

<p>schema 后面跟的mysql的数据库,</p>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog mysql --schema test
presto:test&gt; show tables;
Query 20171012_071844_00002_iz4q8 failed: No worker nodes available

presto:test&gt; show tables;
          Table
--------------------------
 dmp_summary_daily_report
 tb_dmp_stat_appboot
 tb_dmp_stat_asdk_detail
 tb_dmp_stat_device
 tb_leidian1
 tb_leidian2
(6 rows)

Query 20171012_072024_00003_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:01 [6 rows, 190B] [6 rows/s, 196B/s]
</code></pre>

<h2 id="toc_22">kafka连接器</h2>

<p>暂时没这个需求,未测试.</p>

<h2 id="toc_23">系统连接器</h2>

<ul>
<li>系统连接器提供了正在运行的Presto集群的一些信息和指标</li>
<li>那么这个就可以通过标准sql很方便的查询这些信息</li>
<li>系统连接器不需要配置,已经内置了.我们可以很方便的访问名为<code>system</code>的catalog</li>
</ul>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog system
presto&gt; show schemas from system;
       Schema
--------------------
 information_schema
 jdbc
 metadata
 runtime
(4 rows)

Query 20171012_082437_00019_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:00 [4 rows, 57B] [70 rows/s, 997B/s]
</code></pre>

<p>查询有多少个节点</p>

<pre><code>presto&gt; SELECT * FROM system.runtime.nodes;
                  node_id                  |        http_uri         | node_version | coord
-------------------------------------------+-------------------------+--------------+------
 ffffffff-ffff-ffff-ffff-ffffffffffff-u006 | http://10.10.25.13:8585 | 0.161        | false
 ffffffff-ffff-ffff-ffff-ffffffffffff-u007 | http://10.10.25.14:8585 | 0.161        | true
 ffffffff-ffff-ffff-ffff-ffffffffffff-u008 | http://10.10.25.15:8585 | 0.161        | false
(3 rows)

Query 20171012_082952_00023_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
4:08 [3 rows, 228B] [0 rows/s, 0B/s]
</code></pre>

<h2 id="toc_24">JDBC接口</h2>

<h3 id="toc_25">依赖下载安装</h3>

<ul>
<li>下载地址(找到自己的版本下载):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc">https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc</a></li>
<li><code>presto-jdbc-0.161.jar</code>在jar文件下载之后，将其添加到Java应用程序的classpath中。</li>
<li>我不太喜欢用jar包的方式,那么可以在pom文件加入presto-jdbc的依赖</li>
<li><a href="http://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc">http://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc</a> 找到自己相应的版本</li>
</ul>

<pre><code>&lt;!-- https://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.facebook.presto&lt;/groupId&gt;
    &lt;artifactId&gt;presto-jdbc&lt;/artifactId&gt;
    &lt;version&gt;0.161&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Presto支持的URL格式如下：</p>

<pre><code>jdbc:presto://host:port
jdbc:presto://host:port/catalog
jdbc:presto://host:port/catalog/schema
</code></pre>

<p>例如，可以使用下面的URL来连接运行在U007服务器8585端口上的Presto的mysql catalog中的test schema：</p>

<pre><code>jdbc:presto://10.10.25.14:8585/mysql/test
</code></pre>

<p>这个url就是来连接hive catalog中的default schema</p>

<pre><code>jdbc:presto://10.10.25.14:8585/hive/default
</code></pre>

<h3 id="toc_26">Java代码</h3>

<p><strong>读取mysql下的test库下的所有表</strong></p>

<pre><code>public class PrestoJdbcDemo {

  public static void main(String[] args) throws SQLException, ClassNotFoundException {
    Class.forName(&quot;com.facebook.presto.jdbc.PrestoDriver&quot;);
    Connection connection = DriverManager
        .getConnection(&quot;jdbc:presto://10.10.25.14:8585/mysql/test&quot;, &quot;root&quot;, &quot;wankatest***&quot;);
    Statement stmt = connection.createStatement();
    ResultSet rs = stmt.executeQuery(&quot;show tables&quot;);
    while (rs.next()) {
      System.out.println(rs.getString(1));
    }
    rs.close();
    connection.close();

  }
}
</code></pre>

<p><strong>读取hive下的default库下的所有表</strong></p>

<pre><code>public class PrestoJdbcDemo {

  public static void main(String[] args) throws SQLException, ClassNotFoundException {
    Class.forName(&quot;com.facebook.presto.jdbc.PrestoDriver&quot;);
    Connection connection = DriverManager
        .getConnection(&quot;jdbc:presto://10.10.25.14:8585/hive/default&quot;,&quot;root&quot;,null);
    Statement stmt = connection.createStatement();
    ResultSet rs = stmt.executeQuery(&quot;show tables&quot;);
    while (rs.next()) {
      System.out.println(rs.getString(1));
    }
    rs.close();
    connection.close();

  }
}
</code></pre>

<h1 id="toc_27">不同数据源之间的join</h1>

<p>presto的一个特性就是其支持在不同的数据源之间进行join</p>

<p>当连接presto的客户端的时候,也可以不指定连接器</p>

<p>不同的数据源就用catalog名称指定,然后加上库名表明即可.</p>

<pre><code>./presto --server U007:8585
 show tables from mysql.dsp_test;
</code></pre>

<h1 id="toc_28">presto提供的函数和运算符</h1>

<p>参考文档:<a href="http://prestodb-china.com/docs/current/functions.html">http://prestodb-china.com/docs/current/functions.html</a></p>

<h1 id="toc_29">看日志</h1>

<h2 id="toc_30">日志路径</h2>

<p><strong>node.properties</strong>中配置了node.data-dir=/home/druid/data/presto</p>

<pre><code>[druid@U007 presto]$ tree
.
├── etc -&gt; /home/druid/presto-server-0.161/etc
├── plugin -&gt; /home/druid/presto-server-0.161/plugin
└── var
    ├── log
    │   ├── http-request.log
    │   ├── launcher.log
    │   └── server.log
    └── run
        └── launcher.pid

5 directories, 4 files
</code></pre>

<p>出现错误后,我们主要关注var/log目录下的日志.</p>

<p>当服务有问题的时候,看server.log找到报错原因,从而解决问题.</p>

<h1 id="toc_31">常见错误</h1>

<h2 id="toc_32">连接不上连接器</h2>

<p>类似这样的错误</p>

<pre><code>No factory for connector mysql
No factory for connector hive
</code></pre>

<p>如果服务报相关这样的错误,那么就需要关注各个连接器的配置文件是否写对了.各个连接器的配置文件是否在每个presto服务器上都部署了.</p>

<h1 id="toc_33">Presto的实现原理</h1>

<p><img src="http://oz6wyfxp0.bkt.clouddn.com/1512097348.png?imageMogr2/thumbnail/!70p" alt="Presto架构"/></p>

<ul>
<li>Presto的架构</li>
<li>Presto执行原理</li>
</ul>

<p>简单来说:</p>

<ol>
<li>cli客户端把查询需求发送给Coordinator节点.Coordinator节点负责解析sql语句,生成执行计划,分发执行任务给worer节点执行.所以worker节点负责实际执行查询任务.</li>
<li>worker节点启动后向discovery server服务注册,因此coordinator就可以从discovery server获得可以正常工作的worker节点.</li>
</ol>

<p>深入来说:</p>

<ol>
<li>参考网上相关blog.</li>
<li>买书.</li>
<li>看源码!</li>
</ol>

<h1 id="toc_34">参考文档</h1>

<ul>
<li><p><a href="https://prestodb.io">presto官网</a></p></li>
<li><p><a href="https://github.com/prestodb/presto">presto-Github</a></p></li>
<li><p><a href="http://prestodb-china.com">presto-京东维护-中文文档</a></p></li>
<li><p><a href="https://tech.meituan.com/presto.html">Presto实现原理和美团的使用实践</a></p></li>
<li><p><a href="http://getindata.com/tutorial-using-presto-to-combine-data-from-hive-and-mysql-in-one-sql-like-query/">Presto-Hive-Mysql-InteractiveQuery</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[二八定律与长尾理论]]></title>
    <link href="http://dmlcoding.com/15198769093559.html"/>
    <updated>2018-03-01T12:01:49+08:00</updated>
    <id>http://dmlcoding.com/15198769093559.html</id>
    <content type="html"><![CDATA[
<p>刚入广告行业的时候,有时候会听同事们说长尾啥的.一直不太明白长尾是啥意思.最近看一本书,书里面提到了一个二八定律.<br/>
本着好奇的态度,搜索了一下二八定律<a href="%5B%5D()"></a>,没想到顺带都提到了长尾理论.接着这个机会,好好理解一下二八定律与长尾理论.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">什么是二八定律</h1>

<p>我把wiki上的解释摘过来看看.</p>

<p>帕雷托法则（英语：Pareto principle），也称为二八定律或80/20法则，此法则指在众多现象中，80%的结果取决于20%的原因，<br/>
而这一法则在很多方面被广泛的应用。如80%的劳动成果取决于20%的前期努力等等。<br/>
这个法则最初是意大利经济学家维弗雷多·帕雷托在1906年对意大利20%的人口拥有80%的财产的观察而得出的，<br/>
后来管理学思想家约瑟夫·朱兰和其他人把它概括为帕雷托法则。</p>

<p>所以简单说也就是,最重要的</p>

<h1 id="toc_1">什么是长尾理论</h1>

<p>未完待续</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ELK搭建使用]]></title>
    <link href="http://dmlcoding.com/15198769472354.html"/>
    <updated>2018-03-01T12:02:27+08:00</updated>
    <id>http://dmlcoding.com/15198769472354.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>elk搭建记录,学习资料.</p>
</blockquote>

<h1 id="toc_0">ELK学习资料</h1>

<ul>
<li><a href="https://www.gitbook.com/book/chenryn/elk-stack-guide-cn/details">ELKstack 中文指南</a></li>
<li><a href="https://www.gitbook.com/book/looly/elasticsearch-the-definitive-guide-cn/details">Elasticsearch权威指南（中文版）</a></li>
<li>.....</li>
</ul>

<span id="more"></span><!-- more -->

<h1 id="toc_1">ELK下载</h1>

<p>历史版本下载地址 : <a href="https://www.elastic.co/downloads/past-releases">https://www.elastic.co/downloads/past-releases</a><br/>
+ elasticsearch : 2.4.0<br/>
+ logstash : 2.4.0<br/>
+ kibana : 4.6.2</p>

<h1 id="toc_2">安装准备</h1>

<pre><code>[root@U006 opt]# cd /opt
[root@U006 opt]# mkdir elk
[root@U006 opt]# chown -R hadoop:hadoop elk
[root@U006 opt]# su hadoop
[hadoop@U006 opt]$ cd /opt/elk/
[hadoop@U006 elk]$ mkdir jars
[hadoop@U006 jars]$ pwd
/opt/elk/jars

# 上传jar包

[hadoop@U006 jars]$ ll
total 116996
-rw-r--r-- 1 hadoop hadoop 27364449 Sep 18 09:36 elasticsearch-2.4.0.tar.gz
-rw-r--r-- 1 hadoop hadoop 34125464 Sep 18 09:37 kibana-4.6.2-linux-x86_64.tar.gz
-rw-r--r-- 1 hadoop hadoop 58310656 Sep 18 09:41 logstash-2.4.0.tar.gz

</code></pre>

<pre><code>[hadoop@U006 elk]$ ll
total 16
drwxrwxr-x  6 hadoop hadoop 4096 Sep 18 09:45 elasticsearch-2.4.0
drwxrwxr-x  2 hadoop hadoop 4096 Sep 18 09:40 jars
drwxrwxr-x 11 hadoop hadoop 4096 Oct 21  2016 kibana-4.6.2-linux-x86_64
drwxrwxr-x  5 hadoop hadoop 4096 Sep 18 09:47 logstash-2.4.0
</code></pre>

<h1 id="toc_3">安装elasticsearch</h1>

<blockquote>
<p>测试环境搭建,单节点为例</p>
</blockquote>

<pre><code># 解压elasticsearch
tar -zxvf elasticsearch-2.4.0.tar.gz -C /opt/elk/
</code></pre>

<h2 id="toc_4">修改配置文件config/elasticsearch.yml</h2>

<pre><code>[hadoop@U006 config]$ pwd
/opt/elk/elasticsearch-2.4.0/config

[hadoop@U006 config]$ ll
total 8
-rw-rw-r-- 1 hadoop hadoop 3192 Aug 24  2016 elasticsearch.yml
-rw-rw-r-- 1 hadoop hadoop 2571 Aug 24  2016 logging.yml

[hadoop@U006 elasticsearch-2.4.0]$ vim config/elasticsearch.yml

# 打开这三个配置的注释
 cluster.name: test_elasticsearch # es集群名字
 node.name: node1 # 该节点在es中的名字
 network.host: 0.0.0.0 # 任意节点可以访问
</code></pre>

<h2 id="toc_5">启动es</h2>

<pre><code>./bin/elasticsearch
./bin/elasticsearch -d #后台启动
</code></pre>

<pre><code>[hadoop@U006 elasticsearch-2.4.0]$ ./bin/elasticsearch
[2017-09-18 10:08:50,300][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in
[2017-09-18 10:08:51,697][INFO ][node                     ] [node1] version[2.4.0], pid[3098], build[ce9f0c7/2016-08-29T09:14:17Z]
[2017-09-18 10:08:51,697][INFO ][node                     ] [node1] initializing ...
[2017-09-18 10:08:52,332][INFO ][plugins                  ] [node1] modules [lang-groovy, reindex, lang-expression], plugins [], sites []
[2017-09-18 10:08:52,363][INFO ][env                      ] [node1] using [1] data paths, mounts [[/home (/dev/mapper/VolGroup-lv_home)]], net usable_space [195.3gb], net total_space [857.4gb], spins? [possibly], types [ext4]
[2017-09-18 10:08:52,363][INFO ][env                      ] [node1] heap size [989.8mb], compressed ordinary object pointers [true]
[2017-09-18 10:08:54,472][INFO ][node                     ] [node1] initialized
[2017-09-18 10:08:54,473][INFO ][node                     ] [node1] starting ...
[2017-09-18 10:08:54,675][INFO ][transport                ] [node1] publish_address {10.10.25.13:9300}, bound_addresses {[::]:9300}
[2017-09-18 10:08:54,685][INFO ][discovery                ] [node1] test_elasticsearch/oAbAZR_tTaGAJU-5yd3BqQ
[2017-09-18 10:08:57,757][INFO ][cluster.service          ] [node1] new_master {node1}{oAbAZR_tTaGAJU-5yd3BqQ}{10.10.25.13}{10.10.25.13:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2017-09-18 10:08:57,798][INFO ][http                     ] [node1] publish_address {10.10.25.13:9200}, bound_addresses {[::]:9200}
[2017-09-18 10:08:57,799][INFO ][node                     ] [node1] started
[2017-09-18 10:08:57,811][INFO ][gateway                  ] [node1] recovered [0] indices into cluster_state
</code></pre>

<p>打开<a href="http://10.10.25.13:9200/">http://10.10.25.13:9200/</a> 将会看到以下内容.返回数据中包含配置的cluster.name和node.name,以及es的版本等信息.<br/>
<img src="media/15198769472354/es-startresult.png" alt="es-startresult"/></p>

<h2 id="toc_6">安装插件</h2>

<h3 id="toc_7">elasticsearch-head 插件安装</h3>

<h4 id="toc_8">下载安装</h4>

<pre><code># 进入bin目录下
[hadoop@U006 bin]$ ./plugin install mobz/elasticsearch-head
-&gt; Installing mobz/elasticsearch-head...
Trying https://github.com/mobz/elasticsearch-head/archive/master.zip ...
Downloading ...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................DONE
Verifying https://github.com/mobz/elasticsearch-head/archive/master.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
Installed head into /opt/elk/elasticsearch-2.4.0/plugins/head

</code></pre>

<h4 id="toc_9">使用说明</h4>

<blockquote>
<p>head插件是一个用浏览器跟ES集群交互的插件，可以查看集群状态、集群的doc内容、执行搜索和普通的Rest请求等。</p>
</blockquote>

<p>在浏览器中直接访问接口 <a href="http://10.10.25.13:9200/_plugin/head/">http://10.10.25.13:9200/_plugin/head/</a> ,可以看到es集群状态<br/>
<img src="media/15198769472354/es-plugin-head.png" alt="es-plugin-head"/></p>

<h3 id="toc_10">安装Marvel插件</h3>

<h4 id="toc_11">下载安装</h4>

<pre><code># ./bin/plugin install license
# ./bin/plugin install marvel-agent

[hadoop@U006 elasticsearch-2.4.0]$ ./bin/plugin install license
-&gt; Installing license...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.4.0/license-2.4.0.zip ...
Downloading .......DONE
Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.4.0/license-2.4.0.zip checksums if available ...
Downloading .DONE
Installed license into /opt/elk/elasticsearch-2.4.0/plugins/license
[hadoop@U006 elasticsearch-2.4.0]$ ./bin/plugin install marvel-agent
-&gt; Installing marvel-agent...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.4.0/marvel-agent-2.4.0.zip ...
Downloading ..........DONE
Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.4.0/marvel-agent-2.4.0.zip checksums if available ...
Downloading .DONE
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission setFactory
* javax.net.ssl.SSLPermission setHostnameVerifier
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
Installed marvel-agent into /opt/elk/elasticsearch-2.4.0/plugins/marvel-agent


# 安装的插件都会出现在es的plugins目录下
[hadoop@U006 elasticsearch-2.4.0]$ ll
total 56
drwxrwxr-x 2 hadoop hadoop  4096 Sep 18 09:45 bin
drwxrwxr-x 3 hadoop hadoop  4096 Sep 18 10:05 config
drwxrwxr-x 3 hadoop hadoop  4096 Sep 18 09:59 data
drwxrwxr-x 2 hadoop hadoop  4096 Sep 18 09:45 lib
-rw-rw-r-- 1 hadoop hadoop 11358 Aug 24  2016 LICENSE.txt
drwxrwxr-x 2 hadoop hadoop  4096 Sep 18 09:59 logs
drwxrwxr-x 5 hadoop hadoop  4096 Aug 29  2016 modules
-rw-rw-r-- 1 hadoop hadoop   150 Aug 24  2016 NOTICE.txt
drwxrwxr-x 3 hadoop hadoop  4096 Sep 18 10:15 plugins
-rw-rw-r-- 1 hadoop hadoop  8700 Aug 24  2016 README.textile
[hadoop@U006 elasticsearch-2.4.0]$ cd plugins/
[hadoop@U006 plugins]$ ll
total 12
drwxrwxr-x 6 hadoop hadoop 4096 Sep 18 10:15 head
drwxrwxr-x 2 hadoop hadoop 4096 Sep 18 10:34 license
drwxrwxr-x 2 hadoop hadoop 4096 Sep 18 10:34 marvel-agent
</code></pre>

<h4 id="toc_12">使用说明</h4>

<blockquote>
<p>Marvel是Elasticsearch的管理和监控工具，在开发环境下免费使用。它包含了一个叫做Sense的交互式控制台，<br/>
使用户方便的通过浏览器直接与Elasticsearch进行交互。<br/>
marvel插件主要会和kibana进行配置使用,待会看kibana也需要安装marvel插件</p>
</blockquote>

<h2 id="toc_13">注意</h2>

<ul>
<li>如何之前在config/elasticsearch.yml的文件中,没有修改network.host项.那么你只能用localhost或者127.0.0.1访问es了.</li>
<li>注意配置yml结尾的配置文件都需要冒号后面加空格才行</li>
</ul>

<h1 id="toc_14">安装kibana</h1>

<h2 id="toc_15">解压安装</h2>

<pre><code>tar -zxvf kibana-4.6.2-linux-x86_64.tar.gz -C /opt/elk/

</code></pre>

<h2 id="toc_16">修改config/kibana.yml的elasticsearch.url属性即可。</h2>

<p><img src="media/15198769472354/kibana-config.png" alt="kibana-config"/></p>

<h2 id="toc_17">安装插件</h2>

<h3 id="toc_18">安装Marvel插件</h3>

<blockquote>
<p>在安装es的时候,已经给es安装了marvel插件,现在给kibana也安装上marvel插件</p>
</blockquote>

<h4 id="toc_19">下载</h4>

<pre><code>[hadoop@U006 kibana-4.6.2-linux-x86_64]$ bin/kibana plugin --install elasticsearch/marvel/latest
Installing marvel
Attempting to transfer from https://download.elastic.co/elasticsearch/marvel/marvel-latest.tar.gz
.....
Transfer complete
Extracting plugin archive
Extraction complete
Optimizing and caching browser bundles...
Plugin installation complete

</code></pre>

<h4 id="toc_20">启动验证</h4>

<pre><code>bin/elasticsearch
bin/kibana

</code></pre>

<p>查看<a href="http://10.10.25.13:5601/app/marvel">http://10.10.25.13:5601/app/marvel</a> 页面：<br/>
<img src="media/15198769472354/kibana-marvel1.png" alt="kibana-marve"/><br/>
<img src="media/15198769472354/kibana-marvel2.png" alt="kibana-marve"/></p>

<h3 id="toc_21">安装sense插件</h3>

<blockquote>
<p>Sense是flask写的elasticsearch查询工具。<br/>
支持es查询语言自动提示，es结构自动提示，支持两种主题，支持查询历史记录，支持快捷键。</p>
</blockquote>

<h4 id="toc_22">下载</h4>

<pre><code>[hadoop@U006 kibana-4.6.2-linux-x86_64]$ ./bin/kibana plugin --install elastic/sense
Installing sense
Attempting to transfer from https://download.elastic.co/elastic/sense/sense-latest.tar.gz
.....
Transfer complete
Extracting plugin archive
Extraction complete
Optimizing and caching browser bundles...
Plugin installation complete

</code></pre>

<h4 id="toc_23">使用说明</h4>

<p>启动es和kibana<br/>
查看<a href="http://10.10.25.13:5601/app/sense">http://10.10.25.13:5601/app/sense</a> 页面：<br/>
<img src="media/15198769472354/kibana-sense.png" alt="kibana-sense"/></p>

<h1 id="toc_24">安装logstash</h1>

<h2 id="toc_25">解压安装即可</h2>

<pre><code>tar -zxvf logstash-2.4.0.tar.gz -C /opt/elk/

</code></pre>

<h2 id="toc_26">配置logstash的配置文件</h2>

<p>此文件input为从log4j接收日志.output为输出到es集群,进行搜索.</p>

<p>字段具体意思可以看官网 <a href="https://www.elastic.co/guide/en/logstash/current/plugins-inputs-log4j.html">https://www.elastic.co/guide/en/logstash/current/plugins-inputs-log4j.html</a></p>

<pre><code>[hadoop@U006 logstash-2.4.0]$ vim logstash_log4j_to_es.conf
input {
        log4j {
                 mode =&gt; &quot;server&quot;
                 host =&gt; &quot;10.10.25.13&quot;
                 port =&gt; 4567
                 type =&gt; &quot;log4j&quot;
         }
}
output{
        elasticsearch{
                action =&gt; &quot;index&quot;
                hosts =&gt; &quot;10.10.25.13:9200&quot;
                index =&gt; &quot;test_log&quot;
        }
}

</code></pre>

<p><img src="media/15198769472354/logstash-file1.png" alt="logstash-file1"/></p>

<h2 id="toc_27">启动logstash</h2>

<pre><code>bin/logstash agent -f logstash_log4j_to_es.conf
# 或者
bin/logstash -f logstash_log4j_to_es.conf

[hadoop@U006 logstash-2.4.0]$ bin/logstash -f logstash_log4j_to_es.conf
Settings: Default pipeline workers: 24
log4j:WARN No appenders could be found for logger (org.apache.http.client.protocol.RequestAuthCache).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Pipeline main started
</code></pre>

<h1 id="toc_28">ELK框架的综合实际使用</h1>

<blockquote>
<p>在工作中,我们会使用ELK对业务日志进行收集分析.<br/>
也就是把项目中的log4j日志用logstash进行收集,然后输出到es中进行索引搜索.最后使用Kibana进行可视化的搜索展示.</p>
</blockquote>

<h2 id="toc_29">启动elk</h2>

<pre><code>bin/elasticsearch
bin/kibana
bin/logstash -f logstash_log4j_to_es.conf
</code></pre>

<p>打开：<a href="http://10.10.25.13:5601">http://10.10.25.13:5601</a><br/>
<img src="media/15198769472354/kibana-index.png" alt="kibana-index"/></p>

<p>如图所以,这里有个WARN警告:没有默认的索引模式,需要创建一个才能继续.<br/>
那么我们就创建一个索引.</p>

<h2 id="toc_30">创建索引</h2>

<p>Kibana界面日志检索只有当第一条日志通过Logstash进入ElasticSearch后，才能配置Kibana索引。</p>

<p>1、在“Index name or pattern”项下，填入一个elasticsearch的索引名，也即是Logstash配置文件中output项下的index对应的名称；在你这里应该是将“logstash-* ” 改成“test_log”<br/>
2、在“Time-field name”，选用默认的配置：“@timestamp”<br/>
3、点击“create”即可</p>

<p><img src="media/15198769472354/kibana-index2.png" alt="kibana-index2"/></p>

<h2 id="toc_31">log4j日志接入</h2>

<h3 id="toc_32">编写log4j的测试代码</h3>

<pre><code>public class TestFunc {

  Logger logger = LoggerFactory.getLogger(TestFunc.class);

  @Test
  public void testlog4j() throws Exception {
    while (true) {
      long s_time = System.currentTimeMillis();
      logger.info(&quot;当前时间戳: &quot;+s_time+&quot; i am info info hadoop&quot;);
      logger.warn(&quot;当前时间戳: &quot;+s_time+&quot; i am info warn spark hushiwei nice&quot;);
      logger.error(&quot;当前时间戳: &quot;+s_time+&quot; i am info error elk&quot;);
      Thread.sleep(1000L);

    }
  }
  }
</code></pre>

<h3 id="toc_33">log4j的配置</h3>

<p>log4j.properties<br/>
remotehost填写logstash的服务器地址.也就是那个input项里面的地址.<br/>
<code><br/>
log4j.rootLogger=INFO,socket<br/>
log4j.appender.socket=org.apache.log4j.net.SocketAppender<br/>
log4j.appender.socket.RemoteHost=10.10.25.13<br/>
log4j.appender.socket.Port=4567<br/>
log4j.appender.socket.LocationInfo=true<br/>
</code></p>

<h2 id="toc_34">执行代码,输出log4j日志,观察kibana页面变化</h2>

<p>索引日志里面的信息</p>

<p><img src="media/15198769472354/kibana-index3.png" alt="kibana-index3"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python读取文件编码及内容]]></title>
    <link href="http://dmlcoding.com/15198744609518.html"/>
    <updated>2018-03-01T11:21:00+08:00</updated>
    <id>http://dmlcoding.com/15198744609518.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>当你不知道文件的具体编码方式的时候,如何正确的读取文件内容呢?</p>
</blockquote>

<h1 id="toc_0">报错日志</h1>

<p>当我们不知道文件编码方式的时候,贸然的读取文件,有时候就会出现这些问题</p>

<span id="more"></span><!-- more -->

<pre><code>UnicodeDecodeError: &#39;gbk&#39; codec can&#39;t decode byte
......
</code></pre>

<h1 id="toc_1">解决办法</h1>

<p>所以就是编码方式不对,那么需要先能识别文件的编码文件,然后根据此编码方式进行对文件编码，最后返回文件内容。<br/>
可以借助一个第三方库<code>chardet</code><br/>
```</p>

<h1 id="toc_2">安装chardet</h1>

<p>pip install chardet<br/>
```</p>

<h1 id="toc_3">正确实例</h1>

<pre><code>with open(&quot;your_file&quot;, &#39;rb&#39;) as fp:
    file_data = fp.read()
    result = chardet.detect(file_data)
    file_content = file_data.decode(encoding=result[&#39;encoding&#39;])

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[spark开发中遇到的问题]]></title>
    <link href="http://dmlcoding.com/15198733795152.html"/>
    <updated>2018-03-01T11:02:59+08:00</updated>
    <id>http://dmlcoding.com/15198733795152.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">spark连接mysql</h1>

<h2 id="toc_1">问题描述</h2>

<pre><code>总是报no suitable driver以及   jdbc.mysql.driver类似这样的错误
</code></pre>

<span id="more"></span><!-- more -->

<h2 id="toc_2">解决办法1</h2>

<pre><code>1.提交任务的时候带上这个，手动指定mysql jar包的位置
                    SPARK_CLASSPATH=/usr/local/spark-1.4.1-bin-hadoop2.6/lib/mysql-connector-java-5.1.38.jar ./bin/spark-submit --class sparkDemo /root/data/demon-parent-1.0-SNAPSHOT-jar-with-dependencies.jar hdfs://192.168.119.100:9000/examples/custom.txt
</code></pre>

<h2 id="toc_3">解决办法2</h2>

<pre><code>修改了这个配置SPARK_HOME/conf/spark-env.sh文件，在里面加上了这个参数，就OK了

export SPARK_CLASSPATH=$SPATH_CLASSPATH:/usr/hdp/2.4.0.0-169/spark/lib/mysql-connector-java-5.1.38.jar

</code></pre>

<h1 id="toc_4">在spark中使用hive抛出错误</h1>

<h1 id="toc_5">报错日志</h1>

<pre><code>17/08/09 12:11:51 WARN DataNucleus.Persistence: Error creating validator of type org.datanucleus.properties.CorePropertyValidator
ClassLoaderResolver for class &quot;&quot; gave error on creation : {1}
org.datanucleus.exceptions.NucleusUserException: ClassLoaderResolver for class &quot;&quot; gave error on creation : {1}
    at org.datanucleus.NucleusContext.getClassLoaderResolver(NucleusContext.java:1087)
    at org.datanucleus.PersistenceConfiguration.validatePropertyValue(PersistenceConfiguration.java:797)
    at org.datanucleus.PersistenceConfiguration.setProperty(PersistenceConfiguration.java:714)
    at org.datanucleus.PersistenceConfiguration.setPersistenceProperties(PersistenceConfiguration.java:693)
    at org.datanucleus.NucleusContext.&lt;init&gt;(NucleusContext.java:273)
    at org.datanucleus.NucleusContext.&lt;init&gt;(NucleusContext.java:247)
    at org.datanucleus.NucleusContext.&lt;init&gt;(NucleusContext.java:225)
    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.&lt;init&gt;(JDOPersistenceManagerFactory.java:416)
    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:301)
    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
</code></pre>

<h1 id="toc_6">问题分析</h1>

<p>看日志应该是缺少了hive的一些包,在网上搜了一下,是下面几个包<br/>
```<br/>
[hadoop@U007 lib]\( pwd<br/>
/opt/spark-1.6.0/lib<br/>
[hadoop@U007 lib]\) ll<br/>
total 305220<br/>
-rw-r--r-- 1 hadoop hadoop    339666 Apr 15  2016 datanucleus-api-jdo-3.2.6.jar<br/>
-rw-r--r-- 1 hadoop hadoop   1890075 Apr 15  2016 datanucleus-core-3.2.10.jar<br/>
-rw-r--r-- 1 hadoop hadoop   1809447 Apr 15  2016 datanucleus-rdbms-3.2.9.jar<br/>
...</p>

<pre><code>
所以在提交spark任务的时候,把这几个包加入到classpath中即可

# 解决办法
在提交spark的脚本中加上这几个jar包和hive-site.xml文件
如下

</code></pre>

<p>nohup spark-submit \<br/>
 --master yarn \<br/>
 --deploy-mode cluster \<br/>
 --class ${className} \<br/>
 --driver-memory 4g \<br/>
 --executor-memory 2g \<br/>
 --executor-cores 4 \<br/>
 --num-executors 4 \<br/>
 --jars ./lib/datanucleus-api-jdo-3.2.6.jar,./lib/datanucleus-core-3.2.10.jar,./lib/datanucleus-rdbms<br/>
-3.2.9.jar  \<br/>
 --files ./lib/hive-site.xml \<br/>
 ./app-jar-with-dependencies.jar \</p>

<pre><code>
加上--jars 和 --files即可

# 在spark中将数据插入hive动态分区
## 问题描述
当我用standalone以及yarn-client模式进行提交任务的时候,不会报错.但是当我改成yarn-cluster模式进行提交任务,有时候就会报下面的错
## 报错日志
</code></pre>

<p>17/08/09 10:08:01 ERROR scheduler.JobScheduler: Error running job streaming job 1502188440000 ms.0<br/>
java.lang.NoSuchMethodException: org.apache.hadoop.hive.ql.metadata.Hive.loadDynamicPartitions(org.apache.hadoop.fs.Path, java.lang.String, java.util.Map, boolean, int, boolean, boolean)<br/>
    at java.lang.Class.getMethod(Class.java:1670)<br/>
    at org.apache.spark.sql.hive.client.Shim.findMethod(HiveShim.scala:114)<br/>
    at org.apache.spark.sql.hive.client.Shim_v0_12.loadDynamicPartitionsMethod\(lzycompute(HiveShim.scala:168)<br/>
    at org.apache.spark.sql.hive.client.Shim_v0_12.loadDynamicPartitionsMethod(HiveShim.scala:167)<br/>
    at org.apache.spark.sql.hive.client.Shim_v0_12.loadDynamicPartitions(HiveShim.scala:261)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper\)\(anonfun\)loadDynamicPartitions\(1.apply\)mcV\(sp(ClientWrapper.scala:560)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper\)\(anonfun\)loadDynamicPartitions\(1.apply(ClientWrapper.scala:560)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper\)\(anonfun\)loadDynamicPartitions\(1.apply(ClientWrapper.scala:560)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper\)\(anonfun\)withHiveState\(1.apply(ClientWrapper.scala:279)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper.liftedTree1\)1(ClientWrapper.scala:226)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:225)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:268)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper.loadDynamicPartitions(ClientWrapper.scala:559)<br/>
    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult\(lzycompute(InsertIntoHiveTable.scala:225)<br/>
    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult(InsertIntoHiveTable.scala:127)<br/>
    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.doExecute(InsertIntoHiveTable.scala:276)<br/>
    at org.apache.spark.sql.execution.SparkPlan\)\(anonfun\)execute\(5.apply(SparkPlan.scala:132)<br/>
    at org.apache.spark.sql.execution.SparkPlan\)\(anonfun\)execute\(5.apply(SparkPlan.scala:130)<br/>
    at org.apache.spark.rdd.RDDOperationScope\).withScope(RDDOperationScope.scala:150)<br/>
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)<br/>
    at org.apache.spark.sql.execution.QueryExecution.toRdd\(lzycompute(QueryExecution.scala:55)<br/>
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)<br/>
    at org.apache.spark.sql.DataFrame.&lt;init&gt;(DataFrame.scala:145)<br/>
    at org.apache.spark.sql.DataFrame.&lt;init&gt;(DataFrame.scala:130)<br/>
    at org.apache.spark.sql.DataFrame\).apply(DataFrame.scala:52)<br/>
    at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:817)<br/>
```</p>

<h2 id="toc_7">分析</h2>

<p>用client模式的时候,是在13上运行的.没有问题<br/>
用cluster模式的时候,有时候报错,有时候没有报错<br/>
那不禁让我猜想,为啥cluster模式时而报错时而不报错呢?</p>

<p>然后我用client模式,在14上提交,不出我所料,基本上每个job都抛出了那个错误.<br/>
所以定位到问题就是,除了13这个节点外,别的节点缺少了什么包,导致抛出了错误.<br/>
因为抛出来的错误是java.lang.NoSuchMethodException:,所以肯定是缺少了什么包.<br/>
之前cluster模式时而报错时而不报错的原因肯定是,当不报错的时候,正好driver端是在13上</p>

<p>现在的问题就是找出别的机器缺少什么包了.</p>

<p>然后我在spark的环境变量里面发现了这个参数<br/>
<code><br/>
spark.sql.hive.metastore.jars : /usr/lib/hive/lib/*:/opt/spark-1.6.0/lib/spark-assembly-1.6.0-hadoop2.4.0.jar<br/>
</code></p>

<p>我去,13上有这个/usr/lib/hive/lib/* 路径<br/>
14和15上都没有,,,<br/>
问题找到了</p>

<h2 id="toc_8">解决办法1</h2>

<p>把13上这个路径/usr/lib/hive/lib/* 拷贝到14和15上,各自都有一份.这样无论driver端在哪里,都能找到相应的jar包.<br/>
就这样愉快的解决了.<br/>
所以遇到问题,慢慢分析,不要像无头苍蝇一样.<br/>
在网上搜的解决办法,都无法解决这个问题.所以有时候,具体问题具体分析,要慢慢的分析到出错原因.找到了原因,bug就能迎刃而解.</p>

<h2 id="toc_9">解决办法2</h2>

<p>在spark的配置文件中把 <code>spark.sql.hive.metastore.jars</code> 给删了.因为你总不能在每个节点上去拷贝hive的一些依赖吧,如果以后hive升级了,还得替换hive的jar包,太麻烦.所以改成下面的解决办法更好.</p>

<p>在pom文件中加上hive的依赖</p>

<pre><code>&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.10 --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-mllib_2.10 --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-mllib_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming-kafka_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;mysql&lt;/groupId&gt;
    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;version&gt;5.1.32&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
    &lt;version&gt;0.13.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
    &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;
    &lt;version&gt;0.13.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<h1 id="toc_10">sparkstreaming读取kafka数据</h1>

<h2 id="toc_11">问题描述Couldn&#39;t find leaders for Set</h2>

<p>SparkStreaming程序从Kafka读数据的程序运行期间报了描述中的异常.<br/>
通过监控分析发现,是由于有一个Broker挂掉了。可是对应Topic的replica设置的2，就算挂掉一个，应该有replica顶上啊。<br/>
后来发现，这是由于存在Partition的Replica没有跟Leader保持同步更新，也就是通常所说的“没追上”。 查看某个Topic是否存在没追上的情况：</p>

<p>查看某个Topic是否存在没追上的情况：<br/>
<code><br/>
kafka-topics.sh --describe --zookeeper XXX --topic XXX<br/>
</code></p>

<h2 id="toc_12">报错日志</h2>

<pre><code>17/10/13 09:41:13 ERROR DirectKafkaInputDStream: ArrayBuffer(java.nio.channels.ClosedChannelException, org.apache.spark.SparkException: Couldn&#39;t find leader offsets for Set([dsp_request_event,2]))
17/10/13 09:41:13 ERROR StreamingContext: Error starting the context, marking it as stopped
org.apache.spark.SparkException: ArrayBuffer(java.nio.channels.ClosedChannelException, org.apache.spark.SparkException: Couldn&#39;t find leader offsets for Set([dsp_request_event,2]))
    at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.latestLeaderOffsets(DirectKafkaInputDStream.scala:123)
    at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:145)
    at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:352)
    at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:352)
</code></pre>

<h2 id="toc_13">解决办法</h2>

<p>观察其中的Replicas和Isr是否一致，如果出现Isr少于Replicas，则对应Partition存在没追上的情况<br/>
解决方法：<br/>
增大num.replica.fetchers的值，此参数是Replicas从Leader同步数据的线程数，默认为1，增大此参数即增大了同步IO。经过测试，增大此值后，不再有追不上的情况<br/>
确定问题已解决的方法：<br/>
启动出现问题的SparkStreaming程序，在程序正常计算的状态下，kill掉任意一个Broker后，再观察运行情况。在增大同步线程数之前，kill后SparkStreaming会报同样的异常，而增大后程序依然正常运行，问题解决。</p>

<p>参考:<a href="http://blog.csdn.net/yanshu2012/article/details/53995159">http://blog.csdn.net/yanshu2012/article/details/53995159</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[诗经 <<伐檀>>]]></title>
    <link href="http://dmlcoding.com/15198767503226.html"/>
    <updated>2018-03-01T11:59:10+08:00</updated>
    <id>http://dmlcoding.com/15198767503226.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15198767503226/fatan.png" alt="fatan"/></p>

<span id="more"></span><!-- more -->

<blockquote>
<p>抄一首诗,换个心情...</p>
</blockquote>

<h1 id="toc_0">伐檀</h1>

<p>坎坎伐檀兮，置之河之干兮，河水清且涟猗。<br/>
不稼不穑，胡取禾三百廛[1]兮？<br/>
不狩不猎，胡瞻尔庭有县[2]貆兮？<br/>
彼君子兮，不素餐兮！</p>

<p>坎坎伐辐兮，置之河之侧兮，河水清且直猗。<br/>
不稼不穑，胡取禾三百亿兮？<br/>
不狩不猎，胡瞻尔庭有县特[3]兮？<br/>
彼君子兮，不素食兮！</p>

<p>坎坎伐轮兮，置之河之漘[4]兮，河水清且沦猗。<br/>
不稼不穑，胡取禾三百囷[5]兮？<br/>
不狩不猎，胡瞻尔庭有县鹑兮？<br/>
彼君子兮，不素飧兮！</p>

<h1 id="toc_1">伐檀白话</h1>

<p>砍伐檀树声坎坎啊，<br/>
棵棵放倒堆河边啊，<br/>
河水清清微波转哟。<br/>
不播种来不收割，<br/>
为何三百捆禾往家搬啊？<br/>
不冬狩来不夜猎，<br/>
为何见你庭院猪獾悬啊？<br/>
那些老爷君子啊，<br/>
不会白吃闲饭啊！</p>

<p>砍下檀树做车辐啊，<br/>
放在河边堆一处啊。<br/>
河水清清直流注哟。<br/>
不播种来不收割，<br/>
为何三百捆禾要独取啊？<br/>
不冬狩来不夜猎，<br/>
为何见你庭院兽悬柱啊？<br/>
那些老爷君子啊，<br/>
不会白吃饱腹啊！</p>

<p>砍下檀树做车轮啊，<br/>
棵棵放倒河边屯啊。<br/>
河水清清起波纹啊。<br/>
不播种来不收割，<br/>
为何三百捆禾要独吞啊？<br/>
不冬狩来不夜猎，<br/>
为何见你庭院挂鹌鹑啊？<br/>
那些老爷君子啊，<br/>
可不白吃腥荤啊！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ITerm2下使用ssh访问Linux(包括堡垒机)]]></title>
    <link href="http://dmlcoding.com/15198752296399.html"/>
    <updated>2018-03-01T11:33:49+08:00</updated>
    <id>http://dmlcoding.com/15198752296399.html</id>
    <content type="html"><![CDATA[
<p>mac下没有xshell,虽然有SecurtCRT,但是真的太丑了.我还是比较喜欢用Iterm2来进行远程连接.<br/>
这样不可避免的会碰到要记录远程密码,如果每次都输入,那就太麻烦了.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">Iterm2下使用ssh访问Linux</h1>

<p>通过情况下,Iterm2访问远程Linux使用ssh命令,如下:<br/>
<code><br/>
ssh &lt;用户名&gt;@&lt;ip&gt;<br/>
</code><br/>
然后输入访问密码即可登录进去.有时候远程访问的默认端口如果不是22,那就需要额外加上<code>-p</code>参数跟上远程访问端口进行登录了.<br/>
很明显如果每次都要输入访问密码,那在开发过程中是相当的不方便的.</p>

<p>这里有两个方式实现免密登录.都是用Iterm2的Profiles功能加上脚本来实现.</p>

<h2 id="toc_1">方式1:使用spawn脚本文件</h2>

<p>将远程访问的相关内容写成一个脚本,然后在Profile里面调用即可.<br/>
<code><br/>
cd /Users/hushiwei/.ssh/<br/>
$ touch filename<br/>
</code></p>

<p>脚本内容<br/>
```</p>

<h1 id="toc_2">!/usr/bin/expect -f</h1>

<p>set user &lt;用户名&gt;<br/>
  set host <ip地址><br/>
  set password &lt;密码&gt;<br/>
  set timeout -1</p>

<p>spawn ssh \(user@\)host<br/>
  expect &quot;<em>assword:</em>&quot;<br/>
  send &quot;$password\r&quot;<br/>
  interact<br/>
  expect eof<br/>
```</p>

<p>如何调用呢?<br/>
在command中使用命令.command在哪看下面的图你就知道了.<br/>
<code><br/>
expect &lt;保存的脚本完整路径&gt;<br/>
</code></p>

<h2 id="toc_3">方式2:使用sshpass(推荐方式)</h2>

<p>brew安装sshpass<br/>
<code><br/>
brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb<br/>
</code></p>

<p>然后把密码写入到一个文件中<br/>
<code><br/>
hushiwei@localhost  ~/sshpass  pwd<br/>
/Users/hushiwei/sshpass<br/>
hushiwei@localhost  ~/sshpass  more pass<br/>
passwd123<br/>
</code><br/>
参考图中进行配置<br/>
<img src="media/15198752296399/sshpass.png" alt="sshpass"/></p>

<p>command写上命令<br/>
<code><br/>
/usr/local/bin/sshpass -f /Users/hushiwei/sshpass/pass ssh -p22 用户名@密码<br/>
</code><br/>
然后在iterm2的菜单栏选择Profiles,然后点击刚刚的配置,即可免密自动登录到服务器上</p>

<ul>
<li>注意:首先用命令行登录一次</li>
</ul>

<h1 id="toc_4">iterm2登录堡垒机</h1>

<p>通过SSH和密钥文件(.pem格式)登录服务器［可能是堡垒机］</p>

<h2 id="toc_5">首先修改下密钥文件权限</h2>

<pre><code>sudo chmod 600 /Users/hushiwei/sshpass/Jumpserver/hushiwei.pem
</code></pre>

<h2 id="toc_6">其次，终端可直接命令连接</h2>

<pre><code>ssh -i /Users/hushiwei/sshpass/Jumpserver/hushiwei.pem hushiwei@xxx.xxx.xxx.xxx
</code></pre>

<p>注：首次连接时，会弹出密钥文件密码输入框，可以输入并保存！</p>

<p>除了直接命令连接外，也可参考上面Profiles功能，配置好，直接在Profile里调用！简单脚本如下：</p>

<h2 id="toc_7">配置Profile脚本自动登录堡垒机</h2>

<p>脚本文件 vim jumpserver<br/>
```<br/>
hushiwei@localhost  ~/sshpass/Jumpserver  more jumpserver</p>

<h1 id="toc_8">!/usr/bin/expect -f</h1>

<p>set user hushiwei<br/>
 set host xxx.xxx.xxx.xxx<br/>
 set empath /Users/hushiwei/sshpass/Jumpserver/hushiwei.pem<br/>
 set timeout -1</p>

<p>spawn ssh -i \(empath \)user@$host<br/>
 interact<br/>
 expect eof<br/>
```</p>

<p>命令行执行<br/>
```<br/>
hushiwei@localhost  ~/sshpass/Jumpserver  expect /Users/hushiwei/sshpass/Jumpserver/jumpserver<br/>
spawn ssh -i /Users/hushiwei/sshpass/Jumpserver/hushiwei.pem <a href="mailto:hushiwei@xxx.xxx.xxx.xxx">hushiwei@xxx.xxx.xxx.xxx</a><br/>
Last login: Fri Aug 18 11:06:16 2017 from xxx.xxx.xxx.xxx</p>

<h3 id="toc_9">欢迎使用Jumpserver开源跳板机系统</h3>

<pre><code>   1) 输入 ID 直接登录 或 输入部分 IP,主机名,备注 进行搜索登录(如果唯一).
   2) 输入 / + IP, 主机名 or 备注 搜索. 如: /ip
   3) 输入 P/p 显示您有权限的主机.
   4) 输入 G/g 显示您有权限的主机组.
   5) 输入 G/g + 组ID 显示该组下主机. 如: g1
   6) 输入 E/e 批量执行命令.
   7) 输入 U/u 批量上传文件.
   8) 输入 D/d 批量下载文件.
   9) 输入 H/h 帮助.
   0) 输入 Q/q 退出.
</code></pre>

<p>Opt or ID&gt;:<br/>
<code><br/>
参考上面的Profile功能,配置好,直接在Profile里调用即可<br/>
</code></p>

<h1 id="toc_10">在Command里面写入以下即可</h1>

<p>expect /Users/hushiwei/sshpass/Jumpserver/jumpserver<br/>
```</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[监控SparkStreaming程序脚本]]></title>
    <link href="http://dmlcoding.com/15198749505894.html"/>
    <updated>2018-03-01T11:29:10+08:00</updated>
    <id>http://dmlcoding.com/15198749505894.html</id>
    <content type="html"><![CDATA[
<p>虽然Spark on yarn非常的稳定,一般情况下是不会出问题的.但是我们的SparkStreaming程序是一直运行着出实时报表的.<br/>
我们必须得对SparkStreaming程序进行监控,在程序退出后,能够及时的重启.<br/>
基于此需求,我想到了通过调用yarn的rest接口来获取提交到yarn上的任务</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">思路</h1>

<p>调用yarn提供的rest接口来获取所有正在运行的任务<br/>
<code><br/>
curl --compressed -H &quot;Accept: application/json&quot; -X GET &quot;http://master:8088/ws/v1/cluster/apps?states=RUNNING&quot;<br/>
</code><br/>
如果对别的接口有兴趣,可以看看官网.<br/>
+ <a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html</a><br/>
+ <a href="http://www.winseliu.com/blog/2014/12/07/hadoop-mr-rest-api/">http://www.winseliu.com/blog/2014/12/07/hadoop-mr-rest-api/</a></p>

<h1 id="toc_1">脚本</h1>

<p>脚本也很简单,简单看看就明白了</p>

<pre><code># -*- coding: utf-8 -*-
&#39;&#39;&#39;
    Created by hushiwei on 2018/1/5.
    监控SparkStreaming程序
    一旦挂了,执行重启,同时发送邮件和微信报警
&#39;&#39;&#39;

import os
import subprocess
import json
import logging
import time
import urllib2
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.header import Header

wechats = &quot;HuShiwei&quot;
sendEmails = [&#39;hsw_v5@163.com&#39;, &#39;xxxx@gm825.com&#39;]

urlRun = &#39;curl --compressed -H &quot;Accept: application/json&quot; -X GET &quot;http://u007:8089/ws/v1/cluster/apps?states=RUNNING&quot;&#39;
urlAcc = &#39;curl --compressed -H &quot;Accept: application/json&quot; -X GET &quot;http://u007:8089/ws/v1/cluster/apps?states=ACCEPTED&quot;&#39;

monitorPrograms = {
    &quot;com.xxxx.streaming.ADXStreaming&quot;: &quot;/home/hadoop/statistics/ad/adxstreaming/start_adx_streaming_yarn.sh&quot;,
    &quot;com.xxxx.online.streaming.DSPStreaming&quot;: &quot;/home/hadoop/statistics/ad/dsp_ad_puton/dsp_ad_puton_streaming/start_dsp_streaming_yarn_test.sh&quot;,
    &quot;com.xxxx.streaming.CPDAppStreaming&quot;: &quot;/home/hadoop/statistics/ad/dsp_app_promotion/start_dsp_app_promotion_yarn.sh&quot;
}


class WeChat(object):
    &#39;&#39;&#39;
    发送微信工具类
    &#39;&#39;&#39;

    def __init__(self, corpid, corpsecret, tokenpath):
        self.corpid = corpid
        self.corpsecret = corpsecret
        self.tokenpath = tokenpath
        self.logger = logging.getLogger(&#39;wechat&#39;)

    def saveToken(self):
        &#39;&#39;&#39;
        :return:
        &#39;&#39;&#39;
        try:
            with open(self.tokenpath, &#39;r&#39;) as f:
                token = f.read()
                if len(token) &lt; 10:
                    token = self.getToken()
                    self.logger.info(&quot;Can not get token from %s,prepare to get token on api which token is %s&quot; % (
                        self.tokenpath, token))
                    return token
                else:
                    return token
        except IOError:
            token = self.getToken()
            self.logger.info(
                &quot;Can not get token from %s,prepare to get token on api which token is %s&quot; % (self.tokenpath, token))
            return token

    def getToken(self):
        Url = &#39;https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=%s&amp;corpsecret=%s&#39; % (self.corpid, self.corpsecret)
        req = urllib2.Request(Url)
        result = urllib2.urlopen(req)
        json_access_token = json.loads(result.read())
        access_token = json_access_token[&#39;access_token&#39;]

        with open(self.tokenpath, &#39;w&#39;) as f:
            f.write(access_token)
        return access_token

    def setMessage(self, wechatids, text):
        token = self.saveToken()
        message = self.makeMessage(text)
        submiturl = &#39;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token={0}&#39;.format(token)
        data = {&quot;touser&quot;: wechatids, &quot;msgtype&quot;: &quot;text&quot;, &quot;agentid&quot;: &quot;1000002&quot;, &quot;text&quot;: {&quot;content&quot;: message}, &quot;safe&quot;: &quot;0&quot;}
        data = json.dumps(data, ensure_ascii=False)

        send_request = urllib2.Request(submiturl, data)

        self.logger.info(&quot;Send wechat %s&quot; % text)

        response = json.loads(urllib2.urlopen(send_request).read())

        if response[&#39;errcode&#39;] == 42001 or response[&#39;errcode&#39;] == 40014:
            self.logger.info(&quot;Send wechat errorcode : %s&quot; % response[&#39;errcode&#39;])
            os.remove(self.tokenpath)
            self.setMessage(wechatids, text)

    def makeMessage(self, text):
        def date():
            date = time.strftime(&#39;%m-%d %H:%M:%S&#39;, time.localtime())
            return date

        return &quot;%s \nCall Time:%s&quot; % (text, date())


class Message(object):
    &#39;&#39;&#39;
    构造邮箱发送的内容
    &#39;&#39;&#39;

    def format_str(self, strs):
        if not isinstance(strs, unicode):
            strs = unicode(strs)
        return strs

    def __init__(self, from_user, to_user, subject, content, with_attach=False):
        &#39;&#39;&#39;

        :param from_user: 谁发过来的邮件
        :param to_user: 发给谁
        :param subject: 邮件主题
        :param content: 邮件内容
        :param with_attach: 邮件是否包含附件
        &#39;&#39;&#39;

        if with_attach:
            self._message = MIMEMultipart()
            self._message.attach(MIMEText(content, &#39;plain&#39;, &#39;utf-8&#39;))
        else:
            self._message = MIMEText(content, &#39;plain&#39;, &#39;utf-8&#39;)

        self._message[&#39;Subject&#39;] = Header(subject, &#39;utf-8&#39;)
        self._message[&#39;From&#39;] = Header(self.format_str(from_user), &#39;utf-8&#39;)
        self._message[&#39;To&#39;] = Header(self.format_str(to_user), &#39;utf-8&#39;)
        self._with_attach = with_attach

    def attach(self, file_path):
        if self._with_attach == False:
            print &quot;Please init the Message with attr &#39;with_attach = True&#39;&quot;
            exit(1)
        if os.path.isfile(file_path) == False:
            print &quot;The file doesn`t exist!&quot;
            exit(1)
        atta = MIMEText(open(file_path, &#39;rb&#39;).read(), &#39;base64&#39;, &#39;utf-8&#39;)
        atta[&#39;Content-Type&#39;] = &#39;application/octet-stream&#39;
        atta[&#39;Content-Disposition&#39;] = &#39;attachment; filename=&quot;%s&quot;&#39; % Header(os.path.basename(file_path), &#39;utf-8&#39;)
        self._message.attach(atta)

    def getMessage(self):
        return self._message.as_string()


class SMTPClient(object):
    &#39;&#39;&#39;
    发送邮件工具类
    &#39;&#39;&#39;

    def __init__(self, hostname, port, user, passwd):
        &#39;&#39;&#39;
        初始化相关参数
        :param hostname: QQ邮箱:smtp.qq.com
        :param port: QQ邮箱ssl加密端口:465
        :param user: QQ邮箱账号
        :param passwd: QQ邮箱授权秘钥,在web qq邮箱上获取
        &#39;&#39;&#39;
        self._HOST = hostname
        self._PORT = port
        self._USER = user
        self._PASS = passwd

    def send(self, receivers, msg):
        &#39;&#39;&#39;
        发送邮件方法
        :param receivers: 邮件接收者,可以是多个.为列表
        :param msg: 发送的邮件内容
        :return:
        &#39;&#39;&#39;
        if isinstance(msg, Message) == False:
            print &quot;Error Message Instance!&quot;
            exit(1)
        try:
            smtpObj = smtplib.SMTP_SSL(self._HOST, self._PORT)
            smtpObj.connect(self._HOST)
            smtpObj.login(self._USER, self._PASS)
            smtpObj.sendmail(self._USER, receivers, msg.getMessage())
            return (1, &quot;邮件发送成功&quot;)
        except smtplib.SMTPException, e:
            return (0, &quot;Error: 无法发送邮件%s&quot; % e)


def run_it(cmd):
    &#39;&#39;&#39;
    通过python执行shell命令
    :param cmd:
    :return:
    &#39;&#39;&#39;
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True,
                         stderr=subprocess.PIPE)
    # print (&#39;running:%s&#39; % cmd)
    out, err = p.communicate()
    if p.returncode != 0:
        print (&quot;Non zero exit code:%s executing: %s \nerr course ---&gt; %s&quot; % (p.returncode, cmd, err))
    return out


def reStartSparkScript(scriptPath):
    &#39;&#39;&#39;
    执行spark脚本
    1.cd到脚本所在路径
    2.在改路径执行脚本
    :param scripyPath:
    :return:
    &#39;&#39;&#39;
    logger = logging.getLogger(&quot;Main&quot;)
    scriptDir, script = os.path.split(scriptPath)
    os.chdir(scriptDir)
    run_it(&quot;sh %s&quot; % script)
    logger.info(&quot;exec [ %s ] on [ %s ] &quot; % (script, scriptDir))


def collectMonitorStatus(yarnRestApi):
    &#39;&#39;&#39;
    从Yarn的Running接口或者Accept接口中获取我们需要监控的程序状态
    :param str: yarn的running接口或者accept接口
    :return:
    &#39;&#39;&#39;
    strUrl = run_it(yarnRestApi)
    result = []
    obj = json.loads(strUrl)
    if obj[&#39;apps&#39;] is None:
        return result
    else:
        apps = obj[&#39;apps&#39;][&#39;app&#39;]
        result = [(app[&#39;name&#39;], app[&#39;state&#39;]) for app in apps if app[&#39;name&#39;] in monitorPrograms]
        return result


def checkMonitorApps():
    &#39;&#39;&#39;
    调用yarn的running接口和accept接口
    判断这里面是否有我们需要监控的spark程序
        如果没有就执行报警和重启
    :return:
    &#39;&#39;&#39;

    logging.basicConfig(level=logging.INFO,
                        format=&#39;%(asctime)s - %(message)s&#39;,
                        datefmt=&#39;%Y-%m-%d %H:%M:%S&#39;)

    logger = logging.getLogger(&quot;Main&quot;)

    smpt_client = SMTPClient(&#39;smtp.qq.com&#39;, 465, &#39;694244330@qq.com&#39;, &#39;xxxxxx&#39;)
    wechat_client = WeChat(&#39;xxxxxxxxxxxxxx&#39;, &#39;xxxxxxxxxxxxxxxxxxxxxxxxx&#39;, &#39;/tmp/token.txt&#39;)

    runningStatus = collectMonitorStatus(urlRun)
    acceptStatus = collectMonitorStatus(urlAcc)

    runningAcceptApps = dict(runningStatus + acceptStatus)

    logger.info(&quot;SparkStreaming ON Yarn Running And Accept ===&gt;%s &quot; % str(runningAcceptApps))

    for monitor in monitorPrograms:
        if monitor not in runningAcceptApps:
            logging.info(&quot;[ %s ] is not running or accept,prepare to restart!&quot; % monitor)
            msg = Message(&quot;694244330@qq.com&quot;, &quot;hushwiei&quot;, monitor, &#39;%s is failed, prepare to resart! -- %s&#39; % (
                monitor, time.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;, time.localtime())))
            smpt_client.send(sendEmails, msg)
            wechat_client.setMessage(wechats, &quot;%s is not running or accept,prepare to restart!&quot; % monitor)
            reStartSparkScript(monitorPrograms[monitor])


def main():
    checkMonitorApps()


if __name__ == &#39;__main__&#39;:
    main()


</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YARN 资源分配的配置参数]]></title>
    <link href="http://dmlcoding.com/15198728348034.html"/>
    <updated>2018-03-01T10:53:54+08:00</updated>
    <id>http://dmlcoding.com/15198728348034.html</id>
    <content type="html"><![CDATA[
<p>无论是mapreduce程序或者是Spark程序,提交到Yarn上来进行资源管理与分配的时候.都是运行在Yarn的Container容器中.<br/>
而Container容器中是Yarn封装的内存和CPU资源.暂时还不支持对网络IO等资源进行封装分配.那么在开发调优过程中,我们肯定无法避免会对内存进行一些分配.那么Yarn的哪些配置参数是对哪个地方进行分配的,就很重要,也就值得记一记.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">内存资源</h1>

<h2 id="toc_1">ResourceManager</h2>

<table>
<thead>
<tr>
<th style="text-align: left">配置参数</th>
<th style="text-align: left">说明</th>
<th style="text-align: left">备注</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: left">yarn.scheduler.minimum-allocation-mb</td>
<td style="text-align: left">单个任务可申请的最少物理内存量，默认是1024（MB），如果一个任务申请的物理内存量少于该值，则该对应的值改为这个数</td>
<td style="text-align: left"></td>
</tr>
<tr>
<td style="text-align: left">yarn.scheduler.maximum-allocation-mb</td>
<td style="text-align: left">单个任务可申请的最多物理内存量，默认是8192（MB)</td>
<td style="text-align: left"></td>
</tr>
</tbody>
</table>

<p>说明:<br/>
也就是ResourceManager启动的Container容器的最大与最小内存.</p>

<h2 id="toc_2">nodemanager</h2>

<table>
<thead>
<tr>
<th style="text-align: left">配置参数</th>
<th style="text-align: left">说明</th>
<th style="text-align: left">备注</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: left">yarn.nodemanager.resource.memory-mb</td>
<td style="text-align: left">节点最大可用内存,默认8096M</td>
<td style="text-align: left"></td>
</tr>
<tr>
<td style="text-align: left">yarn.nodemanager.vmem-pmem-ratio</td>
<td style="text-align: left">虚拟内存率，任务每使用1MB物理内存，最多可使用虚拟内存量，默认为 2.1</td>
<td style="text-align: left"></td>
</tr>
<tr>
<td style="text-align: left">yarn.nodemanager.pmem-check-enabled</td>
<td style="text-align: left">是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值,则直接将其杀掉，默认是true</td>
<td style="text-align: left"></td>
</tr>
<tr>
<td style="text-align: left">yarn.nodemanager.vmem-check-enabled</td>
<td style="text-align: left">是否启动一个线程检查每个任务正使用的虚拟内存量,如果任务超出分配值，则直接将其杀掉，默认是true</td>
<td style="text-align: left"></td>
</tr>
</tbody>
</table>

<p>说明:<br/>
1. 在 Centos/RHEL 6 下，由于虚拟内存的分配策略比较激进，可以调高 yarn.nodemanager.vmem-pmem-ratio 或者关闭 yarn.nodemanager.vmem-check-enabled。<br/>
2. 不然的话有时候就会碰上抛出容器超出内存限制,然后容器被kill掉.<br/>
3. 比如这个问题 <a href="https://stackoverflow.com/questions/21005643/container-is-running-beyond-memory-limits">https://stackoverflow.com/questions/21005643/container-is-running-beyond-memory-limits</a></p>

<h2 id="toc_3">ApplicationMaster</h2>

<table>
<thead>
<tr>
<th style="text-align: left">配置参数</th>
<th style="text-align: left">说明</th>
<th style="text-align: left">备注</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: left">mapreduce.map.memory.mb</td>
<td style="text-align: left">分配给 Map Container的内存大小，运行时按需指定</td>
<td style="text-align: left"></td>
</tr>
<tr>
<td style="text-align: left">mapreduce.reduce.memory.mb</td>
<td style="text-align: left">分配给 Reduce Container的内存大小，运行时按需指定</td>
<td style="text-align: left"></td>
</tr>
<tr>
<td style="text-align: left">mapreduce.map.java.opts</td>
<td style="text-align: left">运行 Map 任务的 jvm 参数，如 -Xmx，-Xms 等选项</td>
<td style="text-align: left"></td>
</tr>
<tr>
<td style="text-align: left">mapreduce.reduce.java.opts</td>
<td style="text-align: left">运行 Reduce 任务的 jvm 参数，如-Xmx，-Xms等选项</td>
<td style="text-align: left"></td>
</tr>
</tbody>
</table>

<h1 id="toc_4">CPU资源</h1>

<table>
<thead>
<tr>
<th style="text-align: left">配置参数</th>
<th style="text-align: left">说明</th>
<th style="text-align: left">备注</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: left">yarn.nodemanager.resource.cpu-vcores</td>
<td style="text-align: left">该节点上 YARN 可使用的虚拟 CPU 个数，默认是8</td>
<td style="text-align: left"></td>
</tr>
<tr>
<td style="text-align: left">yarn.scheduler.minimum-allocation-vcores</td>
<td style="text-align: left">单个任务可申请的最小虚拟CPU个数, 默认是1</td>
<td style="text-align: left"></td>
</tr>
<tr>
<td style="text-align: left">yarn.scheduler.maximum-allocation-vcores</td>
<td style="text-align: left">单个任务可申请的最多虚拟CPU个数，默认是32</td>
<td style="text-align: left"></td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深入理解Java虚拟机之JDK可视化工具(二)]]></title>
    <link href="http://dmlcoding.com/15198757088879.html"/>
    <updated>2018-03-01T11:41:48+08:00</updated>
    <id>http://dmlcoding.com/15198757088879.html</id>
    <content type="html"><![CDATA[
<p>除了JDK命令行工具,还有几个很强大的JDK可视化工具,希望接下来的学习,可以提高我们解决bug的能力</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">JConsole:Java监视与管理控制台</h1>

<blockquote>
<p>JConsole(Java Monitoring and Management Console)<br/>
JConsole是在JDK1.5时期就已经提供的虚拟机监控工具<br/>
JConsole是一款基于JMX的可视化监视和管理工具,它管理部分的功能是针对JMX MBean进行管理.</p>
</blockquote>

<h2 id="toc_1">启动JConsole</h2>

<ul>
<li>1.安装的bin目录下执行<code>jconsole</code></li>
<li>2.如果配置了 <strong>JAVA_HOME</strong> 直接输入<code>jconsole</code></li>
</ul>

<p>Jconsole启动后,会自动搜索出本机运行的所有虚拟机进程,不需要用户再使用jps来查询了,如下图所示,双击选择其中一个进程即可开始监控.当然也可以使用 <em>远程进程</em> 功能来连接远程服务器,对远程虚拟机进行监控</p>

<p><img src="media/15198757088879/jconsole1.png" alt="jconsole1"/></p>

<p>栈内存     堆内存<br/>
-Xms100m -Xmx100m -XX:+UseSerialGC</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深入理解Java虚拟机之Java内存区域与内存溢出]]></title>
    <link href="http://dmlcoding.com/15198760861207.html"/>
    <updated>2018-03-01T11:48:06+08:00</updated>
    <id>http://dmlcoding.com/15198760861207.html</id>
    <content type="html"><![CDATA[
<p>对于java程序员来说,虽然有虚拟机的自动内存管理机制,我们即使不清楚内存是如何分配的,也不妨碍我们写代码.但是如果你不明白虚拟机究竟做了啥,你既不能快速定位问题,也不能成为一个优秀的程序员.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">概述</h1>

<p>对于java程序员来说,在虚拟机的自动内存管理机制的帮助下,不再需要为每一个new操作去写配对的delete/free代码(C/C++语言是需要的),而且不容易出现内存泄漏和内存溢出问题,看起来由虚拟机管理内存一切都很美好.不过,也正是因为java程序员把内存控制的权利交给了Java虚拟机,一旦出现内存泄露和溢出方面的问题,如果不了解虚拟机是怎样使用内存的,那排查错误将会成为一项异常艰难的工作.<br/>
在这篇文章里,我们会写到这几个部分,了解了这几个部分,也就可以翻越虚拟机内存管理的第一步.<br/>
+ java虚拟机内存的各个区域<br/>
+ 各个区域的作用,服务对象,以及其中可能产生的问题</p>

<h1 id="toc_1">运行时数据区域</h1>

<ul>
<li>Java虚拟机在执行Java程序的过程中,会把它管理的内存划分为若干个不同的数据区域.</li>
<li>这些区域都有各自的用途,以及创建和销毁的时间,有的区域随着虚拟机进程的启动而存在,有些区域则是依赖用户线程的启动和结束而建立和销毁.
<strong>根据Java虚拟机规范规定,Java虚拟机所管理的内存将会包括以下几个运行时数据区域</strong></li>
</ul>

<p><img src="media/15198760861207/jdkquyu.png" alt="jdkquyu"/></p>

<h2 id="toc_2">程序计数器</h2>

<blockquote>
<p>程序计数器(Program Counter Register)是一块较小的内存空间,它的作用可以看做是当前线程所执行的字节码的行号指示器.</p>
</blockquote>

<p>字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令,分支,循环,跳转,异常处理,线程恢复等基础功能都需要依赖这个计数器来完成.(想想我们平时写的java代码,是不是感觉一切都是有原因的)</p>

<p>对于多线程来说.由于每个线程都会执行自己的指令.那么为了线程切换后能恢复到正确的执行位置,因此每条线程都需要有一个独立的程序计数器,这样每条线程之间的计数器互不影响,独立存储.我们称这类内存区域称为 <strong>线程私有</strong> 的内存.</p>

<h2 id="toc_3">Java虚拟机栈</h2>

<blockquote>
<p>与程序计数器一样,Java虚拟机栈(Java Virtual Machine Stacks)也是线程私有的.它的生命周期与线程相同.</p>
</blockquote>

<p>我们平时所说的栈内存,也就是这里的虚拟机栈.那么这个虚拟机栈究竟是什么呢?<br/>
虚拟机栈描述的是java方法执行的内存模型:<br/>
+ 每个方法被执行的时候都会同时创建一个 <strong>栈帧(Stack Frame)</strong> 用于存储<code>局部变量表</code>,<code>操作栈</code>,<code>动态链接</code>,<code>方法出口</code>等信息.<br/>
+ 每一个方法被调用直至执行完成的过程,就对应着一个 <strong>栈帧</strong> 在虚拟机栈中从入栈到出栈的过程.</p>

<p>提一下局部变量表:<br/>
+ 局部变量表存放了编译期可知的各种基本数据类型(boolean,byte,char,short,int,float,long,double),对象引用(reference类型,它不等同于对象本身).<br/>
+ 局部变量表所需的内存空间在编译期间完成分配,当进入一个方法时,这个方法需要在帧中分配多大的局部变量空间是完全确定的,在方法运行期间不会改变局部变量表的大小.<br/>
+ 局部变量表区域可能会抛出两种异常状况<br/>
  + 如果线程请求的栈深度大于虚拟机所允许的深度,将抛出StackOverflowError异常.<br/>
  + 如果虚拟机可以动态扩展,当扩展时无法申请到足够的内存时会抛出OutOfMemoryError.</p>

<h2 id="toc_4">本地方法栈</h2>

<ul>
<li>本地方法栈(Native Method Stacks)与虚拟机所发挥的作用是非常相似的</li>
<li>本地方法栈与虚拟机栈的区别

<ul>
<li>虚拟机栈为虚拟机执行Java方法(也就是字节码)服务</li>
<li>本地方法栈则是为虚拟机使用到的Native方法服务</li>
</ul></li>
<li>有些虚拟机(譬如Sun HotSpot)直接就把本地方法栈和虚拟机栈合二为一</li>
</ul>

<p>关于什么是Native方法呢?<br/>
参考:<a href="http://blog.csdn.net/wike163/article/details/6635321">http://blog.csdn.net/wike163/article/details/6635321</a></p>

<h2 id="toc_5">Java堆</h2>

<p><strong>堆内存的特点</strong><br/>
+ 对于大多数应用来说,Java堆(Java heap)是Java虚拟机所管理的内存中最大的一块.<br/>
+ Java堆是被所有线程共享的一块内存区域,<strong>在虚拟机启动时创建</strong> .<br/>
+ Java堆内存区域的唯一目的就是存放对象实例,几乎所有的对象实例都是在这里分配内存.<br/>
+ Java堆是垃圾收集器管理的主要区域.因为那么多实例在堆上分配内存,实例用完后,我们肯定要及时回收内存,这样才能给新的实例分配足够的内存呢.<br/>
+ Java堆可以处于物理上不连续的内存空间中,只要逻辑上是连续的即可,就像我们的磁盘空间一样.<br/>
+ 如果在堆中没有内存完成实例分配,并且堆也无法再扩展时,将会抛出OutOfMemoryErrory异常.</p>

<p><strong>再细分一下堆内存</strong><br/>
+ 从垃圾回收的角度看<br/>
  + 因为现在的垃圾收集器都是采用分代收集算法,所以Java堆中还可以细分为:<code>新生代</code>,<code>老年代</code>,等等区域.<br/>
  + 后面写到垃圾回收的时候,再细说这部分.<br/>
+ 从内存分配的角度看<br/>
  + 线程共享的Java堆中可能划分出多个线程私有的分配缓存区<br/>
+ 在实现上<br/>
  + 既可以实现成固定大小的,也可以是扩展的.<br/>
  + 不过目前主流的虚拟机都是按照可扩展来实现的<br/>
  + <code>-Xmx</code>来设置程序的堆内存大小<br/>
  + <code>-Xms</code>来设置程序的栈内存大小</p>

<h2 id="toc_6">方法区</h2>

<blockquote>
<p>方法区(Method Area)<br/>
<strong>方法区的特点</strong><br/>
+ 方法区与Java堆一样,是各个线程共享的内存区域.<br/>
+ 它用于存储已被虚拟机加载的类信息,常量,静态变量,即时编译器编译后的代码等数据.<br/>
+ 然后Java虚拟机规范把方法区描述为堆的一个逻辑部分,但是它却有一个别名叫做Non-Heap(非堆),目的应该是与Java堆区分开来.<br/>
+ 方法区被有些人称为&quot;永久代(Permanent Generation)&quot;是因为GC分代收集时候,方法区的变量会在永久代区域.<br/>
+ 当方法区无法满足内存分区需求时,将抛出OutOfMemoryErrory异常.</p>
</blockquote>

<h2 id="toc_7">运行时常量池</h2>

<p>运行时常量池是方法区的一部分.Class文件中除了有类的版本,字段,方法,接口等描述信息外,还有一项信息是 <strong>常量池(Constant Pool Table)</strong>.<br/>
用于存放编译期生成的各种字面量和符号引用,这部分内容将在类加载后存放到方法区的运行时常量池中.</p>

<p>Java语言并不要求常量一定只能在编译器产生,也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池,运行期间也可能将新的常量放入池中.</p>

<h2 id="toc_8">直接内存</h2>

<p>直接内存(Direct Memory)并不是虚拟机运行时数据区的一部分,也不是Java虚拟机规范中定义的内存区域,但是这部分内存也被频繁地使用,而且也可能导致OutOfMemoryErrory异常.那么究竟什么是直接内存呢?</p>

<p>在JDK1.4中新加入了NIO(New Input/Output)类,引入了一种基于通道(Channel)与缓冲区(Buffer)的I/O方式,它可以使用Native函数库直接分配堆外内存,然后通过一个存储在Java堆里面的DirectByteBuffer对象作为这块内存的引用进行操作.</p>

<p>这样能在一些场景中显著提高性能,因为避免了在Java堆和Native堆中来回复制数据.</p>

<h1 id="toc_9">对象访问(问题:在Java语言中,对象访问是如何进行的?)</h1>

<blockquote>
<p>对象访问在java语言中无处不在,是最普通的程序行为,但即使是最简单的访问,也会涉及Java栈,Java堆,方法区这三个最重要内存区域之间的关联关系.<br/>
上面简单介绍了Java虚拟机的运行时数据区.说得比较文字化,不够具体.那么我们具体来探讨一个问题.<br/>
<strong>在Java语言中,对象访问是如何进行的?</strong></p>
</blockquote>

<p>看这行最简单的代码,我们来解释这行代码.<br/>
<code><br/>
Object obj=new Object();<br/>
</code><br/>
假设这句代码出现在方法体中,那么<br/>
+ <strong>Object obj</strong> 这部分的语义将会反映到 <strong>Java栈的本地变量表</strong> 中,作为一个reference类型数据出现.<br/>
+ <strong>new Object()</strong> 这部分的语义将会反映到 <strong>Java堆</strong> 中,形成一块存储了Object类型所有实例数据值的结构化内存.<br/>
  + 这块内存的长度是不固定<br/>
  + 另外,在这个堆中还必须包含能查找到此对象类型数据(如对象类型,父类,实现的接口,方法等)的地址信息,这些类型数据则存储在方法区.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深入理解Java虚拟机之JDK命令行工具(一)]]></title>
    <link href="http://dmlcoding.com/15198759869483.html"/>
    <updated>2018-03-01T11:46:26+08:00</updated>
    <id>http://dmlcoding.com/15198759869483.html</id>
    <content type="html"><![CDATA[
<p>JDK命令行工具,是java提供给我们的礼物,我们怎么能拒绝他们的馈赠呢</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">jps:虚拟机进程状况工具</h1>

<blockquote>
<p>jps(JVM Process Status)<br/>
可以列出正在运行的虚拟机进程,,并显示虚拟机执行主类(main函数的名称),以及这些进程的本地虚拟机的唯一ID(LVMID,Local Virtual Machine Identifier)<br/>
对于本地虚拟机进程来说,LVMID与操作系统的进程ID(PID,Process Identifier)是一致的</p>
</blockquote>

<h2 id="toc_1">jps命令格式</h2>

<pre><code>jps [options] [hostid]
</code></pre>

<h2 id="toc_2">jps工具主要选项</h2>

<table>
<thead>
<tr>
<th style="text-align: center">选项</th>
<th style="text-align: left">作用</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: center">-q</td>
<td style="text-align: left">只输出LVMID,省略主类的名称</td>
</tr>
<tr>
<td style="text-align: center">-m</td>
<td style="text-align: left">输出虚拟机进程启动时传递给主类main()函数的参数</td>
</tr>
<tr>
<td style="text-align: center">-l</td>
<td style="text-align: left">输出主类的全名,如果进程执行的是jar包,输出jar路径</td>
</tr>
<tr>
<td style="text-align: center">-v</td>
<td style="text-align: left">输出虚拟机进程启动时JVM参数</td>
</tr>
</tbody>
</table>

<h2 id="toc_3">jps命令样例1</h2>

<pre><code>[hadoop@U006 ~]$ jps -l
3183 sun.tools.jps.Jps
17831 org.apache.spark.executor.CoarseGrainedExecutorBackend
10004 org.apache.spark.deploy.worker.Worker
17659 org.apache.spark.deploy.SparkSubmit
10254 org.apache.spark.deploy.worker.Worker
9830 org.apache.spark.deploy.master.Master
</code></pre>

<h1 id="toc_4">jstat:虚拟机统计信息监视工具</h1>

<blockquote>
<p>jstat(JVM Statistics Monitoring Tool)<br/>
jstat是用于监视虚拟机各种运行状态信息的命令行工具<br/>
它可以显示本地或远程虚拟机进程中的类装载,内存,垃圾收集,JIT编译等运行数据.<br/>
在没有GUI图形界面,只提供了纯文本控制台环境的服务器上,它将是运行期定位虚拟机性能问题的首选工具</p>
</blockquote>

<h2 id="toc_5">jstat命令格式</h2>

<pre><code>jstat [option vmid [interval[s|ms] [count]]]
</code></pre>

<p><strong>注意</strong>:<br/>
+ 对于命令中的VMID与LVMID需要特别说明一下:如果是本地虚拟机进程,VMID与LVMID是一致的.<br/>
+ 如何是远程虚拟机进程,那VMID的格式应当是:<code>[protocal:][//]lvmid[@hostname[:port]/servername]</code><br/>
+ 参数interval和count代表查询间隔和次数.如果省略这两个参数,说明只查询一次.</p>

<p>假设需要每250毫秒查询一次进程2764垃圾收集的状况,一共查询20次,那么命令应该是:<br/>
<code><br/>
jstat -gc 2764 250 20<br/>
</code></p>

<p>每2毫秒查询10次spark进程的垃圾收集状况:<br/>
<code><br/>
[hadoop@U006 ~]$ jstat -gc 17659 2 10<br/>
 S0C    S1C    S0U    S1U      EC       EU        OC         OU       PC     PU    YGC     YGCT    FGC    FGCT     GCT<br/>
349184.0 349184.0  0.0    0.0   2098176.0 954275.3 5592576.0   246723.4  116224.0 115802.6     52    5.099  48     18.956   24.056<br/>
349184.0 349184.0  0.0    0.0   2098176.0 954275.3 5592576.0   246723.4  116224.0 115802.6     52    5.099  48     18.956   24.056<br/>
349184.0 349184.0  0.0    0.0   2098176.0 954275.3 5592576.0   246723.4  116224.0 115802.6     52    5.099  48     18.956   24.056<br/>
349184.0 349184.0  0.0    0.0   2098176.0 954275.3 5592576.0   246723.4  116224.0 115802.6     52    5.099  48     18.956   24.056<br/>
349184.0 349184.0  0.0    0.0   2098176.0 954275.3 5592576.0   246723.4  116224.0 115802.6     52    5.099  48     18.956   24.056<br/>
349184.0 349184.0  0.0    0.0   2098176.0 954275.3 5592576.0   246723.4  116224.0 115802.6     52    5.099  48     18.956   24.056<br/>
349184.0 349184.0  0.0    0.0   2098176.0 954275.3 5592576.0   246723.4  116224.0 115802.6     52    5.099  48     18.956   24.056<br/>
349184.0 349184.0  0.0    0.0   2098176.0 954275.3 5592576.0   246723.4  116224.0 115802.6     52    5.099  48     18.956   24.056<br/>
349184.0 349184.0  0.0    0.0   2098176.0 954275.3 5592576.0   246723.4  116224.0 115802.6     52    5.099  48     18.956   24.056<br/>
349184.0 349184.0  0.0    0.0   2098176.0 954275.3 5592576.0   246723.4  116224.0 115802.6     52    5.099  48     18.956   24.056<br/>
</code></p>

<h2 id="toc_6">jstat工具主要选项</h2>

<table>
<thead>
<tr>
<th style="text-align: center">选项</th>
<th style="text-align: left">作用</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: center">-class</td>
<td style="text-align: left">监视类装载,卸载数据,总空间及类装载所耗费的时间</td>
</tr>
<tr>
<td style="text-align: center">-gc</td>
<td style="text-align: left">监视Java堆状况,包括Eden区,2个survivor区,老年代,永久代等的容量,已用空间,GC时间合计等信息</td>
</tr>
<tr>
<td style="text-align: center">-gccapacity</td>
<td style="text-align: left">监视内容与-gc基本相同,但输出主要关注Java堆各个区域使用到的最大和最小空间</td>
</tr>
<tr>
<td style="text-align: center">-gcutil</td>
<td style="text-align: left">监视内容与-gc基本相同,但输出主要关注已使用空间占总空间的百分比</td>
</tr>
<tr>
<td style="text-align: center">...</td>
<td style="text-align: left">...</td>
</tr>
</tbody>
</table>

<h2 id="toc_7">jstat执行样例</h2>

<pre><code>hadoop@U006 ~]$ jstat -gcutil 17659
  S0     S1     E      O      P     YGC     YGCT    FGC    FGCT     GCT
  0.00   0.00  37.59   4.41  99.68     77    7.439    73   27.473   34.912
</code></pre>

<p>查询结果表明:<br/>
这个进程的<br/>
+ 新生代Eden区(E,表示Eden)使用了37.59%的空间.<br/>
+ 两个Survivor区(S0,S1,表示Survivor0,Survivor1)里面都是空的.<br/>
+ 老年代(O,表示Old)和永久代(P,表示Permanent)则分别使用了4.41%和99.68%的空间.<br/>
+ 程序运行以来共发生Minor GC(YGC,表示Young GC)77次,总耗时7.439秒.<br/>
+ 发生Full GC(FGC,表示Full GC)73次,Full GC总耗时(FGCT,表示Full GC Time)为27.473秒.<br/>
+ 所有GC总耗时(GCT,表示GC Time)为34.912秒.</p>

<p><strong>总结</strong>:使用jstat工具在纯文本状态下监视虚拟机状态的变化,虽然没有一些可视化监控工具来得直观.但与我而言,更显极客本色.</p>

<h1 id="toc_8">jinfo:Java配置信息工具</h1>

<blockquote>
<p>jinso(Configuration Info For Java)<br/>
jinfo的作用是实时地查看和调整虚拟机的各项参数</p>
</blockquote>

<h2 id="toc_9">jinfo命令格式</h2>

<pre><code>jinfo [option] pid
</code></pre>

<h2 id="toc_10">使用方式</h2>

<p>前面解释过jps的-v选项.这个可以查看虚拟机启动时显示指定的参数列表,比如<br/>
<code><br/>
[hadoop@U006 ~]$ jps -v<br/>
17831 CoarseGrainedExecutorBackend -Xms4096M -Xmx4096M -Dspark.driver.port=42195 -XX:MaxPermSize=256m<br/>
5146 Jps -Dapplication.home=/usr/java/jdk1.7.0_71 -Xms8m<br/>
10004 Worker -Xms1g -Xmx1g -XX:MaxPermSize=256m<br/>
17659 SparkSubmit -Xms8g -Xmx8g -XX:MaxPermSize=256m<br/>
10254 Worker -Xms1g -Xmx1g -XX:MaxPermSize=256m<br/>
9830 Master -Xms1g -Xmx1g -XX:MaxPermSize=256m<br/>
</code><br/>
但如果想知道未被显示指定的参数的系统默认值,除了查找资料,还可以使用这里的<code>jinfo的-flag</code>选项进行查询了.</p>

<p>比如查询CMSInitiatingOccupancyFraction参数值:<br/>
<code><br/>
[hadoop@U006 ~]$ jinfo -flag CMSInitiatingOccupancyFraction 17659<br/>
-XX:CMSInitiatingOccupancyFraction=-1<br/>
</code></p>

<h1 id="toc_11">jmap:Java内存映像工具</h1>

<blockquote>
<p>jmap(Memory Map For Java)<br/>
jmap是用于生成堆转储快照(一般称为Heapdump或dump文件).</p>
</blockquote>

<h1 id="toc_12">jstack:Java堆栈跟踪工具</h1>

<blockquote>
<p>jstack(Stack Trace for Java)<br/>
jstack命令用于生成虚拟机当前时刻的线程快照(一般称为threaddump或javacore文件)</p>
</blockquote>

<p><strong>线程快照:</strong> 就是当前虚拟机内每一条线程正在执行的方法堆栈的集合,生成线程快照的主要目的是 <em>定位线程出现长时间停顿的原因</em>.</p>

<p>线程长时间停顿的常见原因:<br/>
1.线程间死锁,死循环<br/>
2.请求外部资源导致的长时间等待<br/>
3.等等</p>

<p>线程出现停顿的时候通过jstack来查看各个线程的调用堆栈,就可以知道没有响应的线程到底在后台做些什么事情,或者等待着什么资源.</p>

<h2 id="toc_13">jstack命令格式</h2>

<pre><code>jstack [option] vmid
</code></pre>

<h2 id="toc_14">jstack工具的主要选项</h2>

<table>
<thead>
<tr>
<th style="text-align: center">选项</th>
<th style="text-align: left">作用</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: center">-F</td>
<td style="text-align: left">当正常输出的请求不被响应时,强制输出线程堆栈</td>
</tr>
<tr>
<td style="text-align: center">-l</td>
<td style="text-align: left">除堆栈外,显示关于锁的附加信息</td>
</tr>
<tr>
<td style="text-align: center">-m</td>
<td style="text-align: left">如果调用到本地方法的话,可以显示C/C++的堆栈</td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[python 可视化包-Matplotlib]]></title>
    <link href="http://dmlcoding.com/15198750755456.html"/>
    <updated>2018-03-01T11:31:15+08:00</updated>
    <id>http://dmlcoding.com/15198750755456.html</id>
    <content type="html"><![CDATA[
<p>Matplotlib是Python中最常用的可视化工具之一，可以非常方便地创建海量类型地2D图表和一些基本的3D图表。</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">安装方式</h1>

<pre><code># ubuntu上安装
sudo apt install python-matplotlib
# mac上安装
pip install matplotlib
</code></pre>

<h1 id="toc_1">快速入门</h1>

<h2 id="toc_2">快速入门小例子1之画单个图</h2>

<p>我们只要有x轴的数和y轴的数,那么就可以在坐标轴上画出图来了.<br/>
```<br/>
import numpy as np<br/>
import matplotlib as mpl<br/>
import matplotlib.pyplot as plt</p>

<h1 id="toc_3">通过rcParams设置全局横纵轴字体大小</h1>

<p>mpl.rcParams[&#39;xtick.labelsize&#39;]=24<br/>
mpl.rcParams[&#39;ytick.labelsize&#39;]=24</p>

<h1 id="toc_4">x轴的点</h1>

<p>x1 = np.arange(11)</p>

<h1 id="toc_5">y轴的点</h1>

<p>y1 = [<br/>
    1.0847275042134147E-4,<br/>
    2.0106877828356476E-4,<br/>
    1.1836360644802181E-4,<br/>
    0.043453404423487926,<br/>
    0.03113001646083574,<br/>
    0.06,<br/>
    0.012709253496067191,<br/>
    0.06,<br/>
    3.284899860591644E-4,<br/>
    0.015235253124714847,<br/>
    0.0034946847451197242,<br/>
]</p>

<h1 id="toc_6">创建一个图,名字为ctr</h1>

<p>plt.figure(&quot;ctr&quot;)</p>

<h1 id="toc_7">在图上绘制</h1>

<p>plt.plot(x1,y1)</p>

<h1 id="toc_8">将当前figure的图像保存到文件result.png</h1>

<p>plt.savefig(&#39;result.pn<br/>
g&#39;)</p>

<h1 id="toc_9">一定要加上这句才能让画好的图显示在屏幕上</h1>

<p>plt.show()</p>

<pre><code>
如图所示:
![ct](media/15198750755456/ctr1.png)

看上面就没有几行代码,但是就画出了一个图.所以用Matplotlib可以非常方便的绘制我们想要的图形.这里这是用最简单的例子说明一下.

## 快速入门小例子2之把两组坐标画在一个图上进行比较

这里我们有两组数据,希望能够方便的比较这两组数据的差异,那么我们就可以把趋势都画在一个图上
</code></pre>

<p>import numpy as np<br/>
import matplotlib as mpl<br/>
import matplotlib.pyplot as plt</p>

<h1 id="toc_10">通过rcParams设置全局横纵轴字体大小</h1>

<p>mpl.rcParams[&#39;xtick.labelsize&#39;]=24<br/>
mpl.rcParams[&#39;ytick.labelsize&#39;]=24</p>

<h1 id="toc_11">x轴的点</h1>

<p>x1 = np.arange(11)</p>

<h1 id="toc_12">y轴的点</h1>

<p>y1 = [<br/>
    1.0847275042134147E-4,<br/>
    2.0106877828356476E-4,<br/>
    1.1836360644802181E-4,<br/>
    0.043453404423487926,<br/>
    0.03113001646083574,<br/>
    0.06,<br/>
    0.012709253496067191,<br/>
    0.06,<br/>
    3.284899860591644E-4,<br/>
    0.015235253124714847,<br/>
    0.0034946847451197242,<br/>
]</p>

<h1 id="toc_13">创建一个图,名字为ctr</h1>

<h1 id="toc_14">plt.figure(&quot;ctr&quot;)</h1>

<h1 id="toc_15">在图上绘制</h1>

<h1 id="toc_16">plt.plot(x1,y1)</h1>

<p>x2 = np.arange(11)</p>

<p>y2 = [<br/>
    3.529088519807792E-5,<br/>
    1.1895968858318187E-4,<br/>
    0.0013049292594645469,<br/>
    0.046417845349992326,<br/>
    0.03282177644291713,<br/>
    0.06,<br/>
    0.013313023920004725,<br/>
    0.06,<br/>
    3.554547063283854E-4,<br/>
    0.014309633417956262,<br/>
    0.0034946847451197242,<br/>
]</p>

<h1 id="toc_17">plt.figure(&quot;ctrEstimate&quot;)</h1>

<h1 id="toc_18">plt.plot(x2,y2,&#39;k&#39;)</h1>

<h1 id="toc_19">两个图画一起</h1>

<p>plt.figure(&#39;ctr &amp; ctrEstimate&#39;)<br/>
plt.plot(x1, y1)</p>

<h1 id="toc_20">scatter可以方便出散点图</h1>

<h1 id="toc_21">plt.scatter(x1,y11,c=&#39;red&#39;,marker=&#39;v&#39;)</h1>

<h1 id="toc_22">plt.scatter(x2,y22,marker=&#39;<sup>&#39;)</sup></h1>

<h1 id="toc_23">&#39;r&#39;表示用红色线</h1>

<p>plt.plot(x2, y2, &#39;r&#39;)</p>

<p>plt.show()<br/>
```<br/>
如图所示:<br/>
<img src="media/15198750755456/ctr2.png" alt="ct"/></p>

<h2 id="toc_24">入门小例子3之多布局</h2>

<p>在一张图上构建多个布局画多张图<br/>
```<br/>
import matplotlib as mpl<br/>
import matplotlib.pyplot as plt</p>

<h1 id="toc_25">通过rcParams设置全局横纵轴字体大小</h1>

<p>mpl.rcParams[&#39;xtick.labelsize&#39;]=24<br/>
mpl.rcParams[&#39;ytick.labelsize&#39;]=24</p>

<p>x=range(10)</p>

<p>y=[5,4,3,2,1,6,7,8,9,0]</p>

<p>fig=plt.figure(&quot;one figure many subplot&quot;)<br/>
ax=fig.add_subplot(131)<br/>
ax.set_title(&#39;Histogram&#39;)<br/>
ax.bar(x,y)</p>

<p>ax=fig.add_subplot(132)<br/>
ax.set_title(&#39;line chart&#39;)<br/>
ax.plot(x,y)</p>

<p>ax=fig.add_subplot(133)<br/>
ax.set_title(u&#39;Scatter plot&#39;)<br/>
ax.scatter(x,y)</p>

<p>plt.show()<br/>
```<br/>
如图所示</p>

<p><img src="media/15198750755456/figure.png" alt="figure"/></p>

<h1 id="toc_26">matplotlib画图api解释</h1>

<p>单独的讲解api很无聊.直接写代码画图,代码里都有详细的说明</p>

<h2 id="toc_27">画2维的柱图和饼图</h2>

<pre><code>
import numpy as np

import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams[&#39;axes.titlesize&#39;] = 20
mpl.rcParams[&#39;xtick.labelsize&#39;] = 16
mpl.rcParams[&#39;ytick.labelsize&#39;] = 16
mpl.rcParams[&#39;axes.labelsize&#39;] = 16
mpl.rcParams[&#39;xtick.major.size&#39;] = 0
mpl.rcParams[&#39;ytick.major.size&#39;] = 0

# 包含了狗,猫和猎豹的最高奔跑速度,还有对应的可视化颜色
speed_map = {
    &#39;dog&#39;: (48, &#39;#7199cf&#39;),
    &#39;cat&#39;: (45, &#39;#4fc4aa&#39;),
    &#39;cheetah&#39;: (120, &#39;#e1a7a2&#39;)
}

# 整体图的标图
fig=plt.figure(&#39;Bar chart &amp; Pie chart&#39;)

# 在整张图上加入一个子图,121的意思是在一个1行2列的子图中的第一张
ax=fig.add_subplot(121)
ax.set_title(&quot;Running speed - bar chart&quot;)

# 生成x轴每个元素的位置 0 1 2
xticks=np.arange(3)

# 定义柱状图每个柱的宽度
bar_width=0.5

# 动物名称
animals=speed_map.keys()

# 奔跑速度
speeds=[x[0] for x in speed_map.values()]

# 对应颜色
colors=[x[1] for x in speed_map.values()]

# 画柱状图,横轴是动物标签的位置,纵轴是速度,定义柱的宽度,同时设置柱的边缘为透明
bars=ax.bar(xticks,speeds,width=bar_width,edgecolor=&#39;none&#39;)

# 设置y轴的标图
ax.set_ylabel(&#39;Speed(km/h)&#39;)

# x轴每个标签的具体位置,设置为每个柱的中央
ax.set_xticks(xticks+bar_width/2)

# 设置每个标签的名字
ax.set_xticklabels(animals)

# 设置x轴的范围
ax.set_xlim([bar_width/2-0.5,3-bar_width/2])

# 设置y轴的范围
ax.set_ylim([0,125])

# 给每个bar分配指定的颜色
for bar,color in zip(bars,colors):
    bar.set_color(color)


# 在122位置加入新的图
ax=fig.add_subplot(122)
ax.set_title(&#39;Running speed - pie chart&#39;)
labels=[&#39;{}\n{} km/h&#39;.format(animal,speed) for animal,speed in zip(animals,speeds)]

# 画饼状图,并指定标签和对应颜色
ax.pie(speeds,labels=labels,colors=colors)
# ax.plot(speeds)


plt.show()

</code></pre>

<p><img src="media/15198750755456/subplot.png" alt="subplot"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[机器视觉处理与Tesseract介绍]]></title>
    <link href="http://dmlcoding.com/15198725905052.html"/>
    <updated>2018-03-01T10:49:50+08:00</updated>
    <id>http://dmlcoding.com/15198725905052.html</id>
    <content type="html"><![CDATA[
<p>在读取和处理图像、图像相关的机器学习以及创建图像等任务中，Python 一直都是非常出色的语言。虽然有很多库可以进行图像处理，但目前我只接触到Tesseract.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">Tesseract</h1>

<p>Tesseract 是一个 OCR 库,目前由 Google 赞助(Google 也是一家以 OCR 和机器学习技术闻名于世的公司)。Tesseract 是目前公认最优秀、最精确的开源 OCR 系统。 除了极高的精确度,Tesseract 也具有很高的灵活性。它可以通过训练识别出任何字体，也可以识别出任何 Unicode 字符。</p>

<h1 id="toc_1">安装Tesseract</h1>

<h2 id="toc_2">Windows 系统</h2>

<pre><code>下载可执行安装文件https://code.google.com/p/tesseract-ocr/downloads/list安装。
</code></pre>

<h2 id="toc_3">Linux 系统</h2>

<pre><code>可以通过 apt-get 安装: $sudo apt-get tesseract-ocr
</code></pre>

<h2 id="toc_4">Mac OS X系统</h2>

<p>用 Homebrew(<a href="http://brew.sh/)%E7%AD%89%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93%E5%8F%AF%E4%BB%A5%E5%BE%88%E6%96%B9%E4%BE%BF%E5%9C%B0%E5%AE%89%E8%A3%85">http://brew.sh/)等第三方库可以很方便地安装</a><br/>
<code><br/>
brew install tesseract<br/>
</code><br/>
要使用 Tesseract 的功能，比如后面的示例中训练程序识别字母，要先在系统中设置一个新的环境变量 $TESSDATA_PREFIX，让 Tesseract 知道训练的数据文件存储在哪里，然后搞一份tessdata数据文件，放到Tesseract目录下。</p>

<p>在大多数 Linux 系统和 Mac OS X 系统上,你可以这么设置:<br/>
<code><br/>
 $export TESSDATA_PREFIX=/usr/local/share/Tesseract<br/>
</code><br/>
或者<br/>
<code><br/>
hushiwei@localhost  ~  more ~/.bash_profile<br/>
alias l=&#39;ls -lF&#39;<br/>
alias ll=&#39;ls -alF&#39;<br/>
JAVA_HOME=`/usr/libexec/java_home`<br/>
SCALA_HOME=/Users/hushiwei/devApps/scala-2.10.5<br/>
MAVEN_HOME=/Users/hushiwei/devApps/maven-3.3.9<br/>
TESSDATA_PREFIX=/Users/hushiwei/devApps/Tesseract<br/>
</code><br/>
在 Windows 系统上也类似,你可以通过下面这行命令设置环境变量:<br/>
```</p>

<h1 id="toc_5">setx TESSDATA_PREFIX C:\Program Files\Tesseract OCR\Tesseract</h1>

<pre><code># 安装pytesseract
Tesseract 是一个 Python 的命令行工具，不是通过 import 语句导入的库。安装之后,要用 tesseract 命令在 Python 的外面运行，但我们可以通过 pip 安装支持Python 版本的 Tesseract库：
</code></pre>

<p>pip install pytesseract<br/>
```</p>

<h1 id="toc_6">简单示例</h1>

<p>目前只能处理规范的文字,那么什么算<code>格式规范</code>呢?<br/>
格式规范的文字具有以下特点:<br/>
+ 使用一个标准字体(不包含手写体、草书,或者十分“花哨的”字体) • 虽然被复印或拍照,字体还是很清晰,没有多余的痕迹或污点<br/>
+ 排列整齐,没有歪歪斜斜的字<br/>
+ 没有超出图片范围,也没有残缺不全,或紧紧贴在图片的边缘</p>

<p>格式规范的图片示例<br/>
<img src="media/15198725905052/test.png" alt="test"/></p>

<h2 id="toc_7">命令行方式</h2>

<p>那么试一试Tesseract,,看看效果如何.用起来也是非常简单.读取图片,然后把结果写入到一个文本文件中<br/>
<code><br/>
hushiwei@localhost  ~/Desktop  tesseract test.png text<br/>
Tesseract Open Source OCR Engine v3.05.01 with Leptonica<br/>
Warning. Invalid resolution 0 dpi. Using 70 instead.<br/>
</code><br/>
接着打开这个文本看看效果<br/>
```<br/>
hushiwei@localhost  ~/Desktop  more text.txt<br/>
This is some text, written in Arial, that will be read by<br/>
Tesseract. Here are some symbols: !@#$%&quot;&amp;&#39;()</p>

<pre><code>
除了一个小符号没有识别出来,其他的字符基本上都识别对了.

## python代码方式进行识别

用之前安装的`pytesseract`模块,就可以很方便的完成我们想要的效果

</code></pre>

<p>import pytesseract</p>

<p>from PIL import Image</p>

<h1 id="toc_8">打开一个图片</h1>

<p>image=Image.open(&#39;test.png&#39;)</p>

<h1 id="toc_9">调用pytesseract的image_to_string方法识别出图片中的文字,返回识别出来的文字</h1>

<p>text=pytesseract.image_to_string(image)</p>

<h1 id="toc_10">打印文字看看效果</h1>

<p>print text<br/>
```</p>

<p>输出结果<br/>
```<br/>
This is some text, written in Arial, that will be read by<br/>
Tesseract. Here are some symbols: !@#$%&quot;&amp;&#39;()</p>

<p>Process finished with exit code 0</p>

<pre><code>

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hbase笔记]]></title>
    <link href="http://dmlcoding.com/15198765095343.html"/>
    <updated>2018-03-01T11:55:09+08:00</updated>
    <id>http://dmlcoding.com/15198765095343.html</id>
    <content type="html"><![CDATA[
<p>在开发hbase过程中,遇到的一些问题.还有些许知识点的总结</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">hbase的内存分配</h1>

<blockquote>
<p>HBase的默认堆分配策略，40%给blockcache，40%给memstore<br/>
在HBase中，有两个在内存中的结构消费了绝大多数的heap空间。BlockCache缓存读操作的HFile block，Memstore缓存近期的写操作。</p>
</blockquote>

<ul>
<li>hfile.block.cache.size(读多的场景下,适当增大这个参数的值)</li>
<li>hbase.regionserver.global.memstore.upperLimit(写多的场景下,适当增大这个参数的值)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[年中总结]]></title>
    <link href="http://dmlcoding.com/15198716087210.html"/>
    <updated>2018-03-01T10:33:28+08:00</updated>
    <id>http://dmlcoding.com/15198716087210.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15198716087210/1.jpg" alt="1"/></p>

<p>写了这么多句,没有写出一句有意思的话;<br/>
写了这么多篇,没有写出一篇有深意的文章;<br/>
不是流水账,仍似流水账;</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">我干了些啥</h1>

<blockquote>
<p>2017已经过去了一半</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[机器学习实战之逻辑回归]]></title>
    <link href="http://dmlcoding.com/15198753668603.html"/>
    <updated>2018-03-01T11:36:06+08:00</updated>
    <id>http://dmlcoding.com/15198753668603.html</id>
    <content type="html"><![CDATA[
<p>不知道什么是逻辑回归,但是也许看完接下来的文章,你会有个大概的印象吧</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">简单介绍Logistic回归</h1>

<h2 id="toc_1">Logistic回归用到的知识点</h2>

<ul>
<li>Sigmoid函数和Logistic回归分类器</li>
<li>最优化理论初步</li>
<li>梯度下降最优化算法</li>
<li>数据中的缺失项处理</li>
</ul>

<h2 id="toc_2">Logistic回归的一般过程</h2>

<ul>
<li>1.收集数据:采用任意方法收集数据</li>
<li>2.准备数据:由于需要进行距离计算,因此要求数据类型为数值型.另外,结构化数据格式则最佳.</li>
<li>3.分析数据:采用任意方法对数据进行分析.</li>
<li>4.训练算法:大部分时间将用于训练,训练的目的是为了找到最佳的分类回归系数.</li>
<li>5.使用算法:首先,我们需要一些输入数据,并将其转换成对应的结构化数值;接着,基于训练好的回归系数就可以对这些数值进行简单的回归计算,判定他们属于哪个类别;在这之后,我们就可以在输出的类别上做一些其他分析工作.</li>
</ul>

<h1 id="toc_3">基于Logistic回归和Sigmoid函数的分类</h1>

<h2 id="toc_4">Logistic回归</h2>

<ul>
<li>优点

<ul>
<li>计算代价不高</li>
<li>易于理解和实现</li>
</ul></li>
<li>缺点

<ul>
<li>容易欠拟合</li>
<li>分类精度可能不高</li>
</ul></li>
<li>适用类型

<ul>
<li>数值型(数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析))</li>
<li>标称型数据(标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类))</li>
</ul></li>
</ul>

<p>基本公式:<br/>
Sigmoid函数具体的计算公式</p>

<math xmlns="http://www.w3.org/1998/Math/MathML">  <mstyle displaystyle="true">    <mi> &#x03C3;<!--greek small letter sigma--> </mi>    <mfenced>      <mrow>        <mi> z </mi>      </mrow>    </mfenced>    <mo> = </mo>    <mfrac>      <mrow>        <mn> 1 </mn>      </mrow>      <mrow>        <mn> 1 </mn>        <mo> + </mo>        <msup>          <mrow>            <mi> e </mi>          </mrow>          <mrow>            <mo> - </mo>            <mi> z </mi>          </mrow>        </msup>      </mrow>    </mfrac>  </mstyle></math>

]]></content>
  </entry>
  
</feed>
