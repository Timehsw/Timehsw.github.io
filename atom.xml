<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Time渐行渐远]]></title>
  <link href="http://dmlcoding.com/atom.xml" rel="self"/>
  <link href="http://dmlcoding.com/"/>
  <updated>2018-03-27T18:12:43+08:00</updated>
  <id>http://dmlcoding.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[概率分布函数和概率密度函数]]></title>
    <link href="http://dmlcoding.com/15221149143615.html"/>
    <updated>2018-03-27T09:41:54+08:00</updated>
    <id>http://dmlcoding.com/15221149143615.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>研究一个随机变量,不只要看它能取哪些值,更重要的是它取各种值的概率如何!</p>
</blockquote>

<ul>
<li>概率分布函数</li>
<li>概率密度函数</li>
</ul>

<h2 id="toc_0">离散型随机变量</h2>

<p>在理解这两个词之前</p>

<ul>
<li>概率分布函数</li>
<li>概率密度函数</li>
</ul>

<p>先理解这两个词</p>

<ul>
<li>概率函数: 就是用函数的形式来表达概率.也叫分布律.</li>
<li>概率分布: 表达随机变量所有可能取值的分布情况.注意:是所有的取值</li>
<li>分布函数: 即,概率分布函数,</li>
</ul>

<p><img src="media/15221149143615/15221166166372.png" alt=""/></p>

<p>概率分布函数: 它是概率函数各个取值的累加结果.也叫累积概率函数</p>

<ul>
<li>分布函数</li>
</ul>

<h2 id="toc_1">连续型随机变量</h2>

<ul>
<li>概率密度函数: 等同于概率函数,why?</li>
</ul>

<p>在连续型随机变量中为啥要将概率函数叫成概率密度函数呢?<br/>
参考陈希孺老师所著的《概率论与数理统计》<br/>
<img src="media/15221149143615/15221159306143.png" alt=""/><br/>
<img src="media/15221149143615/15221159405543.png" alt=""/></p>

<p>概率密度函数用数学公式表示就是一个定积分的函数，定积分在数学中是用来求面积的，而在这里，你就把概率表示为面积即可！<br/>
<img src="media/15221149143615/15221159485806.png" alt=""/></p>

<p>左边是F(x)<code>连续型随机变量分布函数</code>画出的图形，右边是f(x)<code>连续型随机变量的概率密度函数</code>画出的图像，它们之间的关系就是，概率密度函数是分布函数的导函数。</p>

<p>两张图一对比，你就会发现，如果用右图中的面积来表示概率，利用图形就能很清楚的看出，哪些取值的概率更大！这样看起来是不是特别直观，特别爽！！所以，我们在表示连续型随机变量的概率时，用f(x)概率密度函数来表示，是非常好的！</p>

<h1 id="toc_2">参考</h1>

<p><a href="https://www.jianshu.com/p/b570b1ba92bb">应该如何理解概率分布函数和概率密度函数?</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[分布式系统的一致性]]></title>
    <link href="http://dmlcoding.com/15220339795263.html"/>
    <updated>2018-03-26T11:12:59+08:00</updated>
    <id>http://dmlcoding.com/15220339795263.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15220339795263/15221454798314.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[docker 错误]]></title>
    <link href="http://dmlcoding.com/15217143404677.html"/>
    <updated>2018-03-22T18:25:40+08:00</updated>
    <id>http://dmlcoding.com/15217143404677.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">net/http: TLS handshake timeout</h2>

<p>下载镜像的时候出现<code>net/http: TLS handshake timeout</code>,<br/>
原因是docker默认镜像拉取地址为国外仓库下载速度较慢，则会报错“net/http: TLS handshake timeout”。</p>

<span id="more"></span><!-- more -->

<p>此时，只需要将拉取地址改为国内镜像仓库即可。</p>

<p>标准格式为：<br/>
<code><br/>
$ docker pull registry.docker-cn.com/myname/myrepo:mytag<br/>
</code><br/>
例：<br/>
<code><br/>
$ docker pull registry.docker-cn.com/library/ubuntu:16.04<br/>
</code></p>

<p>为了永久性保留更改，您可以修改 /etc/docker/daemon.json 文件并添加上 registry-mirrors 键值。</p>

<pre><code>{
  &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]
}
</code></pre>

<p>修改保存后重启 Docker 以使配置生效。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mac 搭建 区块链开发环境]]></title>
    <link href="http://dmlcoding.com/15216998348495.html"/>
    <updated>2018-03-22T14:23:54+08:00</updated>
    <id>http://dmlcoding.com/15216998348495.html</id>
    <content type="html"><![CDATA[
<ul>
<li><a href="https://docs.docker.com/docker-for-mac/install/">官方安装文档</a></li>
<li><a href="https://docs.docker.com/docker-for-mac/">官方使用文档</a></li>
</ul>

<span id="more"></span><!-- more -->

<h1 id="toc_0">用docker下载ubuntu镜像</h1>

<pre><code># 下载ubuntu镜像
docker pull ubuntu
# 进入镜像中
docker run -it ubuntu /bin/bash
# 查看ubuntu的发行版本
cat /etc/issue

</code></pre>

<p>新开一个终端查看</p>

<pre><code> hushiwei@hsw  ~/Docker  docker images
REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE
ubuntu                latest              f975c5035748        2 weeks ago         112MB
</code></pre>

<h2 id="toc_1">进入镜像后,即生成了一个容器.更新容易里面的系统</h2>

<p>跟新apt-get,安装常用开发工具</p>

<pre><code>apt-get update
apt-get install vim
apt-get install sudo
</code></pre>

<h2 id="toc_2">提高安全意识,添加普通用户</h2>

<pre><code>adduser deploy
su deploy
</code></pre>

<h2 id="toc_3">给普通用户sudo权限</h2>

<pre><code># root下执行
chmod 777 /etc/sudoers
vim /etc/sudoers
</code></pre>

<p>在/etc/sudoers文件里面添加一行,表示给deploy用户sudo权限,并且不需要密码<br/>
<img src="media/15216998348495/15217671764613.jpg" alt=""/></p>

<p>把这个文件的权限改回去</p>

<pre><code>chmod 440 /etc/sudoers
</code></pre>

<h1 id="toc_4">在容器中进行区块链环境的搭建</h1>

<p>注意:目前为止,我们还没退出容器过.</p>

<h2 id="toc_5">安装curl,nodejs</h2>

<pre><code>sudo apt-get install curl

# 安装nodejs的版本
root@a43d121b81a0:~# curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -
# 完成后输出
## Run `apt-get install nodejs` (as root) to install Node.js v8.x LTS Carbon and npm

# 然后安装nodejs
apt-get install nodejs

# 查看版本
node -v

npm -v
</code></pre>

<h2 id="toc_6">安装testrpc,truffle</h2>

<pre><code># 安装淘宝镜像,速度快
npm install -g cnpm --registry=https://registry.npm.taobao.org

sudo cnpm install -g ethereumjs-testrpc
# 完成后输入testrpc,进行校验

sudo cnpm install -g truffle

</code></pre>

<h1 id="toc_7">提交安装好环境的容器</h1>

<p>在开一个窗口,输入<code>docker ps</code> 找到正在运行的容器的<code>CONTAINER ID</code><br/>
刚刚在这个容器里面安装好了环境,那么我们需要保存一下,所以用docker commit提交这个<br/>
容器,成为镜像<br/>
<code><br/>
docker commit a43d121b81a0 testrpc-truffle-env:v1<br/>
</code><br/>
提交后,再查看images,比之前的ubuntu大了不少.毕竟装了那么多工具进去了<br/>
<code><br/>
 hushiwei@hsw  ~/Docker  docker images<br/>
REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE<br/>
testrpc-truffle-env   v1                  a2073b0b64cc        6 minutes ago       503MB<br/>
ubuntu                latest              f975c5035748        2 weeks ago         112MB<br/>
</code></p>

<h2 id="toc_8">退出再进去</h2>

<p>现在就算退出运行中的容器,之前安装的工具也都还在了.<br/>
执行下面的命令即可进入刚刚提交的镜像中.<code>a2073b0b64cc</code>是刚刚提交的镜像的id</p>

<pre><code>docker run -it a2073b0b64cc /bin/bash
</code></pre>

<h1 id="toc_9">安装idea插件进行开发</h1>

<p>在idea的Plugins中安装插件<code>Intellij-Solidity</code></p>

<h1 id="toc_10">docker挂载本地目录进行开发</h1>

<p>将本地的<code>/Users/hushiwei/BlockChain/ethereum</code>目录,与容器中的<code>/home/deploy</code>进行共享<br/>
```<br/>
docker run -it -v /Users/hushiwei/BlockChain/ethereum:/home/deploy a2073b0b64cc /bin/bash</p>

<pre><code>
# 生成truffle代码
在容器的`/home/deploy/demo3`目录下执行`truffle init` 生成项目
![](media/15216998348495/15217712425759.jpg)

本机对应的目录也可以看到
![](media/15216998348495/15217712751901.jpg)

## 用idea打开ethereum目录作为项目.

![](media/15216998348495/15217713264100.jpg)


</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LeetCode解题思路]]></title>
    <link href="http://dmlcoding.com/15216328231070.html"/>
    <updated>2018-03-21T19:47:03+08:00</updated>
    <id>http://dmlcoding.com/15216328231070.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">Easy</h1>

<span id="more"></span><!-- more -->

<h2 id="toc_1"><u>01</u>TwoSum</h2>

<h2 id="toc_2"><u>07</u>ReverseInteger</h2>

<h2 id="toc_3"><u>09</u>PalindromeNumber</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Macbook 区块链开发]]></title>
    <link href="http://dmlcoding.com/15216132281232.html"/>
    <updated>2018-03-21T14:20:28+08:00</updated>
    <id>http://dmlcoding.com/15216132281232.html</id>
    <content type="html"><![CDATA[
<p>基本概念就不说了,网上一大堆.</p>

<h2 id="toc_0">客户端安装</h2>

<pre><code>brew tap ethereum/ethereum

brew install ethereum

</code></pre>

<span id="more"></span><!-- more -->

<p>很是花了一些时间呢<br/>
<code><br/>
  /usr/local/Cellar/ethereum/1.8.2: 9 files, 54.7MB, built in 26 minutes 26 seconds<br/>
</code></p>

<pre><code>
geth -h
geth console

</code></pre>

<pre><code>
 hushiwei@hsw  ~/BlockChain  geth console
INFO [03-21|15:05:35] Maximum peer count                       ETH=25 LES=0 total=25
INFO [03-21|15:05:35] Starting peer-to-peer node               instance=Geth/v1.8.2-stable/darwin-amd64/go1.10
INFO [03-21|15:05:35] Allocated cache and file handles         database=/Users/hushiwei/Library/Ethereum/geth/chaindata cache=768 handles=1024
INFO [03-21|15:05:35] Initialised chain configuration          config=&quot;{ChainID: 1 Homestead: 1150000 DAO: 1920000 DAOSupport: true EIP150: 2463000 EIP155: 2675000 EIP158: 2675000 Byzantium: 4370000 Constantinople: &lt;nil&gt; Engine: ethash}&quot;
INFO [03-21|15:05:35] Disk storage enabled for ethash caches   dir=/Users/hushiwei/Library/Ethereum/geth/ethash count=3
INFO [03-21|15:05:35] Disk storage enabled for ethash DAGs     dir=/Users/hushiwei/.ethash                      count=2
INFO [03-21|15:05:35] Initialising Ethereum protocol           versions=&quot;[63 62]&quot; network=1
INFO [03-21|15:05:35] Loaded most recent local header          number=0 hash=d4e567…cb8fa3 td=17179869184
INFO [03-21|15:05:35] Loaded most recent local full block      number=0 hash=d4e567…cb8fa3 td=17179869184
INFO [03-21|15:05:35] Loaded most recent local fast block      number=0 hash=d4e567…cb8fa3 td=17179869184
INFO [03-21|15:05:35] Loaded local transaction journal         transactions=0 dropped=0
INFO [03-21|15:05:35] Regenerated local transaction journal    transactions=0 accounts=0
INFO [03-21|15:05:35] Starting P2P networking
INFO [03-21|15:05:37] UDP listener up                          self=enode://239405879cbf4f9d8cfb1dafca4bab87b8f7ce27fbdb486347d7215b1de56663805d520d44044bb5ed324a79e41fbdd0be2adbbe5f31684cf528bf2faef4796f@[::]:30303
INFO [03-21|15:05:37] RLPx listener up                         self=enode://239405879cbf4f9d8cfb1dafca4bab87b8f7ce27fbdb486347d7215b1de56663805d520d44044bb5ed324a79e41fbdd0be2adbbe5f31684cf528bf2faef4796f@[::]:30303
INFO [03-21|15:05:37] IPC endpoint opened                      url=/Users/hushiwei/Library/Ethereum/geth.ipc
Welcome to the Geth JavaScript console!

instance: Geth/v1.8.2-stable/darwin-amd64/go1.10
 modules: admin:1.0 debug:1.0 eth:1.0 miner:1.0 net:1.0 personal:1.0 rpc:1.0 txpool:1.0 web3:1.0

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[大数据框架-开源协议]]></title>
    <link href="http://dmlcoding.com/15209079363420.html"/>
    <updated>2018-03-13T10:25:36+08:00</updated>
    <id>http://dmlcoding.com/15209079363420.html</id>
    <content type="html"><![CDATA[
<p>知道我们经常用的开源框架的开源协议么?</p>

<span id="more"></span><!-- more -->

<table>
<thead>
<tr>
<th>开源项目</th>
<th>开源协议</th>
</tr>
</thead>

<tbody>
<tr>
<td>Superset</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Yanagishima</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Kibana</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Spark</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Tensorflow</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>PlanOut</td>
<td>BSD License</td>
</tr>
<tr>
<td>Hadoop</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Hive</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Sqoop</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Impala</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Presto</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Hbase</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Mysql</td>
<td>GPL license</td>
</tr>
<tr>
<td>Elasticsearch</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>CDH</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Zookeeper</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Hue</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Oozie</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Flume</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Kafka</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Logstash</td>
<td>Apache License 2.0</td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zookeeper笔记]]></title>
    <link href="http://dmlcoding.com/15205689785115.html"/>
    <updated>2018-03-09T12:16:18+08:00</updated>
    <id>http://dmlcoding.com/15205689785115.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">基础知识</h2>

<h3 id="toc_1">znode</h3>

<span id="more"></span><!-- more -->

<h3 id="toc_2">基础api</h3>

<pre><code># 创建一个test_node节点,并包含数据testdata
[zk: localhost:2181(CONNECTED) 12] create /test_note testdata
Created /test_note
[zk: localhost:2181(CONNECTED) 13] ls /test_note
[]
[zk: localhost:2181(CONNECTED) 14] get /test_note
testdata
cZxid = 0x1cc0015709f
ctime = Fri Mar 09 12:15:41 CST 2018
mZxid = 0x1cc0015709f
mtime = Fri Mar 09 12:15:41 CST 2018
pZxid = 0x1cc0015709f
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 8
numChildren = 0

[zk: localhost:2181(CONNECTED) 15] delete /test_note
[zk: localhost:2181(CONNECTED) 16] get /test_note
Node does not exist: /test_note

</code></pre>

<h2 id="toc_3">主-从模式的例子</h2>

<pre><code># -e 表示临时性
# 创建一个临时性的znode,节点名称为master_example ,数据为master1.example.com:2223
# 一个临时节点会在会话过期或关闭时自动被删除
create -e /master_example &quot;master1.example.com:2223&quot;

# 列出目录树
ls /

# 获取master_example znode的元数据和数据
get /master_example
</code></pre>

<p>如果再次执行创建master_example的命令会报<code>Node already exists: /master_example</code></p>

<pre><code># 在master_example上设置一个监视点
stat /master_example true
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kafka-sparkstreaming自己维护offset]]></title>
    <link href="http://dmlcoding.com/15203071412162.html"/>
    <updated>2018-03-06T11:32:21+08:00</updated>
    <id>http://dmlcoding.com/15203071412162.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">auto.offset.reset</h2>

<p>By default, it will start consuming from the latest offset of each Kafka partition. If you set configuration auto.offset.reset in Kafka parameters to smallest, then it will start consuming from the smallest offset.</p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">OffsetRange</h2>

<p>topic主题,分区ID,起始offset,结束offset</p>

<h2 id="toc_2">重写思路</h2>

<p>因为spark源码中<code>KafkaCluster</code>类被限制在<strong>[spark]</strong>包下,所以我们如果想要在项目中调用这个类,那么只能在项目中也新建包<code>org.apache.spark.streaming.kafka</code>.然后再该包下面写调用的逻辑.这里面就可以引用<code>KafkaCluster</code>类了.这个类里面封装了很多实用的方法,比如:获取主题和分区,获取offset等等...</p>

<p>这些api,spark里面都有现成的,我们现在就是需要组织起来!</p>

<pre><code>offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
</code></pre>

<p>简单说一下<br/>
1. 在zookeeper上读取offset前先根据实际情况更新<code>fromOffsets</code><br/>
    1.1 如果是一个新的groupid,那么会从最新的开始读<br/>
    1.2 如果是存在的groupid,根据配置<code>auto.offset.reset</code><br/>
        1.2.1 <code>smallest</code> : 那么会从开始读,获取最开始的offset.<br/>
        1.2.2 <code>largest</code> : 那么会从最新的位置开始读取,获取最新的offset.</p>

<ol>
<li>根据topic获取topic和该topics下所有的partitions</li>
</ol>

<pre><code>val partitionsE = kc.getPartitions(topics)
</code></pre>

<ol>
<li>传入上面获取到的topics和该分区所有的partitions</li>
</ol>

<pre><code>val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions)
</code></pre>

<ol>
<li>获取到该topic下所有分区的offset了.最后还是调用spark中封装好了的api</li>
</ol>

<pre><code>KafkaUtils.createDirectStream[K, V, KD, VD, (K, V, String)](
                ssc, kafkaParams, consumerOffsets, (mmd: MessageAndMetadata[K, V]) =&gt; ( mmd.key, mmd.message, mmd.topic))
</code></pre>

<ol>
<li>更新zookeeper中的kafka消息的偏移量</li>
</ol>

<pre><code>kc.setConsumerOffsets(groupId, Map((topicAndPartition, offsets.untilOffset)))
</code></pre>

<h2 id="toc_3">问题</h2>

<p>sparkstreaming-kafka的源码中是自己把offset维护在kafka集群中了?</p>

<pre><code>./kafka-consumer-groups.sh --bootstrap-server 10.10.25.13:9092 --describe  --group heheda
</code></pre>

<p>因为用命令行工具可以查到,这个工具可以查到基于javaapi方式的offset,查不到在zookeeper中的</p>

<p>网上的自己维护offset,是把offset维护在zookeeper中了?<br/>
用这个方式产生的groupid,在命令行工具中查不到,但是也是调用的源码中的方法呢?<br/>
难道spark提供了这个方法,但是自己却没有用是吗?</p>

<h2 id="toc_4">自己维护和用原生的区别</h2>

<p>区别只在于,自己维护offset,会先去zk中获取offset,逻辑处理完成后再更新zk中的offset.<br/>
然而,在代码层面,区别在于调用了不同的<code>KafkaUtils.createDirectStream</code></p>

<h3 id="toc_5">自己维护</h3>

<p>自己维护的offset,这个方法会传入offset.因为在此之前我们已经从zk中获取到了起始offset</p>

<pre><code>KafkaUtils.createDirectStream[K, V, KD, VD, (K, V, String)](
                ssc, kafkaParams, consumerOffsets, (mmd: MessageAndMetadata[K, V]) =&gt; ( mmd.key, mmd.message, mmd.topic))
</code></pre>

<h3 id="toc_6">原生的</h3>

<p>接受的是一个topic,底层会根据topic去获取存储在kafka中的起始offset</p>

<pre><code>KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, myTopic)
</code></pre>

<p>接下来这个方法里面会调用<code>getFromOffsets</code>来获取起始的offset</p>

<pre><code>val kc = new KafkaCluster(kafkaParams)
val fromOffsets = getFromOffsets(kc, kafkaParams, topics)
</code></pre>

<h2 id="toc_7">代码</h2>

<p>这个代码,网上很多,GitHub上也有现成的了.这里我就不贴出来了!<br/>
这里主要还是学习代码的实现思路!</p>

<h2 id="toc_8">如何引用spark源码中限制了包的代码</h2>

<ol>
<li>新建和源码中同等的包名,如上所述.</li>
<li>把你需要的源码拷贝一份出来,但是可能源码里面又引用了别的,这个不一定好使.</li>
<li>在你需要引用的那个类里,把这个类的包名改成与你需要引用的包名一样.最简单的办法了</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[查看linux系统负荷]]></title>
    <link href="http://dmlcoding.com/15202200627560.html"/>
    <updated>2018-03-05T11:21:02+08:00</updated>
    <id>http://dmlcoding.com/15202200627560.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">uptime</h1>

<pre><code>~ uptime
17:54:40 up 846 days, 55 min, 10 users,  load average: 0.36, 0.33, 0.48
</code></pre>

<span id="more"></span><!-- more -->

<ul>
<li>这行信息的后半部部分,显示 **load average.  **它的意思是系统的平均负荷.</li>
<li>通过里面的三个数字,我们可以从中判断系统负荷是大还是小</li>
<li>这3个数字,分别代表1分钟,5分钟,15分钟内系统的平均负荷.</li>
<li>当CPU完全空闲的时候,平均负荷为0;当CPU工作量饱和的时候,平均负荷为1(假设只有一个核)</li>
<li>那么很显然,load average的值越低,比如0.2或者0.3,就说明电脑的工作量越小,系统负荷比较轻</li>
</ul>

<p>参考文档</p>

<p><a href="http://blog.scoutapp.com/articles/2009/07/31/understanding-load-averages">http://blog.scoutapp.com/articles/2009/07/31/understanding-load-averages</a></p>

<p>总结:</p>

<p>假设服务器有16个核.</p>

<p>那么当load average的值小于16的时候,表示现在系统负荷轻.当load average的值大于16就表示负荷过大了.</p>

<p>查看服务器有多少个CPU核心</p>

<p><u><strong>grep -c &#39;model name&#39; /proc/cpuinfo</strong></u></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MacbookPro 修改应用的默认语言]]></title>
    <link href="http://dmlcoding.com/15202199373374.html"/>
    <updated>2018-03-05T11:18:57+08:00</updated>
    <id>http://dmlcoding.com/15202199373374.html</id>
    <content type="html"><![CDATA[
<p>当我把macbook pro的默认系统语言从中文改成英文后,其他的应用的语言也都变成了英文了.</p>

<p>但是比如地图应用,我还是希望用中文呢?</p>

<h1 id="toc_0">将地图Maps应用的语言改成中文</h1>

<span id="more"></span><!-- more -->

<p>第一步</p>

<pre><code>You don&#39;t need to change language setting. 
You can simply set it in the Maps App&#39;s Menu at [View]-&gt; [Labels]-&gt; [Always Show Labels in English].
 Then cancel it.
</code></pre>

<p>第二步</p>

<pre><code>defaults write com.apple.Maps AppleLanguages &#39;(zh-CN)&#39;
</code></pre>

<hr/>

<pre><code># 英文 -&gt; 简体中文
defaults write com.google.Chrome AppleLanguages &#39;(zh-CN)&#39;

# 简体中文 -&gt; 英文
defaults write com.google.Chrome AppleLanguages &#39;(en-US)&#39;

# 英文优先，简体中文第二。反之改一下顺序
defaults write com.google.Chrome AppleLanguages &quot;(en-US,zh-CN)&quot;
</code></pre>

<h1 id="toc_1">Calendar</h1>

<pre><code>defaults write com.apple.iCal AppleLanguages &#39;(zh-CN)&#39;
</code></pre>

<p>那么问题来了,从上面可以知道只要修改每个应用的系统语言即可</p>

<h1 id="toc_2">如何找到应用的包名</h1>

<p>以<strong>滴答清单</strong>为例</p>

<pre><code># 在&quot;&quot;双引号中输入应用名称,执行下面的命令即可返回应用的包名
hushiwei@hsw ~  osascript -e &#39;id of app &quot;Wunderlist&quot;&#39;
com.wunderkinder.wunderlistdesktop
</code></pre>

<p>知道了应用的包名,那么就可以执行上面的命令来修改应用的语言了</p>

<pre><code>defaults write com.wunderkinder.wunderlistdesktop AppleLanguages &#39;(zh-CN)&#39;
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[思考]]></title>
    <link href="http://dmlcoding.com/15199545224004.html"/>
    <updated>2018-03-02T09:35:22+08:00</updated>
    <id>http://dmlcoding.com/15199545224004.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">为什么有些人能够突飞猛进?</h2>

<ul>
<li>智商高</li>
<li>有想法,有思路</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kafka-sparkstreaming长时间运行后异常退出原因分析]]></title>
    <link href="http://dmlcoding.com/15199040835190.html"/>
    <updated>2018-03-01T19:34:43+08:00</updated>
    <id>http://dmlcoding.com/15199040835190.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">报错信息</h2>

<pre><code>Couldn`t find leader offsets for set([xxxxxxxxxxx,xx])

# 或者
numRecords must not be negative
</code></pre>

<span id="more"></span><!-- more -->

<h2 id="toc_1">分析过程</h2>

<p>从博客中已经找不到直接的解决方案，那么只能从源码入手，从而看看能不能定位到问题所在。<br/>
从这一行代码开始跟踪</p>

<pre><code>KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, myTopic)

</code></pre>

<p>发现了,先调用<code>getFromOffsets(kc, kafkaParams, topics)</code>获取到了起始offset.然后创建了一个<br/>
<code>DirectKafkaInputDStream</code>对象.点进去</p>

<pre><code>  def createDirectStream[
    K: ClassTag,
    V: ClassTag,
    KD &lt;: Decoder[K]: ClassTag,
    VD &lt;: Decoder[V]: ClassTag] (
      ssc: StreamingContext,
      kafkaParams: Map[String, String],
      topics: Set[String]
  ): InputDStream[(K, V)] = {
    val messageHandler = (mmd: MessageAndMetadata[K, V]) =&gt; (mmd.key, mmd.message)
    val kc = new KafkaCluster(kafkaParams)
    val fromOffsets = getFromOffsets(kc, kafkaParams, topics)
    new DirectKafkaInputDStream[K, V, KD, VD, (K, V)](
      ssc, kafkaParams, fromOffsets, messageHandler)
  }

</code></pre>

<p>找到这个对象里面的<code>compute</code>方法,重点从这个方法的逻辑开始分析</p>

<pre><code>  override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {
    val untilOffsets = clamp(latestLeaderOffsets(maxRetries))
    val rdd = KafkaRDD[K, V, U, T, R](
      context.sparkContext, kafkaParams, currentOffsets, untilOffsets, messageHandler)

    // Report the record number and metadata of this batch interval to InputInfoTracker.
    val offsetRanges = currentOffsets.map { case (tp, fo) =&gt;
      val uo = untilOffsets(tp)
      OffsetRange(tp.topic, tp.partition, fo, uo.offset)
    }
    val description = offsetRanges.filter { offsetRange =&gt;
      // Don&#39;t display empty ranges.
      offsetRange.fromOffset != offsetRange.untilOffset
    }.map { offsetRange =&gt;
      s&quot;topic: ${offsetRange.topic}\tpartition: ${offsetRange.partition}\t&quot; +
        s&quot;offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}&quot;
    }.mkString(&quot;\n&quot;)
    // Copy offsetRanges to immutable.List to prevent from being modified by the user
    val metadata = Map(
      &quot;offsets&quot; -&gt; offsetRanges.toList,
      StreamInputInfo.METADATA_KEY_DESCRIPTION -&gt; description)
    val inputInfo = StreamInputInfo(id, rdd.count, metadata)
    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)

    currentOffsets = untilOffsets.map(kv =&gt; kv._1 -&gt; kv._2.offset)
    Some(rdd)
  }
</code></pre>

<p>分析过程不仔细说了,感兴趣的在compute方法处打个断点,跟踪一下相关变量的值,应该就能明白一个大概了.</p>

<h2 id="toc_2">解决办法</h2>

<p>综上所述,解决办法为添加以下参数:</p>

<pre><code>--conf spark.streaming.kafka.maxRetries=50 \
--conf spark.yarn.maxAppAttempts=10 \
--conf spark.yarn.am.attemptFailuresValidityInterval=1h \
</code></pre>

<p>注意:<br/>
1. spark.yarn.maxAppAttempts参数不能超过hadoop集群yarn.resourcemanager.am.max-attempts的最大值，如果超过将会取两者较小值<br/>
2. 由于kafka切换leader导致的数据丢失,然后造成sparkstreaming程序报错,异常退出的情况下.我这通通常都是重启kafka,删除checkpoint,再重启sparkstreaming了.<br/>
3. 后面我们不用原生的方式来存储offset.而是自己维护offset,那么以后更新迭代后就不用再删除checkpoint了,对业务的影响也最小了.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch：用Curator辅助Marvel，实现自动删除旧marvel索引]]></title>
    <link href="http://dmlcoding.com/15198768495613.html"/>
    <updated>2018-03-01T12:00:49+08:00</updated>
    <id>http://dmlcoding.com/15198768495613.html</id>
    <content type="html"><![CDATA[
<p>Marvel几乎是所有Elasticsearch用户的标配。Marvel保留观测数据的代价是，<br/>
它默认每天会新建一个index，命名规律像是这样：.marvel-2017-12-10。<br/>
marvel自建的索引一天可以产生大概500M的数据，而且将会越来越多，占的容量也将越来越大。<br/>
有没有什么办法能让它自动过期？比如说只保留最近两天的观测数据，其他的都抛弃掉。</p>

<p>当然有办法,curator就可以帮你实现.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">curator是什么？</h1>

<p>它是一个命令，可以帮助你管理你在Elasticsearch中的索引，帮你删除，关闭（close），<br/>
打开（open）它们。当然这是比较片面的说法，更完整的说明见：<br/>
<a href="https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html">https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html</a></p>

<h1 id="toc_1">实践</h1>

<p>我们集群里面安装的Elasticsearch的版本是2.1.1.<br/>
按照官网,我装了最新的5.x版本,显示版本不对.<br/>
按照 <a href="http://blog.csdn.net/hereiskxm/article/details/47423715">http://blog.csdn.net/hereiskxm/article/details/47423715</a>  这个博客,我装了3.3.0版本.<br/>
显示也不对.</p>

<p>然后我搜了一下,感觉应该装一个中间的版本,因此我安装了4.0.0版本<br/>
<code><br/>
pip install elasticsearch-curator (4.0.0)<br/>
</code></p>

<p>然后我看了一下这个版本提供的参数</p>

<pre><code> curator --help
Usage: curator [OPTIONS] ACTION_FILE

  Curator for Elasticsearch indices.

  See http://elastic.co/guide/en/elasticsearch/client/curator/current

Options:
  --config PATH  Path to configuration file. Default: ~/.curator/curator.yml
  --dry-run      Do not perform any changes.
  --version      Show the version and exit.
  --help         Show this message and exit.
</code></pre>

<p>和我安装最新的5.X的版本看起来是一致的.正好在这个站点看到配置的办法<br/>
<a href="https://stackoverflow.com/questions/33430055/removing-old-indices-in-elasticsearch/42268400#42268400">https://stackoverflow.com/questions/33430055/removing-old-indices-in-elasticsearch/42268400#42268400</a></p>

<p>之前在博客里面看到的那个3.3.0版本,还不兼容呢.</p>

<h2 id="toc_2">用法</h2>

<blockquote>
<p>目的是删除2天前以.marvel开头的索引</p>
</blockquote>

<p>新建目录 /opt/curator</p>

<pre><code>~ pwd
/opt/curator
~ ll
total 12
-rw-r--r-- 1 root root  184 Dec 12 10:48 config_file.yml
-rw-r--r-- 1 root root 1311 Dec 12 10:37 delete_marvel_indices.yml
drwxr-xr-x 2 root root 4096 Dec 12 10:49 logs
</code></pre>

<h2 id="toc_3">config_file.yml</h2>

<pre><code># 记住,这个logfile得提前新建好.不然会启动报错.
vim config_file.yml

---
client:
  hosts:
   - 10.10.25.217
  port: 9200
logging:
  loglevel: INFO
  logfile: &quot;/opt/curator/logs/actions.log&quot;
  logformat: default
  blacklist: [&#39;elasticsearch&#39;, &#39;urllib3&#39;]

</code></pre>

<h2 id="toc_4">delete_marvel_indices.yml</h2>

<p>删除以.marvel前缀且是2天之前的索引</p>

<h2 id="toc_5">```</h2>

<h1 id="toc_6">Remember, leave a key empty if there is no value.  None will be a string,</h1>

<h1 id="toc_7">not a Python &quot;NoneType&quot;</h1>

<h1 id="toc_8">Also remember that all examples have &#39;disable_action&#39; set to True.  If you</h1>

<h1 id="toc_9">want to use this action as a template, be sure to set this to False after</h1>

<h1 id="toc_10">copying it.</h1>

<p>actions:<br/>
  1:<br/>
    action: delete_indices<br/>
    description: &gt;-<br/>
      Delete indices older than 30 days (based on index name), for rc- prefixed indices.<br/>
    options:<br/>
      ignore_empty_list: True<br/>
      timeout_override:<br/>
      continue_if_exception: False<br/>
      disable_action: False<br/>
    filters:<br/>
    - filtertype: pattern<br/>
      kind: prefix<br/>
      value: rc-<br/>
      exclude:<br/>
    - filtertype: age<br/>
      source: name<br/>
      direction: older<br/>
      timestring: &#39;%Y.%m.%d&#39;<br/>
      unit: days<br/>
      unit_count: 30<br/>
      exclude:<br/>
  2:<br/>
    action: delete_indices<br/>
    description: &gt;-</p>

<pre><code>  Delete indices older than 2 days (based on index name), for .marvel prefixed indices.
options:
  ignore_empty_list: True
  timeout_override:
  continue_if_exception: False
  disable_action: False
filters:
- filtertype: pattern
  kind: prefix
  value: .marvel
  exclude:
- filtertype: age
  source: name
  direction: older
  timestring: &#39;%Y.%m.%d&#39;
  unit: days
  unit_count: 2
  exclude:
</code></pre>

<pre><code>
配置完成.

## 执行命令

</code></pre>

<p>curator --config config_file.yml [--dry-run] delete_marvel_indices.yml<br/>
```</p>

<p>注意:<br/>
1. --dry-run 是可选参数,加上后不会真的删除,只会执行逻辑.你可以通过看日志来判断是否正确.<br/>
确认正确后,去掉--dry-run参数,再执行命令,既是真正的执行删除了.<br/>
2. 如果没有在config_file.yml里面配置logfile参数,那么日志会在console打印出来.</p>

<h1 id="toc_11">配置日常任务</h1>

<p>很明显,我们需要自动化这个过程,让它每天自动执行,因此写一个脚本,让crontab每天自动调用即可</p>

<pre><code>#!/bin/bash

curator --config /opt/curator/config_file.yml  /opt/curator/delete_marvel_indices.yml

echo &quot;delete success&quot;

</code></pre>

<p>配置crontab</p>

<pre><code># 每天2点执行删除脚本
0 2 * * * source /etc/profile;bash /opt/curator/delete_marvel_daily.sh &gt; /opt/curator/delete.log 2&gt;&amp;1

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[机器学习实战之朴素贝叶斯]]></title>
    <link href="http://dmlcoding.com/15198746079468.html"/>
    <updated>2018-03-01T11:23:27+08:00</updated>
    <id>http://dmlcoding.com/15198746079468.html</id>
    <content type="html"><![CDATA[
<p>朴素贝叶斯就是利用先验知识来解决后验概率，因为训练集中我们已经知道了每个单词在类别0和1中的概率，即p(w|c),<br/>
我们就是要利用这个知识去解决在出现这些单词的组合情况下，类别更可能是0还是1,即p(c|w)。<br/>
如果说之前的训练样本少，那么这个p(w|c)就更可能不准确，所以样本越多我们会觉得这个p(w|c)越可信。</p>

<span id="more"></span><!-- more -->

<pre><code class="language-python">import os
import sys
from numpy import *
sys.path.append(os.getcwd())
</code></pre>

<pre><code class="language-python">import bayes
</code></pre>

<pre><code class="language-python"># 返回实验样本和类别标签(侮辱类和非侮辱类)
listOPosts,listClasses=bayes.loadDataSet()
</code></pre>

<pre><code class="language-python"># 创建词汇表
# 将实验样本里面的词汇进行去重
def createVocabList(dataSet):
    vocabSet=set([])
    for document in dataSet:
        vocabSet=vocabSet|set(document)
    return list(vocabSet)
</code></pre>

<pre><code class="language-python">myVocabList=createVocabList(listOPosts)
</code></pre>

<pre><code class="language-python"># 将词汇转成特征向量
# 也就是将每一行样本转成特征向量
# 向量的每一元素为1或者0,分别表示词汇表中的单词在输入文档中是否出现
def setOfWordsVec(vocabList,inputSet):
    returnVec=[0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)]=1
        else:print &quot;the word:%s is not in my Vocabulary!&quot; % word
    return returnVec
</code></pre>

<h1 id="toc_0">训练算法:从词向量计算概率</h1>

<p>朴素贝叶斯分类器训练函数</p>

<pre><code class="language-python"># 输入为文档矩阵以及由每篇文档类别标签所构成的向量
def trainNB0(trainMatrix,trainCategory):
    # 文档总数,有几篇文档,在这里也就是有几个一维数组
    numTrainDocs=len(trainMatrix)
    # 每篇文档里面的单词数,也就是一维数组的长度
    numWords=len(trainMatrix[0])
    # 因为就只有0和1两个分类,将类别列表求和后,就是其中一个类别的个数
    # 然后numTrainDocs也就是文档总数,这样相除后就是这个类别的概率了
    pAbusive=sum(trainCategory)/float(numTrainDocs)
    # 以下两行,初始化概率
    p0Num=ones(numWords);p1Num=ones(numWords)
    p0Denom=2.0;p1Denom=2.0

    # 依次遍历所有的文档
    for i in range(numTrainDocs):
        # 判断这个文档所属类别
        if trainCategory[i]==1:
            # 数组与数组相加,这里就是统计每个词在这个分类里面出现的次数
            p1Num+=trainMatrix[i]
            # 统计该类别下,这些词语一共出现了多少次
            p1Denom+=sum(trainMatrix[i])
        else:
            p0Num+=trainMatrix[i]
            p0Denom+=sum(trainMatrix[i])
    # 通过求对数避免数据下溢出
    p1Vect=log(p1Num/p1Denom)
    p0Vect=log(p0Num/p0Denom)
    return p0Vect,p1Vect,pAbusive
</code></pre>

<pre><code class="language-python"># 所有文档的特征向量
trainMat=[]
</code></pre>

<pre><code class="language-python"># 将文档的每一行,转成词向量,然后追加到trainMat中
for postinDoc in listOPosts:
    trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc))
</code></pre>

<pre><code class="language-python">p0V,p1V,pAb=trainNB0(trainMat,listClasses)
</code></pre>

<pre><code class="language-python">pAb
</code></pre>

<pre><code>0.5
</code></pre>

<pre><code class="language-python">p0V
</code></pre>

<pre><code>array([-2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,
       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -2.56494936,
       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,
       -2.15948425, -3.25809654, -3.25809654, -2.56494936, -3.25809654,
       -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,
       -2.56494936, -3.25809654, -2.56494936, -3.25809654, -2.56494936,
       -2.56494936, -1.87180218])
</code></pre>

<pre><code class="language-python">p1V
</code></pre>

<pre><code>array([-3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -2.35137526, -2.35137526, -2.35137526, -3.04452244, -1.94591015,
       -3.04452244, -2.35137526, -2.35137526, -3.04452244, -1.94591015,
       -3.04452244, -1.65822808, -3.04452244, -2.35137526, -3.04452244,
       -3.04452244, -3.04452244])
</code></pre>

<pre><code class="language-python"># 朴素贝叶斯分类函数
def classifyNB(vec2classify,p0Vec,p1Vec,pClass1):
    #元素相乘
    p1=sum(vec2classify*p1Vec)+log(pClass1)
    p0=sum(vec2classify*p0Vec)+log(pClass1)
    if p1&gt;p0:
        return 1
    else:
        return 0
</code></pre>

<pre><code class="language-python">def testingNB():
    listOPosts,listClasses=bayes.loadDataSet()
    myVocabList=createVocabList(listOPosts)
    trainMat=[]
    for postinDoc in listOPosts:
        trainMat.append(setOfWordsVec(myVocabList,postinDoc))

    p0V,p1V,pAb=trainNB0(array(trainMat),array(listClasses))

    testEntry=[&#39;love&#39;,&#39;my&#39;,&#39;dalmation&#39;]
    thisDoc=array(setOfWordsVec(myVocabList,testEntry))
    print testEntry,&#39;classified as : &#39;,classifyNB(thisDoc,p0V,p1V,pAb)

    testEntry=[&#39;stupid&#39;,&#39;garbage&#39;]
    thisDoc=array(setOfWordsVec(myVocabList,testEntry))
    print testEntry,&#39;classified as : &#39;,classifyNB(thisDoc,p0V,p1V,pAb)
</code></pre>

<pre><code class="language-python">testingNB()
</code></pre>

<pre><code>[&#39;love&#39;, &#39;my&#39;, &#39;dalmation&#39;] classified as :  0
[&#39;stupid&#39;, &#39;garbage&#39;] classified as :  1
</code></pre>

<h1 id="toc_1">使用朴素贝叶斯过滤垃圾邮件</h1>

<ul>
<li>收集数据：提供文本文件</li>
<li>准备数据：将文本文件解析成词条向量</li>
<li>分析数据：检查词条确保解析的正确性</li>
<li>训练算法：使用我们之前建立的trainNB0()函数</li>
<li>测试算法：使用classifyNB()</li>
</ul>

<pre><code class="language-python">mySent=&#39;This book is the best book on Python or M.L. I have ever laid eyes upon.&#39;
</code></pre>

<pre><code class="language-python"># 文件解析及完整的垃圾邮件测试函数
def textParse(bigString):
    import re
    listOfTokens=re.split(r&#39;\W*&#39;,bigString)
    return [tok.lower() for tok in listOfTokens if len(tok)&gt;2]
</code></pre>

<pre><code class="language-python">def spamTest():
    docList=[];classList=[];fullText=[]
    for i in range(1,26):
        # 导入邮件文本，并解析成词条
        wordList=textParse(open(&#39;email/spam/%d.txt&#39; %i).read())
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(1)

        wordList=textParse(open(&#39;email/ham/%d.txt&#39; %i).read())
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(0)
    # 生成词汇表
    vocabList=createVocabList(docList)

    # 随机构建训练集、测试集
    trainingSet=range(50);testSet=[]
    for i in range(10):
        randIndex=int(random.uniform(0,len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])

    # 生成测试集的特征向量
    trainMat=[];trainClasses=[]
    for docIndex in trainingSet:
        trainMat.append(setOfWordsVec(vocabList,docList[docIndex]))
        trainClasses.append(classList[docIndex])

    p0V,p1V,pSpam=trainNB0(trainMat,trainClasses)

    # 测试集，测试错误率
    errorCount=0
    for docIndex in testSet:
        wordVector=setOfWordsVec(vocabList,docList[docIndex])
        if classifyNB(wordVector,p0V,p1V,pSpam)!=classList[docIndex]:
            errorCount+=1
    print &#39;the error rate is : &#39;,float(errorCount)/len(testSet)
</code></pre>

<pre><code class="language-python">spamTest()
</code></pre>

<pre><code>the error rate is :  0.2
</code></pre>

<h1 id="toc_2">使用朴素贝叶斯分类器从个人广告中获取区域倾向</h1>

<ul>
<li>收集数据:从RSS源收集内容,这里需要对RSS源构建一个接口</li>
<li>准备数据:将文本文件解析成词条向量</li>
<li>分析数据:检查词条确保解析的正确性</li>
<li>训练算法:使用我们之前建立的trainNB0()函数</li>
<li>测试算法:观察错误率,确保分类器可用.可以修改切分程序,以降低错误率,提高分类结果.</li>
<li>使用算法:构建一个完整的程序,封装所有内容.给定两个RSS源,该程序会显示最常用的公共词.</li>
</ul>

<p>下面将使用来自不同城市的广告训练一个分类器，然后观察分类器的效果。我们的目的并不是使用该分类器进行分类，而是通过观察单词和条件概率值来发现与特定城市相关的内容。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Presto的学习笔记]]></title>
    <link href="http://dmlcoding.com/15198745641385.html"/>
    <updated>2018-03-01T11:22:44+08:00</updated>
    <id>http://dmlcoding.com/15198745641385.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">是什么?可以做什么?</h1>

<ol>
<li>Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节。</li>
<li>Presto支持在线数据查询，包括Hive, Cassandra, 关系数据库以及专有数据存储。 一条Presto查询可以将多个数据源的数据进行合并，可以跨越整个组织进行分析。</li>
<li>作为Hive和Pig（Hive和Pig都是通过MapReduce的管道流来完成HDFS数据的查询）的替代者，Presto不仅可以访问HDFS，也可以操作不同的数据源，包括：RDBMS和其他的数据源（例如：Cassandra）。</li>
<li>查询后的数据自动分页,这个很不错.</li>
</ol>

<span id="more"></span><!-- more -->

<h1 id="toc_1">源码编译</h1>

<p>下载<strong>presto</strong>源码包地址:<a href="https://github.com/prestodb/presto/releases">https://github.com/prestodb/presto/releases</a></p>

<p>安装文档地址(注意这个中文文档的版本是0.100):</p>

<p>注意:</p>

<ul>
<li>jdk得是1.8以上</li>
<li>我是用的presto0.161</li>
</ul>

<pre><code>tar -xzvf presto-0.161.tar.gz

# 编译
./mvnw clean install -DskipTests
</code></pre>

<p>在pom.xml文件中加入阿里云的仓库,加速下载依赖</p>

<pre><code>&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;nexus-aliyun&lt;/id&gt;
        &lt;name&gt;Nexus aliyun&lt;/name&gt;
        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
</code></pre>

<p>编译报错</p>

<pre><code>[ERROR] Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.13:revision (default) on project presto-spi: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml -&gt; [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.13:revision (default) on project presto-spi: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:185)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:181)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.maven.plugin.MojoExecutionException: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml
    at pl.project13.maven.git.GitCommitIdMojo.throwWhenRequiredDirectoryNotFound(GitCommitIdMojo.java:432)
    at pl.project13.maven.git.GitCommitIdMojo.execute(GitCommitIdMojo.java:337)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
</code></pre>

<p>解决办法</p>

<pre><code>pom文件中加入这个插件
&lt;pluginManagement&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;pl.project13.maven&lt;/groupId&gt;
                &lt;artifactId&gt;git-commit-id-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;skip&gt;true&lt;/skip&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/pluginManagement&gt;
</code></pre>

<h1 id="toc_2">部署包安装</h1>

<h2 id="toc_3">下载地址</h2>

<p>下载presto-cli(记住要下载的presto-cli-xxxx-executable.jar):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli">https://repo1.maven.org/maven2/com/facebook/presto/presto-cli</a></p>

<p>下载部署包的地址:<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-server/">https://repo1.maven.org/maven2/com/facebook/presto/presto-server/</a></p>

<h2 id="toc_4">服务器说明</h2>

<p>机器共有三台U006,U007,U008</p>

<ul>
<li>coordinator

<ul>
<li>U007</li>
</ul></li>
<li>discovery

<ul>
<li>U007</li>
</ul></li>
<li>worker

<ul>
<li>U006</li>
<li>U008</li>
</ul></li>
</ul>

<p>coordinator和worker的其他配置都是一样的,除了config.properties不一样.具体哪里不一样,看下面的配置说明.</p>

<h1 id="toc_5">配置说明</h1>

<p>以下配置均是presto0.161版本的.如果你的版本不一样,启动后如果报错了,那么可能是配置文件里面的参数和版本对应不上,请找相应版本的配置.</p>

<p>在presto-server-0.161目录下新建etc目录,下面的配置文件均在此etc目录下</p>

<pre><code>[druid@U007 presto-server-0.161]$ tree etc/
etc/
├── catalog
│   ├── hive.properties
│   └── jmx.properties
├── config.properties
├── jvm.config
├── log.properties
└── node.properties
</code></pre>

<h2 id="toc_6">jvm.config</h2>

<blockquote>
<p>包含一系列在启动JVM的时候需要使用的命令行选项。这份配置文件的格式是：一系列的选项，每行配置一个单独的选项。由于这些选项不在shell命令中使用。 因此即使将每个选项通过空格或者其他的分隔符分开，java程序也不会将这些选项分开，而是作为一个命令行选项处理,信息如下：</p>
</blockquote>

<p>VM 系统属性 <code>HADOOP_USER_NAME</code> 来指定用户名</p>

<pre><code>-server
-Xmx16G
-XX:+UseG1GC
-XX:G1HeapRegionSize=32M
-XX:+UseGCOverheadLimit
-XX:+ExplicitGCInvokesConcurrent
-XX:+HeapDumpOnOutOfMemoryError
-XX:OnOutOfMemoryError=kill -9 %p
-DHADOOP_USER_NAME=hdfs
</code></pre>

<h2 id="toc_7">log.properties</h2>

<blockquote>
<p>这个配置文件中允许你根据不同的日志结构设置不同的日志级别。每个logger都有一个名字（通常是使用logger的类的全标示类名）. Loggers通过名字中的“.“来表示层级和集成关系，信息如下：</p>
</blockquote>

<pre><code>com.facebook.presto=INFO
</code></pre>

<ul>
<li>配置日志等级，类似于log4j。四个等级：DEBUG,INFO,WARN,ERROR</li>
</ul>

<h2 id="toc_8">node.properties</h2>

<blockquote>
<p>包含针对于每个节点的特定的配置信息。 一个节点就是在一台机器上安装的Presto实例，<strong>etc/node.properties</strong>配置文件至少包含如下配置信息</p>
</blockquote>

<pre><code>node.environment=production
node.id=ffffffff-ffff-ffff-ffff-ffffffffffff # 每个节点的node.id一定要不一样
node.data-dir=/home/druid/data/presto # 计算临时存储目录,presto得有读写权限
</code></pre>

<p>说明:</p>

<ol>
<li>node.environment： 集群名称, 所有在同一个集群中的Presto节点必须拥有相同的集群名称.</li>
<li>node.id： 每个Presto节点的唯一标示。每个节点的node.id都必须是唯一的。在Presto进行重启或者升级过程中每个节点的node.id必须保持不变。如果在一个节点上安装多个Presto实例（例如：在同一台机器上安装多个Presto节点），那么每个Presto节点必须拥有唯一的node.id.</li>
<li>node.data-dir： 数据存储目录的位置（操作系统上的路径）, Presto将会把日期和数据存储在这个目录下</li>
</ol>

<h2 id="toc_9">config.properties</h2>

<h3 id="toc_10">coordinator 主节点配置</h3>

<pre><code>coordinator=true
node-scheduler.include-coordinator=false
http-server.http.port=8585
query.max-memory=10GB
discovery-server.enabled=true
discovery.uri=http://U007:8585
</code></pre>

<p>说明:</p>

<ol>
<li><p>coordinator表示此节点是否作为一个coordinator。每个节点可以是一个worker，也可以同时是一个coordinator，但作为性能考虑，一般大型机群最好将两者分开。</p></li>
<li><p>若coordinator设置成true，则此节点成为一个coordinator。</p></li>
<li><p>若node-scheduler.include-coordinator设置成true，则成为一个worker，两者可以同时设置成true，此节点拥有两种身份。在一个节点上的Presto server即作为coordinator又作为worke将会降低查询性能。因为如果一个服务器作为worker使用，那么大部分的资源都会被worker占用，那么就不会有足够的资源进行关键任务调度、管理和监控查询执行.</p></li>
<li><p>http-server.http.port：指定HTTP server的端口。Presto 使用 HTTP进行内部和外部的所有通讯.</p></li>
<li><p>query.max-memory=10GB：一个单独的任务使用的最大内存 (一个查询计划的某个执行部分会在一个特定的节点上执行)。 这个配置参数限制的GROUP BY语句中的Group的数目、JOIN关联中的右关联表的大小、ORDER BY语句中的行数和一个窗口函数中处理的行数。 该参数应该根据并发查询的数量和查询的复杂度进行调整。如果该参数设置的太低，很多查询将不能执行；但是如果设置的太高将会导致JVM把内存耗光.</p></li>
<li><p>discovery-server.enabled：Presto 通过Discovery 服务来找到集群中所有的节点。为了能够找到集群中所有的节点，每一个Presto实例都会在启动的时候将自己注册到discovery服务。Presto为了简化部署，并且也不想再增加一个新的服务进程，Presto coordinator 可以运行一个内嵌在coordinator 里面的Discovery 服务。这个内嵌的Discovery 服务和Presto共享HTTP server并且使用同样的端口.</p></li>
<li><p>discovery.uri：Discovery server的URI。由于启用了Presto coordinator内嵌的Discovery 服务，因此这个uri就是Presto coordinator的uri。注意：这个URI一定不能以“/“结尾</p></li>
</ol>

<h3 id="toc_11">worker节点配置</h3>

<pre><code>coordinator=false
node-scheduler.include-coordinator=true
http-server.http.port=8585
query.max-memory=5GB
query.max-memory-per-node=1GB
discovery.uri=http://U007:8585
</code></pre>

<h2 id="toc_12">catalog</h2>

<p>hive.properties(hive连接器的配置)</p>

<p>连接hive</p>

<pre><code># 在etc/catalog目录下,新建hive.properties文件,配置上hive的一些信息
[druid@U006 catalog]$ pwd
/home/druid/presto-server-0.161/etc/catalog
[druid@U006 catalog]$ more hive.properties
connector.name=hive-cdh5
hive.metastore.uri=thrift://U006:9083
hive.config.resources=/etc/hadoop/conf.cloudera.yarn/core-site.xml,/etc/hadoop/conf.clouder
a.yarn/hdfs-site.xml
</code></pre>

<p>保证每个节点presto对core-site.xml,hdfs-site.xml两个文件有读权限</p>

<h1 id="toc_13">启动停止presto</h1>

<h2 id="toc_14">单节点启动</h2>

<p>在每个节点依次执行启动脚本</p>

<pre><code># 后台运行
bin/launcher start
# 前台运行
bin/launcher run

# 重启presto
bin/launcher restart

# 停止presto
bin/launcher stop
</code></pre>

<h2 id="toc_15">批量启动停止脚本</h2>

<pre><code>ssh -t ${i} -C &#39;. /usr/local/bin/env.sh &amp;&amp; /usr/local/presto-server-0.161/bin/launcher restart&#39;
</code></pre>

<h1 id="toc_16">监控presto</h1>

<p>启动完成后,在浏览器输入:</p>

<pre><code>http://U007:8585
</code></pre>

<p>这个地址也就是coordinator的discovery.uri</p>

<h1 id="toc_17">cli连接</h1>

<h2 id="toc_18">连接器注意说明</h2>

<ul>
<li><p>cli下载地址(找到自己的版本下载):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli">https://repo1.maven.org/maven2/com/facebook/presto/presto-cli</a></p></li>
<li><p>连接器的配置文件必须是以<code>.properties</code>后缀结尾的,前面的名字就是连接器的catalog名字</p></li>
<li><p>每次新加连接器配置文件后,都需要在presto的所有机器上加上相同的配置文件,然后重启</p></li>
<li><p>要下载对应版本的cli连接器,不然可能不好使.名字类似<code>presto-cli-0.161-executable.jar</code></p></li>
</ul>

<h2 id="toc_19">hive连接器</h2>

<p>配置说明(hive连接器的配置在说catalog的时候已经配置好了,你可以回头看看):</p>

<ul>
<li>connector.name=hive-cdh5(根据你的hive版本来选择)</li>
<li>hive.metastore.uri=thrift://U006:9083(hive的metastore地址)</li>
<li>hive.config.resources=/etc/hadoop/conf.cloudera.yarn/core-(配置文件的地址)</li>
</ul>

<pre><code>chmod +x presto-cli-0.161-executable.jar
./presto-cli-0.161-executable.jar --server U007:8585 --catalog hive --schema default
或者 mv presto-cli-0.161-executable.jar presto都可以
./presto --server U007:8585 --catalog hive --schema default
执行该语句后在 presto shell 中执行: show tables 查看 hive 中的 default 库下的表。如果出现对应的表，表安装验证成功
</code></pre>

<h2 id="toc_20">jmx连接器</h2>

<ul>
<li>JMX提供了有关JVM中运行的Java虚拟机和软件的信息</li>
<li>jmx连接器用于在presto服务器中查询JMX信息</li>
</ul>

<p>在etc/catalog目录下新建<strong>jmx.properties</strong></p>

<pre><code>connector.name=jmx
</code></pre>

<p>现在连接presto cli以启用JMX插件</p>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog jmx --schema jmx
presto:jmx&gt; show schemas from jmx;
       Schema
--------------------
 current
 history
 information_schema
(3 rows)

Query 20171012_063601_00020_yuhat, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:00 [3 rows, 47B] [39 rows/s, 614B/s]
</code></pre>

<h2 id="toc_21">MySQL连接器</h2>

<p>vim etc/catalog/mysql.properties</p>

<pre><code>connector.name=mysql
connection-url=jdbc:mysql://10.10.25.13:3306
connection-user=root
connection-password=wankatest***
</code></pre>

<p>schema 后面跟的mysql的数据库,</p>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog mysql --schema test
presto:test&gt; show tables;
Query 20171012_071844_00002_iz4q8 failed: No worker nodes available

presto:test&gt; show tables;
          Table
--------------------------
 dmp_summary_daily_report
 tb_dmp_stat_appboot
 tb_dmp_stat_asdk_detail
 tb_dmp_stat_device
 tb_leidian1
 tb_leidian2
(6 rows)

Query 20171012_072024_00003_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:01 [6 rows, 190B] [6 rows/s, 196B/s]
</code></pre>

<h2 id="toc_22">kafka连接器</h2>

<p>暂时没这个需求,未测试.</p>

<h2 id="toc_23">系统连接器</h2>

<ul>
<li>系统连接器提供了正在运行的Presto集群的一些信息和指标</li>
<li>那么这个就可以通过标准sql很方便的查询这些信息</li>
<li>系统连接器不需要配置,已经内置了.我们可以很方便的访问名为<code>system</code>的catalog</li>
</ul>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog system
presto&gt; show schemas from system;
       Schema
--------------------
 information_schema
 jdbc
 metadata
 runtime
(4 rows)

Query 20171012_082437_00019_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:00 [4 rows, 57B] [70 rows/s, 997B/s]
</code></pre>

<p>查询有多少个节点</p>

<pre><code>presto&gt; SELECT * FROM system.runtime.nodes;
                  node_id                  |        http_uri         | node_version | coord
-------------------------------------------+-------------------------+--------------+------
 ffffffff-ffff-ffff-ffff-ffffffffffff-u006 | http://10.10.25.13:8585 | 0.161        | false
 ffffffff-ffff-ffff-ffff-ffffffffffff-u007 | http://10.10.25.14:8585 | 0.161        | true
 ffffffff-ffff-ffff-ffff-ffffffffffff-u008 | http://10.10.25.15:8585 | 0.161        | false
(3 rows)

Query 20171012_082952_00023_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
4:08 [3 rows, 228B] [0 rows/s, 0B/s]
</code></pre>

<h2 id="toc_24">JDBC接口</h2>

<h3 id="toc_25">依赖下载安装</h3>

<ul>
<li>下载地址(找到自己的版本下载):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc">https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc</a></li>
<li><code>presto-jdbc-0.161.jar</code>在jar文件下载之后，将其添加到Java应用程序的classpath中。</li>
<li>我不太喜欢用jar包的方式,那么可以在pom文件加入presto-jdbc的依赖</li>
<li><a href="http://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc">http://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc</a> 找到自己相应的版本</li>
</ul>

<pre><code>&lt;!-- https://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.facebook.presto&lt;/groupId&gt;
    &lt;artifactId&gt;presto-jdbc&lt;/artifactId&gt;
    &lt;version&gt;0.161&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Presto支持的URL格式如下：</p>

<pre><code>jdbc:presto://host:port
jdbc:presto://host:port/catalog
jdbc:presto://host:port/catalog/schema
</code></pre>

<p>例如，可以使用下面的URL来连接运行在U007服务器8585端口上的Presto的mysql catalog中的test schema：</p>

<pre><code>jdbc:presto://10.10.25.14:8585/mysql/test
</code></pre>

<p>这个url就是来连接hive catalog中的default schema</p>

<pre><code>jdbc:presto://10.10.25.14:8585/hive/default
</code></pre>

<h3 id="toc_26">Java代码</h3>

<p><strong>读取mysql下的test库下的所有表</strong></p>

<pre><code>public class PrestoJdbcDemo {

  public static void main(String[] args) throws SQLException, ClassNotFoundException {
    Class.forName(&quot;com.facebook.presto.jdbc.PrestoDriver&quot;);
    Connection connection = DriverManager
        .getConnection(&quot;jdbc:presto://10.10.25.14:8585/mysql/test&quot;, &quot;root&quot;, &quot;wankatest***&quot;);
    Statement stmt = connection.createStatement();
    ResultSet rs = stmt.executeQuery(&quot;show tables&quot;);
    while (rs.next()) {
      System.out.println(rs.getString(1));
    }
    rs.close();
    connection.close();

  }
}
</code></pre>

<p><strong>读取hive下的default库下的所有表</strong></p>

<pre><code>public class PrestoJdbcDemo {

  public static void main(String[] args) throws SQLException, ClassNotFoundException {
    Class.forName(&quot;com.facebook.presto.jdbc.PrestoDriver&quot;);
    Connection connection = DriverManager
        .getConnection(&quot;jdbc:presto://10.10.25.14:8585/hive/default&quot;,&quot;root&quot;,null);
    Statement stmt = connection.createStatement();
    ResultSet rs = stmt.executeQuery(&quot;show tables&quot;);
    while (rs.next()) {
      System.out.println(rs.getString(1));
    }
    rs.close();
    connection.close();

  }
}
</code></pre>

<h1 id="toc_27">不同数据源之间的join</h1>

<p>presto的一个特性就是其支持在不同的数据源之间进行join</p>

<p>当连接presto的客户端的时候,也可以不指定连接器</p>

<p>不同的数据源就用catalog名称指定,然后加上库名表明即可.</p>

<pre><code>./presto --server U007:8585
 show tables from mysql.dsp_test;
</code></pre>

<h1 id="toc_28">presto提供的函数和运算符</h1>

<p>参考文档:<a href="http://prestodb-china.com/docs/current/functions.html">http://prestodb-china.com/docs/current/functions.html</a></p>

<h1 id="toc_29">看日志</h1>

<h2 id="toc_30">日志路径</h2>

<p><strong>node.properties</strong>中配置了node.data-dir=/home/druid/data/presto</p>

<pre><code>[druid@U007 presto]$ tree
.
├── etc -&gt; /home/druid/presto-server-0.161/etc
├── plugin -&gt; /home/druid/presto-server-0.161/plugin
└── var
    ├── log
    │   ├── http-request.log
    │   ├── launcher.log
    │   └── server.log
    └── run
        └── launcher.pid

5 directories, 4 files
</code></pre>

<p>出现错误后,我们主要关注var/log目录下的日志.</p>

<p>当服务有问题的时候,看server.log找到报错原因,从而解决问题.</p>

<h1 id="toc_31">常见错误</h1>

<h2 id="toc_32">连接不上连接器</h2>

<p>类似这样的错误</p>

<pre><code>No factory for connector mysql
No factory for connector hive
</code></pre>

<p>如果服务报相关这样的错误,那么就需要关注各个连接器的配置文件是否写对了.各个连接器的配置文件是否在每个presto服务器上都部署了.</p>

<h1 id="toc_33">Presto的实现原理</h1>

<p><img src="http://oz6wyfxp0.bkt.clouddn.com/1512097348.png?imageMogr2/thumbnail/!70p" alt="Presto架构"/></p>

<ul>
<li>Presto的架构</li>
<li>Presto执行原理</li>
</ul>

<p>简单来说:</p>

<ol>
<li>cli客户端把查询需求发送给Coordinator节点.Coordinator节点负责解析sql语句,生成执行计划,分发执行任务给worer节点执行.所以worker节点负责实际执行查询任务.</li>
<li>worker节点启动后向discovery server服务注册,因此coordinator就可以从discovery server获得可以正常工作的worker节点.</li>
</ol>

<p>深入来说:</p>

<ol>
<li>参考网上相关blog.</li>
<li>买书.</li>
<li>看源码!</li>
</ol>

<h1 id="toc_34">参考文档</h1>

<ul>
<li><p><a href="https://prestodb.io">presto官网</a></p></li>
<li><p><a href="https://github.com/prestodb/presto">presto-Github</a></p></li>
<li><p><a href="http://prestodb-china.com">presto-京东维护-中文文档</a></p></li>
<li><p><a href="https://tech.meituan.com/presto.html">Presto实现原理和美团的使用实践</a></p></li>
<li><p><a href="http://getindata.com/tutorial-using-presto-to-combine-data-from-hive-and-mysql-in-one-sql-like-query/">Presto-Hive-Mysql-InteractiveQuery</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[字符串编码转换]]></title>
    <link href="http://dmlcoding.com/15219482100406.html"/>
    <updated>2018-03-25T11:23:30+08:00</updated>
    <id>http://dmlcoding.com/15219482100406.html</id>
    <content type="html"><![CDATA[
<ul>
<li>乱码之类的几乎都是由汉字引起的。</li>
<li>任何平台的任何编码 都能和 Unicode 互相转换</li>
<li>UTF-8 与 GBK 互相转换，那就先把UTF-8转换成Unicode，再从Unicode转换成GBK，反之同理。</li>
</ul>

<span id="more"></span><!-- more -->

<h2 id="toc_0">decode与encode</h2>

<ul>
<li><code>decode</code>的作用是将其他编码的字符串转换成 Unicode 编码</li>
<li><code>encode</code>的作用是将 Unicode 编码转换成其他编码的字符串</li>
<li>一句话：UTF-8是对Unicode字符集进行编码的一种编码方式</li>
</ul>

<h2 id="toc_1">utf-8转gbk</h2>

<pre><code class="language-python"># 这是一个 UTF-8 编码的字符串
utf8Str = &quot;你好地球&quot;

# 1. 将 UTF-8 编码的字符串 转换成 Unicode 编码
unicodeStr = utf8Str.decode(&quot;UTF-8&quot;)

# 2. 再将 Unicode 编码格式字符串 转换成 GBK 编码
gbkData = unicodeStr.encode(&quot;GBK&quot;)

</code></pre>

<h2 id="toc_2">gbk转utf-8</h2>

<pre><code>
# 1. 再将 GBK 编码格式字符串 转化成 Unicode
unicodeStr = gbkData.decode(&quot;gbk&quot;)

# 2. 再将 Unicode 编码格式字符串转换成 UTF-8
utf8Str = unicodeStr.encode(&quot;UTF-8&quot;)

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[json和jsonPATH[整理自网络]]]></title>
    <link href="http://dmlcoding.com/15219476833792.html"/>
    <updated>2018-03-25T11:14:43+08:00</updated>
    <id>http://dmlcoding.com/15219476833792.html</id>
    <content type="html"><![CDATA[
<p>JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。</p>

<p>JSON和XML的比较可谓不相上下。</p>

<span id="more"></span><!-- more -->

<p>Python 2.7中自带了JSON模块，直接<code>import json</code>就可以使用了。</p>

<p>官方文档：<a href="http://docs.python.org/library/json.html">http://docs.python.org/library/json.html</a></p>

<p>Json在线解析网站：<a href="http://www.json.cn/#">http://www.json.cn/#</a></p>

<h1 id="toc_0">JSON</h1>

<p>json简单说就是javascript中的对象和数组，所以这两种结构就是对象和数组两种结构，通过这两种结构可以表示各种复杂的结构</p>

<ol>
<li><p>对象：对象在js中表示为<code>{ }</code>括起来的内容，数据结构为 <code>{ key：value, key：value, ... }</code>的键值对的结构，在面向对象的语言中，key为对象的属性，value为对应的属性值，所以很容易理解，取值方法为 对象.key 获取属性值，这个属性值的类型可以是数字、字符串、数组、对象这几种。</p></li>
<li><p>数组：数组在js中是中括号<code>[ ]</code>括起来的内容，数据结构为 <code>[&quot;Python&quot;, &quot;javascript&quot;, &quot;C++&quot;, ...]</code>，取值方式和所有语言中一样，使用索引获取，字段值的类型可以是 数字、字符串、数组、对象几种。</p></li>
</ol>

<h2 id="toc_1">导入json模块</h2>

<pre><code>import json
</code></pre>

<p>json模块提供了四个功能：<code>dumps</code>、<code>dump</code>、<code>loads</code>、<code>load</code>，用于字符串 和 python数据类型间进行转换。</p>

<h2 id="toc_2">1. json.loads()</h2>

<p>把Json格式字符串解码转换成Python对象 从json到python的类型转化对照如下：</p>

<p><img src="media/15219476833792/15219477820474.jpg" alt=""/></p>

<pre><code># json_loads.py

import json

strList = &#39;[1, 2, 3, 4]&#39;

strDict = &#39;{&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;大猫&quot;}&#39;

json.loads(strList) 
# [1, 2, 3, 4]

json.loads(strDict) # json数据自动按Unicode存储
# {u&#39;city&#39;: u&#39;\u5317\u4eac&#39;, u&#39;name&#39;: u&#39;\u5927\u732b&#39;}

</code></pre>

<h2 id="toc_3">2. json.dumps()</h2>

<p>实现python类型转化为json字符串，返回一个str对象 把一个Python对象编码转换成Json字符串</p>

<p>从python原始类型向json类型的转化对照如下：</p>

<p><img src="media/15219476833792/15219478024682.jpg" alt=""/></p>

<pre><code># json_dumps.py

import json
import chardet

listStr = [1, 2, 3, 4]
tupleStr = (1, 2, 3, 4)
dictStr = {&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;大猫&quot;}

json.dumps(listStr)
# &#39;[1, 2, 3, 4]&#39;
json.dumps(tupleStr)
# &#39;[1, 2, 3, 4]&#39;

# 注意：json.dumps() 序列化时默认使用的ascii编码
# 添加参数 ensure_ascii=False 禁用ascii编码，按utf-8编码
# chardet.detect()返回字典, 其中confidence是检测精确度

json.dumps(dictStr) 
# &#39;{&quot;city&quot;: &quot;\\u5317\\u4eac&quot;, &quot;name&quot;: &quot;\\u5927\\u5218&quot;}&#39;

chardet.detect(json.dumps(dictStr))
# {&#39;confidence&#39;: 1.0, &#39;encoding&#39;: &#39;ascii&#39;}

print json.dumps(dictStr, ensure_ascii=False) 
# {&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;大刘&quot;}

chardet.detect(json.dumps(dictStr, ensure_ascii=False))
# {&#39;confidence&#39;: 0.99, &#39;encoding&#39;: &#39;utf-8&#39;}

</code></pre>

<p><strong><em>chardet是一个非常优秀的编码识别模块，可通过pip安装</em></strong></p>

<h2 id="toc_4">3. json.dump()</h2>

<p>将Python内置类型序列化为json对象后写入文件</p>

<pre><code># json_dump.py

import json

listStr = [{&quot;city&quot;: &quot;北京&quot;}, {&quot;name&quot;: &quot;大刘&quot;}]
json.dump(listStr, open(&quot;listStr.json&quot;,&quot;w&quot;), ensure_ascii=False)

dictStr = {&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;大刘&quot;}
json.dump(dictStr, open(&quot;dictStr.json&quot;,&quot;w&quot;), ensure_ascii=False)

</code></pre>

<h2 id="toc_5">4. json.load()</h2>

<p>读取文件中json形式的字符串元素 转化成python类型</p>

<pre><code># json_load.py

import json

strList = json.load(open(&quot;listStr.json&quot;))
print strList

# [{u&#39;city&#39;: u&#39;\u5317\u4eac&#39;}, {u&#39;name&#39;: u&#39;\u5927\u5218&#39;}]

strDict = json.load(open(&quot;dictStr.json&quot;))
print strDict
# {u&#39;city&#39;: u&#39;\u5317\u4eac&#39;, u&#39;name&#39;: u&#39;\u5927\u5218&#39;}

</code></pre>

<h1 id="toc_6">JsonPath</h1>

<p>JsonPath 是一种信息抽取类库，是从JSON文档中抽取指定信息的工具，提供多种语言实现版本，包括：Javascript, Python， PHP 和 Java。</p>

<p>JsonPath 对于 JSON 来说，相当于 XPATH 对于 XML。</p>

<blockquote>
<p>下载地址：<a href="https://pypi.python.org/pypi/jsonpath/">https://pypi.python.org/pypi/jsonpath</a></p>

<p>安装方法：点击<code>Download URL</code>链接下载jsonpath，解压之后执行<code>python setup.py install</code></p>

<p>官方文档：<a href="http://goessner.net/articles/JsonPath/">http://goessner.net/articles/JsonPath</a></p>
</blockquote>

<h2 id="toc_7">JsonPath与XPath语法对比：</h2>

<p>Json结构清晰，可读性高，复杂度低，非常容易匹配，下表中对应了XPath的用法。</p>

<table>
<thead>
<tr>
<th style="text-align: center">XPath</th>
<th>JSONPath</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: center"><code>/</code></td>
<td><code>$</code></td>
<td>根节点</td>
</tr>
<tr>
<td style="text-align: center"><code>.</code></td>
<td><code>@</code></td>
<td>现行节点</td>
</tr>
<tr>
<td style="text-align: center"><code>/</code></td>
<td><code>.</code>or<code>[]</code></td>
<td>取子节点</td>
</tr>
<tr>
<td style="text-align: center"><code>..</code></td>
<td>n/a</td>
<td>取父节点，Jsonpath未支持</td>
</tr>
<tr>
<td style="text-align: center"><code>//</code></td>
<td><code>..</code></td>
<td>就是不管位置，选择所有符合条件的条件</td>
</tr>
<tr>
<td style="text-align: center"><code>*</code></td>
<td><code>*</code></td>
<td>匹配所有元素节点</td>
</tr>
<tr>
<td style="text-align: center"><code>@</code></td>
<td>n/a</td>
<td>根据属性访问，Json不支持，因为Json是个Key-value递归结构，不需要。</td>
</tr>
<tr>
<td style="text-align: center"><code>[]</code></td>
<td><code>[]</code></td>
<td>迭代器标示（可以在里边做简单的迭代操作，如数组下标，根据内容选值等）</td>
</tr>
<tr>
<td style="text-align: center"></td>
<td></td>
<td><code>[,]</code></td>
</tr>
<tr>
<td style="text-align: center"><code>[]</code></td>
<td><code>?()</code></td>
<td>支持过滤操作.</td>
</tr>
<tr>
<td style="text-align: center">n/a</td>
<td><code>()</code></td>
<td>支持表达式计算</td>
</tr>
<tr>
<td style="text-align: center"><code>()</code></td>
<td>n/a</td>
<td>分组，JsonPath不支持</td>
</tr>
</tbody>
</table>

<h2 id="toc_8">示例：</h2>

<p>我们以拉勾网城市JSON文件 <a href="http://www.lagou.com/lbs/getAllCitySearchLabels.json">http://www.lagou.com/lbs/getAllCitySearchLabels.json</a> 为例，获取所有城市。</p>

<pre><code># jsonpath_lagou.py

import urllib2
import jsonpath
import json
import chardet

url = &#39;http://www.lagou.com/lbs/getAllCitySearchLabels.json&#39;
request =urllib2.Request(url)
response = urllib2.urlopen(request)
html = response.read()

# 把json格式字符串转换成python对象
jsonobj = json.loads(html)

# 从根节点开始，匹配name节点
citylist = jsonpath.jsonpath(jsonobj,&#39;$..name&#39;)

print citylist
print type(citylist)
fp = open(&#39;city.json&#39;,&#39;w&#39;)

content = json.dumps(citylist, ensure_ascii=False)
print content

fp.write(content.encode(&#39;utf-8&#39;))
fp.close()

</code></pre>

<h2 id="toc_9">注意事项：</h2>

<p>json.loads() 是把 Json格式字符串解码转换成Python对象，如果在json.loads的时候出错，要注意被解码的Json字符的编码。</p>

<p>如果传入的字符串的编码不是UTF-8的话，需要指定字符编码的参数 <code>encoding</code></p>

<pre><code>dataDict = json.loads(jsonStrGBK);

</code></pre>

<ul>
<li><p>dataJsonStr是JSON字符串，假设其编码本身是非UTF-8的话而是GBK 的，那么上述代码会导致出错，改为对应的：</p>

<pre><code>  dataDict = json.loads(jsonStrGBK, encoding=&quot;GBK&quot;);

</code></pre></li>
<li><p>如果 dataJsonStr通过encoding指定了合适的编码，但是其中又包含了其他编码的字符，则需要先去将dataJsonStr转换为Unicode，然后再指定编码格式调用json.loads()</p></li>
</ul>

<pre><code>``` python

</code></pre>

<p>dataJsonStrUni = dataJsonStr.decode(&quot;GB2312&quot;); dataDict = json.loads(dataJsonStrUni, encoding=&quot;GB2312&quot;);</p>

<pre><code>

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[xml和xpath[整理自网络]]]></title>
    <link href="http://dmlcoding.com/15219455706450.html"/>
    <updated>2018-03-25T10:39:30+08:00</updated>
    <id>http://dmlcoding.com/15219455706450.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">什么是XML</h2>

<ul>
<li>XML 指可扩展标记语言（EXtensible Markup Language）</li>
<li>XML 是一种标记语言，很类似 HTML</li>
<li>XML 的设计宗旨是传输数据，而非显示数据</li>
<li>XML 的标签需要我们自行定义。</li>
<li>XML 被设计为具有自我描述性。</li>
<li>XML 是 W3C 的推荐标准</li>
</ul>

<p>W3School官方文档：<a href="http://www.w3school.com.cn/xml/index.asp">http://www.w3school.com.cn/xml/index.asp</a></p>

<h4 id="toc_1">XML 和 HTML 的区别</h4>

<table>
<thead>
<tr>
<th>数据格式</th>
<th style="text-align: center">描述</th>
<th>设计目标</th>
</tr>
</thead>

<tbody>
<tr>
<td>XML</td>
<td style="text-align: center">Extensible Markup Language <code>（可扩展标记语言）</code></td>
<td>被设计为传输和存储数据，其焦点是数据的内容。</td>
</tr>
<tr>
<td>HTML</td>
<td style="text-align: center">HyperText Markup Language <code>（超文本标记语言）</code></td>
<td>显示数据以及如何更好显示数据。</td>
</tr>
<tr>
<td>HTML DOM</td>
<td style="text-align: center">Document Object Model for HTML <code>(文档对象模型)</code></td>
<td>通过 HTML DOM，可以访问所有的 HTML 元素，连同它们所包含的文本和属性。可以对其中的内容进行修改和删除，同时也可以创建新的元素。</td>
</tr>
</tbody>
</table>

<h5 id="toc_2">XML文档示例</h5>

<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;

&lt;bookstore&gt; 

  &lt;book category=&quot;cooking&quot;&gt; 
    &lt;title lang=&quot;en&quot;&gt;Everyday Italian&lt;/title&gt;  
    &lt;author&gt;Giada De Laurentiis&lt;/author&gt;  
    &lt;year&gt;2005&lt;/year&gt;  
    &lt;price&gt;30.00&lt;/price&gt; 
  &lt;/book&gt;  

  &lt;book category=&quot;children&quot;&gt; 
    &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt;  
    &lt;author&gt;J K. Rowling&lt;/author&gt;  
    &lt;year&gt;2005&lt;/year&gt;  
    &lt;price&gt;29.99&lt;/price&gt; 
  &lt;/book&gt;  

  &lt;book category=&quot;web&quot;&gt; 
    &lt;title lang=&quot;en&quot;&gt;XQuery Kick Start&lt;/title&gt;  
    &lt;author&gt;James McGovern&lt;/author&gt;  
    &lt;author&gt;Per Bothner&lt;/author&gt;  
    &lt;author&gt;Kurt Cagle&lt;/author&gt;  
    &lt;author&gt;James Linn&lt;/author&gt;  
    &lt;author&gt;Vaidyanathan Nagarajan&lt;/author&gt;  
    &lt;year&gt;2003&lt;/year&gt;  
    &lt;price&gt;49.99&lt;/price&gt; 
  &lt;/book&gt; 

  &lt;book category=&quot;web&quot; cover=&quot;paperback&quot;&gt; 
    &lt;title lang=&quot;en&quot;&gt;Learning XML&lt;/title&gt;  
    &lt;author&gt;Erik T. Ray&lt;/author&gt;  
    &lt;year&gt;2003&lt;/year&gt;  
    &lt;price&gt;39.95&lt;/price&gt; 
  &lt;/book&gt; 

&lt;/bookstore&gt;

</code></pre>

<h5 id="toc_3">HTML DOM 模型示例</h5>

<p>HTML DOM 定义了访问和操作 HTML 文档的标准方法，以树结构方式表达 HTML 文档。</p>

<p><img src="media/15219455706450/15219456473425.jpg" alt=""/></p>

<hr/>

<h3 id="toc_4">XML的节点关系</h3>

<h4 id="toc_5">1. 父（Parent）</h4>

<p>每个元素以及属性都有一个父。</p>

<p>下面是一个简单的XML例子中，book 元素是 title、author、year 以及 price 元素的父：</p>

<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;

&lt;book&gt;
  &lt;title&gt;Harry Potter&lt;/title&gt;
  &lt;author&gt;J K. Rowling&lt;/author&gt;
  &lt;year&gt;2005&lt;/year&gt;
  &lt;price&gt;29.99&lt;/price&gt;
&lt;/book&gt;

</code></pre>

<h4 id="toc_6">2. 子（Children）</h4>

<p>元素节点可有零个、一个或多个子。</p>

<p>在下面的例子中，title、author、year 以及 price 元素都是 book 元素的子：</p>

<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;

&lt;book&gt;
  &lt;title&gt;Harry Potter&lt;/title&gt;
  &lt;author&gt;J K. Rowling&lt;/author&gt;
  &lt;year&gt;2005&lt;/year&gt;
  &lt;price&gt;29.99&lt;/price&gt;
&lt;/book&gt;

</code></pre>

<h4 id="toc_7">3. 同胞（Sibling）</h4>

<p>拥有相同的父的节点</p>

<p>在下面的例子中，title、author、year 以及 price 元素都是同胞：</p>

<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;

&lt;book&gt;
  &lt;title&gt;Harry Potter&lt;/title&gt;
  &lt;author&gt;J K. Rowling&lt;/author&gt;
  &lt;year&gt;2005&lt;/year&gt;
  &lt;price&gt;29.99&lt;/price&gt;
&lt;/book&gt;

</code></pre>

<h4 id="toc_8">4. 先辈（Ancestor）</h4>

<p>某节点的父、父的父，等等。</p>

<p>在下面的例子中，title 元素的先辈是 book 元素和 bookstore 元素：</p>

<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;

&lt;bookstore&gt;

&lt;book&gt;
  &lt;title&gt;Harry Potter&lt;/title&gt;
  &lt;author&gt;J K. Rowling&lt;/author&gt;
  &lt;year&gt;2005&lt;/year&gt;
  &lt;price&gt;29.99&lt;/price&gt;
&lt;/book&gt;

&lt;/bookstore&gt;

</code></pre>

<h4 id="toc_9">5. 后代（Descendant）</h4>

<p>某个节点的子，子的子，等等。</p>

<p>在下面的例子中，bookstore 的后代是 book、title、author、year 以及 price 元素：</p>

<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;

&lt;bookstore&gt;

&lt;book&gt;
  &lt;title&gt;Harry Potter&lt;/title&gt;
  &lt;author&gt;J K. Rowling&lt;/author&gt;
  &lt;year&gt;2005&lt;/year&gt;
  &lt;price&gt;29.99&lt;/price&gt;
&lt;/book&gt;

&lt;/bookstore&gt;

</code></pre>

<h2 id="toc_10">什么是XPath？</h2>

<blockquote>
<p>XPath (XML Path Language) 是一门在 XML 文档中查找信息的语言，可用来在 XML 文档中对元素和属性进行遍历。</p>

<p>W3School官方文档：<a href="http://www.w3school.com.cn/xpath/index.asp">http://www.w3school.com.cn/xpath/index.asp</a></p>
</blockquote>

<h3 id="toc_11">XPath 开发工具</h3>

<ol>
<li>开源的XPath表达式编辑工具:XMLQuire(XML格式文件可用)</li>
<li>Chrome插件 XPath Helper</li>
<li>Firefox插件 XPath Checker</li>
</ol>

<h3 id="toc_12">选取节点</h3>

<p>XPath 使用路径表达式来选取 XML 文档中的节点或者节点集。这些路径表达式和我们在常规的电脑文件系统中看到的表达式非常相似。</p>

<p>下面列出了最常用的路径表达式：</p>

<table>
<thead>
<tr>
<th>表达式</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>nodename</td>
<td>选取此节点的所有子节点。</td>
</tr>
<tr>
<td>/</td>
<td>从根节点选取。</td>
</tr>
<tr>
<td>//</td>
<td>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。</td>
</tr>
<tr>
<td>.</td>
<td>选取当前节点。</td>
</tr>
<tr>
<td>..</td>
<td>选取当前节点的父节点。</td>
</tr>
<tr>
<td>@</td>
<td>选取属性。</td>
</tr>
</tbody>
</table>

<p>在下面的表格中，我们已列出了一些路径表达式以及表达式的结果：</p>

<table>
<thead>
<tr>
<th></th>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>

<tbody>
<tr>
<td>bookstore</td>
<td>选取 bookstore 元素的所有子节点。</td>
<td></td>
</tr>
<tr>
<td>/bookstore</td>
<td>选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！</td>
<td></td>
</tr>
<tr>
<td>bookstore/book</td>
<td>选取属于 bookstore 的子元素的所有 book 元素。</td>
<td></td>
</tr>
<tr>
<td>//book</td>
<td>选取所有 book 子元素，而不管它们在文档中的位置。</td>
<td></td>
</tr>
<tr>
<td>bookstore//book</td>
<td>选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。</td>
<td></td>
</tr>
<tr>
<td>//@lang</td>
<td>选取名为 lang 的所有属性。</td>
<td></td>
</tr>
</tbody>
</table>

<h3 id="toc_13">谓语（Predicates）</h3>

<p>谓语用来查找某个特定的节点或者包含某个指定的值的节点，被嵌在方括号中。</p>

<p>在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果：</p>

<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>

<tbody>
<tr>
<td>/bookstore/book[1]</td>
<td>选取属于 bookstore 子元素的第一个 book 元素。</td>
</tr>
<tr>
<td>/bookstore/book[last()]</td>
<td>选取属于 bookstore 子元素的最后一个 book 元素。</td>
</tr>
<tr>
<td>/bookstore/book[last()-1]</td>
<td>选取属于 bookstore 子元素的倒数第二个 book 元素。</td>
</tr>
<tr>
<td>/bookstore/book[position()35.00]</td>
<td>选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。</td>
</tr>
<tr>
<td>/bookstore/book[price&gt;35.00]/title</td>
<td>选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。</td>
</tr>
</tbody>
</table>

<h3 id="toc_14">选取未知节点</h3>

<p>XPath 通配符可用来选取未知的 XML 元素。</p>

<table>
<thead>
<tr>
<th>通配符</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>*</td>
<td>匹配任何元素节点。</td>
</tr>
<tr>
<td>@*</td>
<td>匹配任何属性节点。</td>
</tr>
<tr>
<td>node()</td>
<td>匹配任何类型的节点。</td>
</tr>
</tbody>
</table>

<p>在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：</p>

<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>

<tbody>
<tr>
<td>/bookstore/*</td>
<td>选取 bookstore 元素的所有子元素。</td>
</tr>
<tr>
<td>//*</td>
<td>选取文档中的所有元素。</td>
</tr>
<tr>
<td>//title[@*]</td>
<td>选取所有带有属性的 title 元素。</td>
</tr>
</tbody>
</table>

<h3 id="toc_15">选取若干路径</h3>

<p>通过在路径表达式中使用“|”运算符，您可以选取若干个路径。</p>

<p>实例</p>

<p>在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：</p>

<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>

<tbody>
<tr>
<td>//book/title</td>
<td>//book/price</td>
</tr>
<tr>
<td>//title</td>
<td>//price</td>
</tr>
<tr>
<td>/bookstore/book/title</td>
<td>//price</td>
</tr>
</tbody>
</table>

<h3 id="toc_16">XPath的运算符</h3>

<p>下面列出了可用在 XPath 表达式中的运算符：</p>

<p><img src="media/15219455706450/15219457279252.jpg" alt=""/></p>

<h5 id="toc_17">这些就是XPath的语法内容，在运用到Python抓取时要先转换为xml。</h5>

<h2 id="toc_18">lxml库</h2>

<blockquote>
<p>lxml 是 一个HTML/XML的解析器，主要的功能是如何解析和提取 HTML/XML 数据。</p>

<p>lxml和正则一样，也是用 C 实现的，是一款高性能的 Python HTML/XML 解析器，我们可以利用之前学习的XPath语法，来快速的定位特定元素以及节点信息。</p>

<p>lxml python 官方文档：<a href="http://lxml.de/index.html">http://lxml.de/index.html</a></p>

<p>需要安装C语言库，可使用 pip 安装：<code>pip install lxml</code> （或通过wheel方式安装）</p>
</blockquote>

<h3 id="toc_19">初步使用</h3>

<p>我们利用它来解析 HTML 代码，简单示例：</p>

<pre><code># lxml_test.py

# 使用 lxml 的 etree 库
from lxml import etree 

text = &#39;&#39;&#39;
&lt;div&gt;
    &lt;ul&gt;
         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; # 注意，此处缺少一个 &lt;/li&gt; 闭合标签
     &lt;/ul&gt;
 &lt;/div&gt;
&#39;&#39;&#39;

#利用etree.HTML，将字符串解析为HTML文档
html = etree.HTML(text) 

# 按字符串序列化HTML文档
result = etree.tostring(html) 

print(result)

</code></pre>

<p>输出结果：</p>

<pre><code>&lt;html&gt;&lt;body&gt;
&lt;div&gt;
    &lt;ul&gt;
         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
 &lt;/div&gt;
&lt;/body&gt;&lt;/html&gt;

</code></pre>

<p>lxml 可以自动修正 html 代码，例子里不仅补全了 li 标签，还添加了 body，html 标签。</p>

<h3 id="toc_20">文件读取：</h3>

<p>除了直接读取字符串，lxml还支持从文件里读取内容。我们新建一个hello.html文件：</p>

<pre><code>&lt;!-- hello.html --&gt;

&lt;div&gt;
    &lt;ul&gt;
         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;
     &lt;/ul&gt;
 &lt;/div&gt;

</code></pre>

<p>再利用 etree.parse() 方法来读取文件。</p>

<pre><code># lxml_parse.py

from lxml import etree

# 读取外部文件 hello.html
html = etree.parse(&#39;./hello.html&#39;)
result = etree.tostring(html, pretty_print=True)

print(result)

</code></pre>

<p>输出结果与之前相同：</p>

<pre><code>&lt;html&gt;&lt;body&gt;
&lt;div&gt;
    &lt;ul&gt;
         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;
         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
 &lt;/div&gt;
&lt;/body&gt;&lt;/html&gt;

</code></pre>

<h3 id="toc_21">XPath实例测试</h3>

<h4 id="toc_22">1. 获取所有的 <code>&lt;li&gt;</code> 标签</h4>

<pre><code># xpath_li.py

from lxml import etree

html = etree.parse(&#39;hello.html&#39;)
print type(html)  # 显示etree.parse() 返回类型

result = html.xpath(&#39;//li&#39;)

print result  # 打印&lt;li&gt;标签的元素集合
print len(result)
print type(result)
print type(result[0])

</code></pre>

<p>输出结果：</p>

<pre><code>&lt;type &#39;lxml.etree._ElementTree&#39;&gt;
[&lt;Element li at 0x1014e0e18&gt;, &lt;Element li at 0x1014e0ef0&gt;, &lt;Element li at 0x1014e0f38&gt;, &lt;Element li at 0x1014e0f80&gt;, &lt;Element li at 0x1014e0fc8&gt;]
5
&lt;type &#39;list&#39;&gt;
&lt;type &#39;lxml.etree._Element&#39;&gt;

</code></pre>

<h4 id="toc_23">2. 继续获取<code>&lt;li&gt;</code> 标签的所有 <code>class</code>属性</h4>

<pre><code># xpath_li.py

from lxml import etree

html = etree.parse(&#39;hello.html&#39;)
result = html.xpath(&#39;//li/@class&#39;)

print result

</code></pre>

<p>运行结果</p>

<pre><code>[&#39;item-0&#39;, &#39;item-1&#39;, &#39;item-inactive&#39;, &#39;item-1&#39;, &#39;item-0&#39;]

</code></pre>

<h4 id="toc_24">3. 继续获取<code>&lt;li&gt;</code>标签下<code>hre</code> 为 <code>link1.html</code> 的 <code>&lt;a&gt;</code> 标签</h4>

<pre><code># xpath_li.py

from lxml import etree

html = etree.parse(&#39;hello.html&#39;)
result = html.xpath(&#39;//li/a[@href=&quot;link1.html&quot;]&#39;)

print result

</code></pre>

<p>运行结果</p>

<pre><code>[&lt;Element a at 0x10ffaae18&gt;]

</code></pre>

<h4 id="toc_25">4. 获取<code>&lt;li&gt;</code> 标签下的所有 <code>&lt;span&gt;</code> 标签</h4>

<pre><code># xpath_li.py

from lxml import etree

html = etree.parse(&#39;hello.html&#39;)

#result = html.xpath(&#39;//li/span&#39;)
#注意这么写是不对的：
#因为 / 是用来获取子元素的，而 &lt;span&gt; 并不是 &lt;li&gt; 的子元素，所以，要用双斜杠

result = html.xpath(&#39;//li//span&#39;)

print result

</code></pre>

<p>运行结果</p>

<pre><code>[&lt;Element span at 0x10d698e18&gt;]

</code></pre>

<h4 id="toc_26">5. 获取 <code>&lt;li&gt;</code> 标签下的<code>&lt;a&gt;</code>标签里的所有 class</h4>

<pre><code># xpath_li.py

from lxml import etree

html = etree.parse(&#39;hello.html&#39;)
result = html.xpath(&#39;//li/a//@class&#39;)

print result

</code></pre>

<p>运行结果</p>

<pre><code>[&#39;blod&#39;]

</code></pre>

<h4 id="toc_27">6. 获取最后一个 <code>&lt;li&gt;</code> 的 <code>&lt;a&gt;</code> 的 href</h4>

<pre><code># xpath_li.py

from lxml import etree

html = etree.parse(&#39;hello.html&#39;)

result = html.xpath(&#39;//li[last()]/a/@href&#39;)
# 谓语 [last()] 可以找到最后一个元素

print result

</code></pre>

<p>运行结果</p>

<pre><code>[&#39;link5.html&#39;]

</code></pre>

<h4 id="toc_28">7. 获取倒数第二个元素的内容</h4>

<pre><code># xpath_li.py

from lxml import etree

html = etree.parse(&#39;hello.html&#39;)
result = html.xpath(&#39;//li[last()-1]/a&#39;)

# text 方法可以获取元素内容
print result[0].text

</code></pre>

<p>运行结果</p>

<pre><code>fourth item

</code></pre>

<h4 id="toc_29">8. 获取 <code>class</code> 值为 <code>bold</code> 的标签名</h4>

<pre><code># xpath_li.py

from lxml import etree

html = etree.parse(&#39;hello.html&#39;)

result = html.xpath(&#39;//*[@class=&quot;bold&quot;]&#39;)

# tag方法可以获取标签名
print result[0].tag

</code></pre>

<p>运行结果</p>

<pre><code>span

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[python-requests[整理自网络]]]></title>
    <link href="http://dmlcoding.com/15219449578725.html"/>
    <updated>2018-03-25T10:29:17+08:00</updated>
    <id>http://dmlcoding.com/15219449578725.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Requests: 让 HTTP 服务人类</h2>

<p>虽然Python的标准库中 urllib2 模块已经包含了平常我们使用的大多数功能，但是它的 API 使用起来让人感觉不太好，而 Requests 自称 “HTTP for Humans”，说明使用更简洁方便。</p>

<p>Requests 继承了urllib2的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的 URL 和 POST 数据自动编码。</p>

<span id="more"></span><!-- more -->

<p>Requests的文档非常完备，中文文档也相当不错。Requests能完全满足当前网络的需求，支持Python 2.6—3.5，而且能在PyPy下完美运行。</p>

<p>开源地址：<a href="https://github.com/kennethreitz/requests">https://github.com/kennethreitz/requests</a></p>

<p>中文文档 API： <a href="http://docs.python-requests.org/zh_CN/latest/index.html">http://docs.python-requests.org/zh_CN/latest/index.html</a></p>

<h2 id="toc_1">安装方式</h2>

<p>利用 pip 安装 或者利用 easy_install 都可以完成安装：</p>

<pre><code>$ pip install requests

$ easy_install requests

</code></pre>

<h2 id="toc_2">基本GET请求（headers参数 和 parmas参数）</h2>

<h5 id="toc_3">1. 最基本的GET请求可以直接用get方法</h5>

<pre><code>response = requests.get(&quot;http://www.baidu.com/&quot;)

# 也可以这么写
# response = requests.request(&quot;get&quot;, &quot;http://www.baidu.com/&quot;)

</code></pre>

<h5 id="toc_4">2. 添加 headers 和 查询参数</h5>

<p>如果想添加 headers，可以传入<code>headers</code>参数来增加请求头中的headers信息。如果要将参数放在url中传递，可以利用 <code>params</code> 参数。</p>

<pre><code>
import requests

kw = {&#39;wd&#39;:&#39;长城&#39;}

headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;}

# params 接收一个字典或者字符串的查询参数，字典类型自动转换为url编码，不需要urlencode()
response = requests.get(&quot;http://www.baidu.com/s?&quot;, params = kw, headers = headers)

# 查看响应内容，response.text 返回的是Unicode格式的数据
print response.text

# 查看响应内容，response.content返回的字节流数据
print respones.content

# 查看完整url地址
print response.url

# 查看响应头部字符编码
print response.encoding

# 查看响应码
print response.status_code

</code></pre>

<p>运行结果</p>

<pre><code>......

......

&#39;http://www.baidu.com/s?wd=%E9%95%BF%E5%9F%8E&#39;

&#39;utf-8&#39;

200

</code></pre>

<blockquote>
<ul>
<li><p>使用response.text 时，Requests 会基于 HTTP 响应的文本编码自动解码响应内容，大多数 Unicode 字符集都能被无缝地解码。</p></li>
<li><p>使用response.content 时，返回的是服务器响应数据的原始二进制字节流，可以用来保存图片等二进制文件。</p></li>
</ul>
</blockquote>

<h2 id="toc_5">基本POST请求（data参数）</h2>

<h5 id="toc_6">1. 最基本的GET请求可以直接用post方法</h5>

<pre><code>response = requests.post(&quot;http://www.baidu.com/&quot;, data = data)

</code></pre>

<h5 id="toc_7">2. 传入data数据</h5>

<p>对于 POST 请求来说，我们一般需要为它增加一些参数。那么最基本的传参方法可以利用 <code>data</code> 这个参数。</p>

<pre><code>import requests

formdata = {
    &quot;type&quot;:&quot;AUTO&quot;,
    &quot;i&quot;:&quot;i love python&quot;,
    &quot;doctype&quot;:&quot;json&quot;,
    &quot;xmlVersion&quot;:&quot;1.8&quot;,
    &quot;keyfrom&quot;:&quot;fanyi.web&quot;,
    &quot;ue&quot;:&quot;UTF-8&quot;,
    &quot;action&quot;:&quot;FY_BY_ENTER&quot;,
    &quot;typoResult&quot;:&quot;true&quot;
}

url = &quot;http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;smartresult=ugc&amp;sessionFrom=null&quot;

headers={ &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36&quot;}

response = requests.post(url, data = formdata, headers = headers)

print response.text

# 如果是json文件可以直接显示
print response.json()

</code></pre>

<p>运行结果</p>

<pre><code>{&quot;type&quot;:&quot;EN2ZH_CN&quot;,&quot;errorCode&quot;:0,&quot;elapsedTime&quot;:2,&quot;translateResult&quot;:[[{&quot;src&quot;:&quot;i love python&quot;,&quot;tgt&quot;:&quot;我喜欢python&quot;}]],&quot;smartResult&quot;:{&quot;type&quot;:1,&quot;entries&quot;:[&quot;&quot;,&quot;肆文&quot;,&quot;高德纳&quot;]}}

{u&#39;errorCode&#39;: 0, u&#39;elapsedTime&#39;: 0, u&#39;translateResult&#39;: [[{u&#39;src&#39;: u&#39;i love python&#39;, u&#39;tgt&#39;: u&#39;\u6211\u559c\u6b22python&#39;}]], u&#39;smartResult&#39;: {u&#39;type&#39;: 1, u&#39;entries&#39;: [u&#39;&#39;, u&#39;\u8086\u6587&#39;, u&#39;\u9ad8\u5fb7\u7eb3&#39;]}, u&#39;type&#39;: u&#39;EN2ZH_CN&#39;}

</code></pre>

<h2 id="toc_8">代理（proxies参数）</h2>

<p>如果需要使用代理，你可以通过为任意请求方法提供 <code>proxies</code> 参数来配置单个请求：</p>

<pre><code>import requests

# 根据协议类型，选择不同的代理
proxies = {
  &quot;http&quot;: &quot;http://12.34.56.79:9527&quot;,
  &quot;https&quot;: &quot;http://12.34.56.79:9527&quot;,
}

response = requests.get(&quot;http://www.baidu.com&quot;, proxies = proxies)
print response.text

</code></pre>

<p>也可以通过本地环境变量 <code>HTTP_PROXY</code> 和 <code>HTTPS_PROXY</code> 来配置代理：</p>

<pre><code>export HTTP_PROXY=&quot;http://12.34.56.79:9527&quot;
export HTTPS_PROXY=&quot;https://12.34.56.79:9527&quot;

</code></pre>

<h2 id="toc_9">私密代理验证（特定格式） 和 Web客户端验证（auth 参数）</h2>

<p>urllib2 这里的做法比较复杂，requests只需要一步：</p>

<h4 id="toc_10">私密代理</h4>

<pre><code>
import requests

# 如果代理需要使用HTTP Basic Auth，可以使用下面这种格式：
proxy = { &quot;http&quot;: &quot;mr_mao_hacker:sffqry9r@61.158.163.130:16816&quot; }

response = requests.get(&quot;http://www.baidu.com&quot;, proxies = proxy)

print response.text

</code></pre>

<h4 id="toc_11">web客户端验证</h4>

<p>如果是Web客户端验证，需要添加 auth = (账户名, 密码)</p>

<pre><code>import requests

auth=(&#39;test&#39;, &#39;123456&#39;)

response = requests.get(&#39;http://192.168.199.107&#39;, auth = auth)

print response.text

</code></pre>

<blockquote>
<p>urllib2 泪奔...</p>
</blockquote>

<h2 id="toc_12">Cookies 和 Session</h2>

<h3 id="toc_13">Cookies</h3>

<p>如果一个响应中包含了cookie，那么我们可以利用 cookies参数拿到：</p>

<pre><code>
import requests

response = requests.get(&quot;http://www.baidu.com/&quot;)

# 7\. 返回CookieJar对象:
cookiejar = response.cookies

# 8\. 将CookieJar转为字典：
cookiedict = requests.utils.dict_from_cookiejar(cookiejar)

print cookiejar

print cookiedict

</code></pre>

<p>运行结果：</p>

<pre><code>&lt;RequestsCookieJar[&lt;Cookie BDORZ=27315 for .baidu.com/&gt;]&gt;

{&#39;BDORZ&#39;: &#39;27315&#39;}

</code></pre>

<h3 id="toc_14">Session</h3>

<p>在 requests 里，session对象是一个非常常用的对象，这个对象代表一次用户会话：从客户端浏览器连接服务器开始，到客户端浏览器与服务器断开。</p>

<p>会话能让我们在跨请求时候保持某些参数，比如在同一个 Session 实例发出的所有请求之间保持 cookie 。</p>

<h4 id="toc_15">实现人人网登录</h4>

<pre><code>import requests

# 1. 创建session对象，可以保存Cookie值
ssion = requests.session()

# 2. 处理 headers
headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;}

# 3. 需要登录的用户名和密码
data = {&quot;email&quot;:&quot;mr_mao_hacker@163.com&quot;, &quot;password&quot;:&quot;alarmchime&quot;}  

# 4. 发送附带用户名和密码的请求，并获取登录后的Cookie值，保存在ssion里
ssion.post(&quot;http://www.renren.com/PLogin.do&quot;, data = data)

# 5. ssion包含用户登录后的Cookie值，可以直接访问那些登录后才可以访问的页面
response = ssion.get(&quot;http://www.renren.com/410043129/profile&quot;)

# 6. 打印响应内容
print response.text

</code></pre>

<h2 id="toc_16">处理HTTPS请求 SSL证书验证</h2>

<p>Requests也可以为HTTPS请求验证SSL证书：</p>

<ul>
<li>要想检查某个主机的SSL证书，你可以使用 verify 参数（也可以不写）</li>
</ul>

<pre><code>import requests
response = requests.get(&quot;https://www.baidu.com/&quot;, verify=True)

# 也可以省略不写
# response = requests.get(&quot;https://www.baidu.com/&quot;)
print r.text

</code></pre>

<p>运行结果：</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;百度一下，你就知道 ....

</code></pre>

<ul>
<li>如果SSL证书验证不通过，或者不信任服务器的安全证书，则会报出SSLError，据说 12306 证书是自己做的：</li>
</ul>

<p>来测试一下：</p>

<pre><code>import requests
response = requests.get(&quot;https://www.12306.cn/mormhweb/&quot;)
print response.text

</code></pre>

<p>果然：</p>

<p><code>SSLError: (&quot;bad handshake: Error([(&#39;SSL routines&#39;, &#39;ssl3_get_server_certificate&#39;, &#39;certificate verify failed&#39;)],)&quot;,)</code></p>

<p><strong>如果我们想跳过 12306 的证书验证，把 verify 设置为 False 就可以正常请求了。</strong></p>

<pre><code>r = requests.get(&quot;https://www.12306.cn/mormhweb/&quot;, verify = False)
</code></pre>

]]></content>
  </entry>
  
</feed>
