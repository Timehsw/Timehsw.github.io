<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Time渐行渐远]]></title>
  <link href="http://dmlcoding.com/atom.xml" rel="self"/>
  <link href="http://dmlcoding.com/"/>
  <updated>2018-03-14T14:34:52+08:00</updated>
  <id>http://dmlcoding.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[大数据框架-开源协议]]></title>
    <link href="http://dmlcoding.com/15209079363420.html"/>
    <updated>2018-03-13T10:25:36+08:00</updated>
    <id>http://dmlcoding.com/15209079363420.html</id>
    <content type="html"><![CDATA[
<p>知道我们经常用的开源框架的开源协议么?</p>

<span id="more"></span><!-- more -->

<table>
<thead>
<tr>
<th>开源项目</th>
<th>开源协议</th>
</tr>
</thead>

<tbody>
<tr>
<td>Superset</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Yanagishima</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Kibana</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Spark</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Tensorflow</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>PlanOut</td>
<td>BSD License</td>
</tr>
<tr>
<td>Hadoop</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Hive</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Sqoop</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Impala</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Presto</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Hbase</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Mysql</td>
<td>GPL license</td>
</tr>
<tr>
<td>Elasticsearch</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>CDH</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Zookeeper</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Hue</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Oozie</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Flume</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Kafka</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Logstash</td>
<td>Apache License 2.0</td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zookeeper笔记]]></title>
    <link href="http://dmlcoding.com/15205689785115.html"/>
    <updated>2018-03-09T12:16:18+08:00</updated>
    <id>http://dmlcoding.com/15205689785115.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">基础知识</h2>

<h3 id="toc_1">znode</h3>

<h3 id="toc_2">基础api</h3>

<pre><code># 创建一个test_node节点,并包含数据testdata
[zk: localhost:2181(CONNECTED) 12] create /test_note testdata
Created /test_note
[zk: localhost:2181(CONNECTED) 13] ls /test_note
[]
[zk: localhost:2181(CONNECTED) 14] get /test_note
testdata
cZxid = 0x1cc0015709f
ctime = Fri Mar 09 12:15:41 CST 2018
mZxid = 0x1cc0015709f
mtime = Fri Mar 09 12:15:41 CST 2018
pZxid = 0x1cc0015709f
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 8
numChildren = 0

[zk: localhost:2181(CONNECTED) 15] delete /test_note
[zk: localhost:2181(CONNECTED) 16] get /test_note
Node does not exist: /test_note

</code></pre>

<h2 id="toc_3">主-从模式的例子</h2>

<pre><code># -e 表示临时性
# 创建一个临时性的znode,节点名称为master_example ,数据为master1.example.com:2223
# 一个临时节点会在会话过期或关闭时自动被删除
create -e /master_example &quot;master1.example.com:2223&quot;

# 列出目录树
ls /

# 获取master_example znode的元数据和数据
get /master_example
</code></pre>

<p>如果再次执行创建master_example的命令会报<code>Node already exists: /master_example</code></p>

<pre><code># 在master_example上设置一个监视点
stat /master_example true
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[分布式应用浅谈]]></title>
    <link href="http://dmlcoding.com/15205677979326.html"/>
    <updated>2018-03-09T11:56:37+08:00</updated>
    <id>http://dmlcoding.com/15205677979326.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">分布式应用的架构</h2>

<h2 id="toc_1">分布式应用的需求</h2>

<h2 id="toc_2">分布式应用面临的问题</h2>

<h2 id="toc_3">分布式应用的解决方案</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kafka-sparkstreaming自己维护offset]]></title>
    <link href="http://dmlcoding.com/15203071412162.html"/>
    <updated>2018-03-06T11:32:21+08:00</updated>
    <id>http://dmlcoding.com/15203071412162.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">auto.offset.reset</h2>

<p>By default, it will start consuming from the latest offset of each Kafka partition. If you set configuration auto.offset.reset in Kafka parameters to smallest, then it will start consuming from the smallest offset.</p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">OffsetRange</h2>

<p>topic主题,分区ID,起始offset,结束offset</p>

<h2 id="toc_2">重写思路</h2>

<p>因为spark源码中<code>KafkaCluster</code>类被限制在<strong>[spark]</strong>包下,所以我们如果想要在项目中调用这个类,那么只能在项目中也新建包<code>org.apache.spark.streaming.kafka</code>.然后再该包下面写调用的逻辑.这里面就可以引用<code>KafkaCluster</code>类了.这个类里面封装了很多实用的方法,比如:获取主题和分区,获取offset等等...</p>

<p>这些api,spark里面都有现成的,我们现在就是需要组织起来!</p>

<pre><code>offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
</code></pre>

<p>简单说一下<br/>
1. 在zookeeper上读取offset前先根据实际情况更新<code>fromOffsets</code><br/>
    1.1 如果是一个新的groupid,那么会从最新的开始读<br/>
    1.2 如果是存在的groupid,根据配置<code>auto.offset.reset</code><br/>
        1.2.1 <code>smallest</code> : 那么会从开始读,获取最开始的offset.<br/>
        1.2.2 <code>largest</code> : 那么会从最新的位置开始读取,获取最新的offset.</p>

<ol>
<li>根据topic获取topic和该topics下所有的partitions</li>
</ol>

<pre><code>val partitionsE = kc.getPartitions(topics)
</code></pre>

<ol>
<li>传入上面获取到的topics和该分区所有的partitions</li>
</ol>

<pre><code>val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions)
</code></pre>

<ol>
<li>获取到该topic下所有分区的offset了.最后还是调用spark中封装好了的api</li>
</ol>

<pre><code>KafkaUtils.createDirectStream[K, V, KD, VD, (K, V, String)](
                ssc, kafkaParams, consumerOffsets, (mmd: MessageAndMetadata[K, V]) =&gt; ( mmd.key, mmd.message, mmd.topic))
</code></pre>

<ol>
<li>更新zookeeper中的kafka消息的偏移量</li>
</ol>

<pre><code>kc.setConsumerOffsets(groupId, Map((topicAndPartition, offsets.untilOffset)))
</code></pre>

<h2 id="toc_3">问题</h2>

<p>sparkstreaming-kafka的源码中是自己把offset维护在kafka集群中了?</p>

<pre><code>./kafka-consumer-groups.sh --bootstrap-server 10.10.25.13:9092 --describe  --group heheda
</code></pre>

<p>因为用命令行工具可以查到,这个工具可以查到基于javaapi方式的offset,查不到在zookeeper中的</p>

<p>网上的自己维护offset,是把offset维护在zookeeper中了?<br/>
用这个方式产生的groupid,在命令行工具中查不到,但是也是调用的源码中的方法呢?<br/>
难道spark提供了这个方法,但是自己却没有用是吗?</p>

<h2 id="toc_4">自己维护和用原生的区别</h2>

<p>区别只在于,自己维护offset,会先去zk中获取offset,逻辑处理完成后再更新zk中的offset.<br/>
然而,在代码层面,区别在于调用了不同的<code>KafkaUtils.createDirectStream</code></p>

<h3 id="toc_5">自己维护</h3>

<p>自己维护的offset,这个方法会传入offset.因为在此之前我们已经从zk中获取到了起始offset</p>

<pre><code>KafkaUtils.createDirectStream[K, V, KD, VD, (K, V, String)](
                ssc, kafkaParams, consumerOffsets, (mmd: MessageAndMetadata[K, V]) =&gt; ( mmd.key, mmd.message, mmd.topic))
</code></pre>

<h3 id="toc_6">原生的</h3>

<p>接受的是一个topic,底层会根据topic去获取存储在kafka中的起始offset</p>

<pre><code>KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, myTopic)
</code></pre>

<p>接下来这个方法里面会调用<code>getFromOffsets</code>来获取起始的offset</p>

<pre><code>val kc = new KafkaCluster(kafkaParams)
val fromOffsets = getFromOffsets(kc, kafkaParams, topics)
</code></pre>

<h2 id="toc_7">代码</h2>

<p>这个代码,网上很多,GitHub上也有现成的了.这里我就不贴出来了!<br/>
这里主要还是学习代码的实现思路!</p>

<h2 id="toc_8">如何引用spark源码中限制了包的代码</h2>

<ol>
<li>新建和源码中同等的包名,如上所述.</li>
<li>把你需要的源码拷贝一份出来,但是可能源码里面又引用了别的,这个不一定好使.</li>
<li>在你需要引用的那个类里,把这个类的包名改成与你需要引用的包名一样.最简单的办法了</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[查看linux系统负荷]]></title>
    <link href="http://dmlcoding.com/15202200627560.html"/>
    <updated>2018-03-05T11:21:02+08:00</updated>
    <id>http://dmlcoding.com/15202200627560.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">uptime</h1>

<pre><code>~ uptime
17:54:40 up 846 days, 55 min, 10 users,  load average: 0.36, 0.33, 0.48
</code></pre>

<span id="more"></span><!-- more -->

<ul>
<li>这行信息的后半部部分,显示 **load average.  **它的意思是系统的平均负荷.</li>
<li>通过里面的三个数字,我们可以从中判断系统负荷是大还是小</li>
<li>这3个数字,分别代表1分钟,5分钟,15分钟内系统的平均负荷.</li>
<li>当CPU完全空闲的时候,平均负荷为0;当CPU工作量饱和的时候,平均负荷为1(假设只有一个核)</li>
<li>那么很显然,load average的值越低,比如0.2或者0.3,就说明电脑的工作量越小,系统负荷比较轻</li>
</ul>

<p>参考文档</p>

<p><a href="http://blog.scoutapp.com/articles/2009/07/31/understanding-load-averages">http://blog.scoutapp.com/articles/2009/07/31/understanding-load-averages</a></p>

<p>总结:</p>

<p>假设服务器有16个核.</p>

<p>那么当load average的值小于16的时候,表示现在系统负荷轻.当load average的值大于16就表示负荷过大了.</p>

<p>查看服务器有多少个CPU核心</p>

<p><u><strong>grep -c &#39;model name&#39; /proc/cpuinfo</strong></u></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MacbookPro 修改应用的默认语言]]></title>
    <link href="http://dmlcoding.com/15202199373374.html"/>
    <updated>2018-03-05T11:18:57+08:00</updated>
    <id>http://dmlcoding.com/15202199373374.html</id>
    <content type="html"><![CDATA[
<p>当我把macbook pro的默认系统语言从中文改成英文后,其他的应用的语言也都变成了英文了.</p>

<p>但是比如地图应用,我还是希望用中文呢?</p>

<h1 id="toc_0">将地图Maps应用的语言改成中文</h1>

<span id="more"></span><!-- more -->

<p>第一步</p>

<pre><code>You don&#39;t need to change language setting. 
You can simply set it in the Maps App&#39;s Menu at [View]-&gt; [Labels]-&gt; [Always Show Labels in English].
 Then cancel it.
</code></pre>

<p>第二步</p>

<pre><code>defaults write com.apple.Maps AppleLanguages &#39;(zh-CN)&#39;
</code></pre>

<hr/>

<pre><code># 英文 -&gt; 简体中文
defaults write com.google.Chrome AppleLanguages &#39;(zh-CN)&#39;

# 简体中文 -&gt; 英文
defaults write com.google.Chrome AppleLanguages &#39;(en-US)&#39;

# 英文优先，简体中文第二。反之改一下顺序
defaults write com.google.Chrome AppleLanguages &quot;(en-US,zh-CN)&quot;
</code></pre>

<h1 id="toc_1">Calendar</h1>

<pre><code>defaults write com.apple.iCal AppleLanguages &#39;(zh-CN)&#39;
</code></pre>

<p>那么问题来了,从上面可以知道只要修改每个应用的系统语言即可</p>

<h1 id="toc_2">如何找到应用的包名</h1>

<p>以<strong>滴答清单</strong>为例</p>

<pre><code># 在&quot;&quot;双引号中输入应用名称,执行下面的命令即可返回应用的包名
hushiwei@hsw ~  osascript -e &#39;id of app &quot;Wunderlist&quot;&#39;
com.wunderkinder.wunderlistdesktop
</code></pre>

<p>知道了应用的包名,那么就可以执行上面的命令来修改应用的语言了</p>

<pre><code>defaults write com.wunderkinder.wunderlistdesktop AppleLanguages &#39;(zh-CN)&#39;
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[思考]]></title>
    <link href="http://dmlcoding.com/15199545224004.html"/>
    <updated>2018-03-02T09:35:22+08:00</updated>
    <id>http://dmlcoding.com/15199545224004.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">为什么有些人能够突飞猛进?</h2>

<ul>
<li>智商高</li>
<li>有想法,有思路</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kafka-sparkstreaming长时间运行后异常退出原因分析]]></title>
    <link href="http://dmlcoding.com/15199040835190.html"/>
    <updated>2018-03-01T19:34:43+08:00</updated>
    <id>http://dmlcoding.com/15199040835190.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">报错信息</h2>

<pre><code>Couldn`t find leader offsets for set([xxxxxxxxxxx,xx])

# 或者
numRecords must not be negative
</code></pre>

<span id="more"></span><!-- more -->

<h2 id="toc_1">分析过程</h2>

<p>从博客中已经找不到直接的解决方案，那么只能从源码入手，从而看看能不能定位到问题所在。<br/>
从这一行代码开始跟踪</p>

<pre><code>KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, myTopic)

</code></pre>

<p>发现了,先调用<code>getFromOffsets(kc, kafkaParams, topics)</code>获取到了起始offset.然后创建了一个<br/>
<code>DirectKafkaInputDStream</code>对象.点进去</p>

<pre><code>  def createDirectStream[
    K: ClassTag,
    V: ClassTag,
    KD &lt;: Decoder[K]: ClassTag,
    VD &lt;: Decoder[V]: ClassTag] (
      ssc: StreamingContext,
      kafkaParams: Map[String, String],
      topics: Set[String]
  ): InputDStream[(K, V)] = {
    val messageHandler = (mmd: MessageAndMetadata[K, V]) =&gt; (mmd.key, mmd.message)
    val kc = new KafkaCluster(kafkaParams)
    val fromOffsets = getFromOffsets(kc, kafkaParams, topics)
    new DirectKafkaInputDStream[K, V, KD, VD, (K, V)](
      ssc, kafkaParams, fromOffsets, messageHandler)
  }

</code></pre>

<p>找到这个对象里面的<code>compute</code>方法,重点从这个方法的逻辑开始分析</p>

<pre><code>  override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {
    val untilOffsets = clamp(latestLeaderOffsets(maxRetries))
    val rdd = KafkaRDD[K, V, U, T, R](
      context.sparkContext, kafkaParams, currentOffsets, untilOffsets, messageHandler)

    // Report the record number and metadata of this batch interval to InputInfoTracker.
    val offsetRanges = currentOffsets.map { case (tp, fo) =&gt;
      val uo = untilOffsets(tp)
      OffsetRange(tp.topic, tp.partition, fo, uo.offset)
    }
    val description = offsetRanges.filter { offsetRange =&gt;
      // Don&#39;t display empty ranges.
      offsetRange.fromOffset != offsetRange.untilOffset
    }.map { offsetRange =&gt;
      s&quot;topic: ${offsetRange.topic}\tpartition: ${offsetRange.partition}\t&quot; +
        s&quot;offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}&quot;
    }.mkString(&quot;\n&quot;)
    // Copy offsetRanges to immutable.List to prevent from being modified by the user
    val metadata = Map(
      &quot;offsets&quot; -&gt; offsetRanges.toList,
      StreamInputInfo.METADATA_KEY_DESCRIPTION -&gt; description)
    val inputInfo = StreamInputInfo(id, rdd.count, metadata)
    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)

    currentOffsets = untilOffsets.map(kv =&gt; kv._1 -&gt; kv._2.offset)
    Some(rdd)
  }
</code></pre>

<p>分析过程不仔细说了,感兴趣的在compute方法处打个断点,跟踪一下相关变量的值,应该就能明白一个大概了.</p>

<h2 id="toc_2">解决办法</h2>

<p>综上所述,解决办法为添加以下参数:</p>

<pre><code>--conf spark.streaming.kafka.maxRetries=50 \
--conf spark.yarn.maxAppAttempts=10 \
--conf spark.yarn.am.attemptFailuresValidityInterval=1h \
</code></pre>

<p>注意:<br/>
1. spark.yarn.maxAppAttempts参数不能超过hadoop集群yarn.resourcemanager.am.max-attempts的最大值，如果超过将会取两者较小值<br/>
2. 由于kafka切换leader导致的数据丢失,然后造成sparkstreaming程序报错,异常退出的情况下.我这通通常都是重启kafka,删除checkpoint,再重启sparkstreaming了.<br/>
3. 后面我们不用原生的方式来存储offset.而是自己维护offset,那么以后更新迭代后就不用再删除checkpoint了,对业务的影响也最小了.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch：用Curator辅助Marvel，实现自动删除旧marvel索引]]></title>
    <link href="http://dmlcoding.com/15198768495613.html"/>
    <updated>2018-03-01T12:00:49+08:00</updated>
    <id>http://dmlcoding.com/15198768495613.html</id>
    <content type="html"><![CDATA[
<p>Marvel几乎是所有Elasticsearch用户的标配。Marvel保留观测数据的代价是，<br/>
它默认每天会新建一个index，命名规律像是这样：.marvel-2017-12-10。<br/>
marvel自建的索引一天可以产生大概500M的数据，而且将会越来越多，占的容量也将越来越大。<br/>
有没有什么办法能让它自动过期？比如说只保留最近两天的观测数据，其他的都抛弃掉。</p>

<p>当然有办法,curator就可以帮你实现.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">curator是什么？</h1>

<p>它是一个命令，可以帮助你管理你在Elasticsearch中的索引，帮你删除，关闭（close），<br/>
打开（open）它们。当然这是比较片面的说法，更完整的说明见：<br/>
<a href="https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html">https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html</a></p>

<h1 id="toc_1">实践</h1>

<p>我们集群里面安装的Elasticsearch的版本是2.1.1.<br/>
按照官网,我装了最新的5.x版本,显示版本不对.<br/>
按照 <a href="http://blog.csdn.net/hereiskxm/article/details/47423715">http://blog.csdn.net/hereiskxm/article/details/47423715</a>  这个博客,我装了3.3.0版本.<br/>
显示也不对.</p>

<p>然后我搜了一下,感觉应该装一个中间的版本,因此我安装了4.0.0版本<br/>
<code><br/>
pip install elasticsearch-curator (4.0.0)<br/>
</code></p>

<p>然后我看了一下这个版本提供的参数</p>

<pre><code> curator --help
Usage: curator [OPTIONS] ACTION_FILE

  Curator for Elasticsearch indices.

  See http://elastic.co/guide/en/elasticsearch/client/curator/current

Options:
  --config PATH  Path to configuration file. Default: ~/.curator/curator.yml
  --dry-run      Do not perform any changes.
  --version      Show the version and exit.
  --help         Show this message and exit.
</code></pre>

<p>和我安装最新的5.X的版本看起来是一致的.正好在这个站点看到配置的办法<br/>
<a href="https://stackoverflow.com/questions/33430055/removing-old-indices-in-elasticsearch/42268400#42268400">https://stackoverflow.com/questions/33430055/removing-old-indices-in-elasticsearch/42268400#42268400</a></p>

<p>之前在博客里面看到的那个3.3.0版本,还不兼容呢.</p>

<h2 id="toc_2">用法</h2>

<blockquote>
<p>目的是删除2天前以.marvel开头的索引</p>
</blockquote>

<p>新建目录 /opt/curator</p>

<pre><code>~ pwd
/opt/curator
~ ll
total 12
-rw-r--r-- 1 root root  184 Dec 12 10:48 config_file.yml
-rw-r--r-- 1 root root 1311 Dec 12 10:37 delete_marvel_indices.yml
drwxr-xr-x 2 root root 4096 Dec 12 10:49 logs
</code></pre>

<h2 id="toc_3">config_file.yml</h2>

<pre><code># 记住,这个logfile得提前新建好.不然会启动报错.
vim config_file.yml

---
client:
  hosts:
   - 10.10.25.217
  port: 9200
logging:
  loglevel: INFO
  logfile: &quot;/opt/curator/logs/actions.log&quot;
  logformat: default
  blacklist: [&#39;elasticsearch&#39;, &#39;urllib3&#39;]

</code></pre>

<h2 id="toc_4">delete_marvel_indices.yml</h2>

<p>删除以.marvel前缀且是2天之前的索引</p>

<h2 id="toc_5">```</h2>

<h1 id="toc_6">Remember, leave a key empty if there is no value.  None will be a string,</h1>

<h1 id="toc_7">not a Python &quot;NoneType&quot;</h1>

<h1 id="toc_8">Also remember that all examples have &#39;disable_action&#39; set to True.  If you</h1>

<h1 id="toc_9">want to use this action as a template, be sure to set this to False after</h1>

<h1 id="toc_10">copying it.</h1>

<p>actions:<br/>
  1:<br/>
    action: delete_indices<br/>
    description: &gt;-<br/>
      Delete indices older than 30 days (based on index name), for rc- prefixed indices.<br/>
    options:<br/>
      ignore_empty_list: True<br/>
      timeout_override:<br/>
      continue_if_exception: False<br/>
      disable_action: False<br/>
    filters:<br/>
    - filtertype: pattern<br/>
      kind: prefix<br/>
      value: rc-<br/>
      exclude:<br/>
    - filtertype: age<br/>
      source: name<br/>
      direction: older<br/>
      timestring: &#39;%Y.%m.%d&#39;<br/>
      unit: days<br/>
      unit_count: 30<br/>
      exclude:<br/>
  2:<br/>
    action: delete_indices<br/>
    description: &gt;-</p>

<pre><code>  Delete indices older than 2 days (based on index name), for .marvel prefixed indices.
options:
  ignore_empty_list: True
  timeout_override:
  continue_if_exception: False
  disable_action: False
filters:
- filtertype: pattern
  kind: prefix
  value: .marvel
  exclude:
- filtertype: age
  source: name
  direction: older
  timestring: &#39;%Y.%m.%d&#39;
  unit: days
  unit_count: 2
  exclude:
</code></pre>

<pre><code>
配置完成.

## 执行命令

</code></pre>

<p>curator --config config_file.yml [--dry-run] delete_marvel_indices.yml<br/>
```</p>

<p>注意:<br/>
1. --dry-run 是可选参数,加上后不会真的删除,只会执行逻辑.你可以通过看日志来判断是否正确.<br/>
确认正确后,去掉--dry-run参数,再执行命令,既是真正的执行删除了.<br/>
2. 如果没有在config_file.yml里面配置logfile参数,那么日志会在console打印出来.</p>

<h1 id="toc_11">配置日常任务</h1>

<p>很明显,我们需要自动化这个过程,让它每天自动执行,因此写一个脚本,让crontab每天自动调用即可</p>

<pre><code>#!/bin/bash

curator --config /opt/curator/config_file.yml  /opt/curator/delete_marvel_indices.yml

echo &quot;delete success&quot;

</code></pre>

<p>配置crontab</p>

<pre><code># 每天2点执行删除脚本
0 2 * * * source /etc/profile;bash /opt/curator/delete_marvel_daily.sh &gt; /opt/curator/delete.log 2&gt;&amp;1

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PythonVirtualenv总结]]></title>
    <link href="http://dmlcoding.com/15198743666304.html"/>
    <updated>2018-03-01T11:19:26+08:00</updated>
    <id>http://dmlcoding.com/15198743666304.html</id>
    <content type="html"><![CDATA[
<p>我的电脑是macbookpro.我在电脑里面分别装了python2.7和python3.6.</p>

<p>当我用pip安装了virtual后,我如果想要对应版本的python</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">virtualenv</h1>

<h2 id="toc_1">安装</h2>

<pre><code>pip install virtualenv
</code></pre>

<h2 id="toc_2">virtualenv的参数</h2>

<pre><code>hushiwei@hsw  ~/virtual  virtualenv
You must provide a DEST_DIR
Usage: virtualenv [OPTIONS] DEST_DIR

Options:
  --version             show program&#39;s version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Increase verbosity.
  -q, --quiet           Decrease verbosity.
  -p PYTHON_EXE, --python=PYTHON_EXE
                        The Python interpreter to use, e.g.,
                        --python=python2.5 will use the python2.5 interpreter
                        to create the new environment.  The default is the
                        interpreter that virtualenv was installed with
                        (/usr/local/opt/python3/bin/python3.6)
  --clear               Clear out the non-root install and start from scratch.
  --no-site-packages    DEPRECATED. Retained only for backward compatibility.
                        Not having access to global site-packages is now the
                        default behavior.
  --system-site-packages
                        Give the virtual environment access to the global
                        site-packages.
  --always-copy         Always copy files rather than symlinking.
  --unzip-setuptools    Unzip Setuptools when installing it.
  --relocatable         Make an EXISTING virtualenv environment relocatable.
                        This fixes up scripts and makes all .pth files
                        relative.
  --no-setuptools       Do not install setuptools in the new virtualenv.
  --no-pip              Do not install pip in the new virtualenv.
  --no-wheel            Do not install wheel in the new virtualenv.
  --extra-search-dir=DIR
                        Directory to look for setuptools/pip distributions in.
                        This option can be used multiple times.
  --download            Download preinstalled packages from PyPI.
  --no-download, --never-download
                        Do not download preinstalled packages from PyPI.
  --prompt=PROMPT       Provides an alternative prompt prefix for this
                        environment.
  --distribute          DEPRECATED. Retained only for backward compatibility.
                        This option has no effect.
</code></pre>

<p>可以看到里面的--python参数可以指定虚拟环境的python版本.并且说明了默认是python3.6</p>

<h2 id="toc_3">创建虚拟环境</h2>

<h3 id="toc_4">virtual安装Python2.7</h3>

<pre><code>virtualenv --python=python python2env
</code></pre>

<h3 id="toc_5">virtual安装Python3.6环境</h3>

<pre><code>virtualenv --python=python3 python3env
</code></pre>

<h2 id="toc_6">激活进入虚拟环境</h2>

<pre><code>source python2env/bin/activate
source python3env/bin/activate
</code></pre>

<h2 id="toc_7">退出虚拟环境</h2>

<pre><code>deactivate
</code></pre>

<h2 id="toc_8">删除虚拟环境</h2>

<pre><code># 直接用rm删除目录就是删除虚拟环境了
rm -rf 虚拟环境的目录名称
</code></pre>

<h2 id="toc_9">激活虚拟环境,将全部依赖写入文件</h2>

<pre><code>pip freeze &gt; requirements.txt
</code></pre>

<p>进入项目内,安装全部依赖</p>

<pre><code>pip install -r requirements.txt
</code></pre>

<h1 id="toc_10">virtualenvwrapper</h1>

<blockquote>
<p>virtualenvwrapper是virtualenv的扩展管理包，用于更方便管理虚拟环境.</p>
</blockquote>

<p>他可以做;</p>

<ol>
<li>将所有虚拟环境整合在一个目录下</li>
<li>管理（新增，删除，复制）虚拟环境</li>
<li>切换虚拟环境</li>
</ol>

<h2 id="toc_11">安装</h2>

<pre><code>pip install virtualenvwrapper
</code></pre>

<h2 id="toc_12">使用方法</h2>

<p>1.初始化配置</p>

<p>默认virtualenvwrapper安装在/usr/local/bin下面，实际上需要运行virtualenvwrapper.sh文件才行；</p>

<p>所以需要先进行配置一下：</p>

<p>1.1 创建虚拟环境管理目录:</p>

<pre><code>mkdir $HOME/.local/virtualenvs
</code></pre>

<p>1.2 在~/.bash_profile中添加行</p>

<pre><code>export VIRTUALENV_USE_DISTRIBUTE=1        #  总是使用 pip/distribute
export WORKON_HOME=$HOME/.local/virtualenvs       # 所有虚拟环境存储的目录
if [ -e $HOME/.local/bin/virtualenvwrapper.sh ];then
    source $HOME/.local/bin/virtualenvwrapper.sh
else if [ -e /usr/local/bin/virtualenvwrapper.sh ];then
    source /usr/local/bin/virtualenvwrapper.sh
fi
  fi
export PIP_VIRTUALENV_BASE=$WORKON_HOME
export PIP_RESPECT_VIRTUALENV=true
</code></pre>

<p>2.使用方法</p>

<p>所有的命令可使用：<code>virtualenvwrapper --help</code> 进行查看，这里列出几个常用的：</p>

<ul>
<li><p>创建基本环境：mkvirtualenv [环境名]</p></li>
<li><p>删除环境：rmvirtualenv [环境名]</p></li>
<li><p>激活环境：workon [环境名]</p></li>
<li><p>退出环境：deactivate</p></li>
<li><p>列出所有环境：workon 或者 lsvirtualenv -b</p></li>
<li><p>在使用mkvirtualenv命令的时候,-p选项可以指定使用哪一个python环境</p></li>
</ul>

<p>3.举例</p>

<p>安装python2.7</p>

<pre><code>mkvirtualenv -p python python2env
</code></pre>

<p>安装python3.6</p>

<pre><code>mkvirtualenv -p python3 python3env
</code></pre>

<p>查看现在装了几个虚拟环境</p>

<pre><code>hushiwei@hsw  ~  workon
python2env
python3env
</code></pre>

<p>所有命令都可在后面使用<code>--help</code>参数查看具体用法！Enjoy it !</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[机器学习实战之朴素贝叶斯]]></title>
    <link href="http://dmlcoding.com/15198746079468.html"/>
    <updated>2018-03-01T11:23:27+08:00</updated>
    <id>http://dmlcoding.com/15198746079468.html</id>
    <content type="html"><![CDATA[
<p>朴素贝叶斯就是利用先验知识来解决后验概率，因为训练集中我们已经知道了每个单词在类别0和1中的概率，即p(w|c),<br/>
我们就是要利用这个知识去解决在出现这些单词的组合情况下，类别更可能是0还是1,即p(c|w)。<br/>
如果说之前的训练样本少，那么这个p(w|c)就更可能不准确，所以样本越多我们会觉得这个p(w|c)越可信。</p>

<span id="more"></span><!-- more -->

<pre><code class="language-python">import os
import sys
from numpy import *
sys.path.append(os.getcwd())
</code></pre>

<pre><code class="language-python">import bayes
</code></pre>

<pre><code class="language-python"># 返回实验样本和类别标签(侮辱类和非侮辱类)
listOPosts,listClasses=bayes.loadDataSet()
</code></pre>

<pre><code class="language-python"># 创建词汇表
# 将实验样本里面的词汇进行去重
def createVocabList(dataSet):
    vocabSet=set([])
    for document in dataSet:
        vocabSet=vocabSet|set(document)
    return list(vocabSet)
</code></pre>

<pre><code class="language-python">myVocabList=createVocabList(listOPosts)
</code></pre>

<pre><code class="language-python"># 将词汇转成特征向量
# 也就是将每一行样本转成特征向量
# 向量的每一元素为1或者0,分别表示词汇表中的单词在输入文档中是否出现
def setOfWordsVec(vocabList,inputSet):
    returnVec=[0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)]=1
        else:print &quot;the word:%s is not in my Vocabulary!&quot; % word
    return returnVec
</code></pre>

<h1 id="toc_0">训练算法:从词向量计算概率</h1>

<p>朴素贝叶斯分类器训练函数</p>

<pre><code class="language-python"># 输入为文档矩阵以及由每篇文档类别标签所构成的向量
def trainNB0(trainMatrix,trainCategory):
    # 文档总数,有几篇文档,在这里也就是有几个一维数组
    numTrainDocs=len(trainMatrix)
    # 每篇文档里面的单词数,也就是一维数组的长度
    numWords=len(trainMatrix[0])
    # 因为就只有0和1两个分类,将类别列表求和后,就是其中一个类别的个数
    # 然后numTrainDocs也就是文档总数,这样相除后就是这个类别的概率了
    pAbusive=sum(trainCategory)/float(numTrainDocs)
    # 以下两行,初始化概率
    p0Num=ones(numWords);p1Num=ones(numWords)
    p0Denom=2.0;p1Denom=2.0

    # 依次遍历所有的文档
    for i in range(numTrainDocs):
        # 判断这个文档所属类别
        if trainCategory[i]==1:
            # 数组与数组相加,这里就是统计每个词在这个分类里面出现的次数
            p1Num+=trainMatrix[i]
            # 统计该类别下,这些词语一共出现了多少次
            p1Denom+=sum(trainMatrix[i])
        else:
            p0Num+=trainMatrix[i]
            p0Denom+=sum(trainMatrix[i])
    # 通过求对数避免数据下溢出
    p1Vect=log(p1Num/p1Denom)
    p0Vect=log(p0Num/p0Denom)
    return p0Vect,p1Vect,pAbusive
</code></pre>

<pre><code class="language-python"># 所有文档的特征向量
trainMat=[]
</code></pre>

<pre><code class="language-python"># 将文档的每一行,转成词向量,然后追加到trainMat中
for postinDoc in listOPosts:
    trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc))
</code></pre>

<pre><code class="language-python">p0V,p1V,pAb=trainNB0(trainMat,listClasses)
</code></pre>

<pre><code class="language-python">pAb
</code></pre>

<pre><code>0.5
</code></pre>

<pre><code class="language-python">p0V
</code></pre>

<pre><code>array([-2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,
       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -2.56494936,
       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,
       -2.15948425, -3.25809654, -3.25809654, -2.56494936, -3.25809654,
       -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,
       -2.56494936, -3.25809654, -2.56494936, -3.25809654, -2.56494936,
       -2.56494936, -1.87180218])
</code></pre>

<pre><code class="language-python">p1V
</code></pre>

<pre><code>array([-3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -2.35137526, -2.35137526, -2.35137526, -3.04452244, -1.94591015,
       -3.04452244, -2.35137526, -2.35137526, -3.04452244, -1.94591015,
       -3.04452244, -1.65822808, -3.04452244, -2.35137526, -3.04452244,
       -3.04452244, -3.04452244])
</code></pre>

<pre><code class="language-python"># 朴素贝叶斯分类函数
def classifyNB(vec2classify,p0Vec,p1Vec,pClass1):
    #元素相乘
    p1=sum(vec2classify*p1Vec)+log(pClass1)
    p0=sum(vec2classify*p0Vec)+log(pClass1)
    if p1&gt;p0:
        return 1
    else:
        return 0
</code></pre>

<pre><code class="language-python">def testingNB():
    listOPosts,listClasses=bayes.loadDataSet()
    myVocabList=createVocabList(listOPosts)
    trainMat=[]
    for postinDoc in listOPosts:
        trainMat.append(setOfWordsVec(myVocabList,postinDoc))

    p0V,p1V,pAb=trainNB0(array(trainMat),array(listClasses))

    testEntry=[&#39;love&#39;,&#39;my&#39;,&#39;dalmation&#39;]
    thisDoc=array(setOfWordsVec(myVocabList,testEntry))
    print testEntry,&#39;classified as : &#39;,classifyNB(thisDoc,p0V,p1V,pAb)

    testEntry=[&#39;stupid&#39;,&#39;garbage&#39;]
    thisDoc=array(setOfWordsVec(myVocabList,testEntry))
    print testEntry,&#39;classified as : &#39;,classifyNB(thisDoc,p0V,p1V,pAb)
</code></pre>

<pre><code class="language-python">testingNB()
</code></pre>

<pre><code>[&#39;love&#39;, &#39;my&#39;, &#39;dalmation&#39;] classified as :  0
[&#39;stupid&#39;, &#39;garbage&#39;] classified as :  1
</code></pre>

<h1 id="toc_1">使用朴素贝叶斯过滤垃圾邮件</h1>

<ul>
<li>收集数据：提供文本文件</li>
<li>准备数据：将文本文件解析成词条向量</li>
<li>分析数据：检查词条确保解析的正确性</li>
<li>训练算法：使用我们之前建立的trainNB0()函数</li>
<li>测试算法：使用classifyNB()</li>
</ul>

<pre><code class="language-python">mySent=&#39;This book is the best book on Python or M.L. I have ever laid eyes upon.&#39;
</code></pre>

<pre><code class="language-python"># 文件解析及完整的垃圾邮件测试函数
def textParse(bigString):
    import re
    listOfTokens=re.split(r&#39;\W*&#39;,bigString)
    return [tok.lower() for tok in listOfTokens if len(tok)&gt;2]
</code></pre>

<pre><code class="language-python">def spamTest():
    docList=[];classList=[];fullText=[]
    for i in range(1,26):
        # 导入邮件文本，并解析成词条
        wordList=textParse(open(&#39;email/spam/%d.txt&#39; %i).read())
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(1)

        wordList=textParse(open(&#39;email/ham/%d.txt&#39; %i).read())
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(0)
    # 生成词汇表
    vocabList=createVocabList(docList)

    # 随机构建训练集、测试集
    trainingSet=range(50);testSet=[]
    for i in range(10):
        randIndex=int(random.uniform(0,len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])

    # 生成测试集的特征向量
    trainMat=[];trainClasses=[]
    for docIndex in trainingSet:
        trainMat.append(setOfWordsVec(vocabList,docList[docIndex]))
        trainClasses.append(classList[docIndex])

    p0V,p1V,pSpam=trainNB0(trainMat,trainClasses)

    # 测试集，测试错误率
    errorCount=0
    for docIndex in testSet:
        wordVector=setOfWordsVec(vocabList,docList[docIndex])
        if classifyNB(wordVector,p0V,p1V,pSpam)!=classList[docIndex]:
            errorCount+=1
    print &#39;the error rate is : &#39;,float(errorCount)/len(testSet)
</code></pre>

<pre><code class="language-python">spamTest()
</code></pre>

<pre><code>the error rate is :  0.2
</code></pre>

<h1 id="toc_2">使用朴素贝叶斯分类器从个人广告中获取区域倾向</h1>

<ul>
<li>收集数据:从RSS源收集内容,这里需要对RSS源构建一个接口</li>
<li>准备数据:将文本文件解析成词条向量</li>
<li>分析数据:检查词条确保解析的正确性</li>
<li>训练算法:使用我们之前建立的trainNB0()函数</li>
<li>测试算法:观察错误率,确保分类器可用.可以修改切分程序,以降低错误率,提高分类结果.</li>
<li>使用算法:构建一个完整的程序,封装所有内容.给定两个RSS源,该程序会显示最常用的公共词.</li>
</ul>

<p>下面将使用来自不同城市的广告训练一个分类器，然后观察分类器的效果。我们的目的并不是使用该分类器进行分类，而是通过观察单词和条件概率值来发现与特定城市相关的内容。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Presto的学习笔记]]></title>
    <link href="http://dmlcoding.com/15198745641385.html"/>
    <updated>2018-03-01T11:22:44+08:00</updated>
    <id>http://dmlcoding.com/15198745641385.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">是什么?可以做什么?</h1>

<ol>
<li>Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节。</li>
<li>Presto支持在线数据查询，包括Hive, Cassandra, 关系数据库以及专有数据存储。 一条Presto查询可以将多个数据源的数据进行合并，可以跨越整个组织进行分析。</li>
<li>作为Hive和Pig（Hive和Pig都是通过MapReduce的管道流来完成HDFS数据的查询）的替代者，Presto不仅可以访问HDFS，也可以操作不同的数据源，包括：RDBMS和其他的数据源（例如：Cassandra）。</li>
<li>查询后的数据自动分页,这个很不错.</li>
</ol>

<span id="more"></span><!-- more -->

<h1 id="toc_1">源码编译</h1>

<p>下载<strong>presto</strong>源码包地址:<a href="https://github.com/prestodb/presto/releases">https://github.com/prestodb/presto/releases</a></p>

<p>安装文档地址(注意这个中文文档的版本是0.100):</p>

<p>注意:</p>

<ul>
<li>jdk得是1.8以上</li>
<li>我是用的presto0.161</li>
</ul>

<pre><code>tar -xzvf presto-0.161.tar.gz

# 编译
./mvnw clean install -DskipTests
</code></pre>

<p>在pom.xml文件中加入阿里云的仓库,加速下载依赖</p>

<pre><code>&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;nexus-aliyun&lt;/id&gt;
        &lt;name&gt;Nexus aliyun&lt;/name&gt;
        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
</code></pre>

<p>编译报错</p>

<pre><code>[ERROR] Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.13:revision (default) on project presto-spi: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml -&gt; [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.13:revision (default) on project presto-spi: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:185)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:181)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.maven.plugin.MojoExecutionException: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml
    at pl.project13.maven.git.GitCommitIdMojo.throwWhenRequiredDirectoryNotFound(GitCommitIdMojo.java:432)
    at pl.project13.maven.git.GitCommitIdMojo.execute(GitCommitIdMojo.java:337)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
</code></pre>

<p>解决办法</p>

<pre><code>pom文件中加入这个插件
&lt;pluginManagement&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;pl.project13.maven&lt;/groupId&gt;
                &lt;artifactId&gt;git-commit-id-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;skip&gt;true&lt;/skip&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/pluginManagement&gt;
</code></pre>

<h1 id="toc_2">部署包安装</h1>

<h2 id="toc_3">下载地址</h2>

<p>下载presto-cli(记住要下载的presto-cli-xxxx-executable.jar):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli">https://repo1.maven.org/maven2/com/facebook/presto/presto-cli</a></p>

<p>下载部署包的地址:<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-server/">https://repo1.maven.org/maven2/com/facebook/presto/presto-server/</a></p>

<h2 id="toc_4">服务器说明</h2>

<p>机器共有三台U006,U007,U008</p>

<ul>
<li>coordinator

<ul>
<li>U007</li>
</ul></li>
<li>discovery

<ul>
<li>U007</li>
</ul></li>
<li>worker

<ul>
<li>U006</li>
<li>U008</li>
</ul></li>
</ul>

<p>coordinator和worker的其他配置都是一样的,除了config.properties不一样.具体哪里不一样,看下面的配置说明.</p>

<h1 id="toc_5">配置说明</h1>

<p>以下配置均是presto0.161版本的.如果你的版本不一样,启动后如果报错了,那么可能是配置文件里面的参数和版本对应不上,请找相应版本的配置.</p>

<p>在presto-server-0.161目录下新建etc目录,下面的配置文件均在此etc目录下</p>

<pre><code>[druid@U007 presto-server-0.161]$ tree etc/
etc/
├── catalog
│   ├── hive.properties
│   └── jmx.properties
├── config.properties
├── jvm.config
├── log.properties
└── node.properties
</code></pre>

<h2 id="toc_6">jvm.config</h2>

<blockquote>
<p>包含一系列在启动JVM的时候需要使用的命令行选项。这份配置文件的格式是：一系列的选项，每行配置一个单独的选项。由于这些选项不在shell命令中使用。 因此即使将每个选项通过空格或者其他的分隔符分开，java程序也不会将这些选项分开，而是作为一个命令行选项处理,信息如下：</p>
</blockquote>

<p>VM 系统属性 <code>HADOOP_USER_NAME</code> 来指定用户名</p>

<pre><code>-server
-Xmx16G
-XX:+UseG1GC
-XX:G1HeapRegionSize=32M
-XX:+UseGCOverheadLimit
-XX:+ExplicitGCInvokesConcurrent
-XX:+HeapDumpOnOutOfMemoryError
-XX:OnOutOfMemoryError=kill -9 %p
-DHADOOP_USER_NAME=hdfs
</code></pre>

<h2 id="toc_7">log.properties</h2>

<blockquote>
<p>这个配置文件中允许你根据不同的日志结构设置不同的日志级别。每个logger都有一个名字（通常是使用logger的类的全标示类名）. Loggers通过名字中的“.“来表示层级和集成关系，信息如下：</p>
</blockquote>

<pre><code>com.facebook.presto=INFO
</code></pre>

<ul>
<li>配置日志等级，类似于log4j。四个等级：DEBUG,INFO,WARN,ERROR</li>
</ul>

<h2 id="toc_8">node.properties</h2>

<blockquote>
<p>包含针对于每个节点的特定的配置信息。 一个节点就是在一台机器上安装的Presto实例，<strong>etc/node.properties</strong>配置文件至少包含如下配置信息</p>
</blockquote>

<pre><code>node.environment=production
node.id=ffffffff-ffff-ffff-ffff-ffffffffffff # 每个节点的node.id一定要不一样
node.data-dir=/home/druid/data/presto # 计算临时存储目录,presto得有读写权限
</code></pre>

<p>说明:</p>

<ol>
<li>node.environment： 集群名称, 所有在同一个集群中的Presto节点必须拥有相同的集群名称.</li>
<li>node.id： 每个Presto节点的唯一标示。每个节点的node.id都必须是唯一的。在Presto进行重启或者升级过程中每个节点的node.id必须保持不变。如果在一个节点上安装多个Presto实例（例如：在同一台机器上安装多个Presto节点），那么每个Presto节点必须拥有唯一的node.id.</li>
<li>node.data-dir： 数据存储目录的位置（操作系统上的路径）, Presto将会把日期和数据存储在这个目录下</li>
</ol>

<h2 id="toc_9">config.properties</h2>

<h3 id="toc_10">coordinator 主节点配置</h3>

<pre><code>coordinator=true
node-scheduler.include-coordinator=false
http-server.http.port=8585
query.max-memory=10GB
discovery-server.enabled=true
discovery.uri=http://U007:8585
</code></pre>

<p>说明:</p>

<ol>
<li><p>coordinator表示此节点是否作为一个coordinator。每个节点可以是一个worker，也可以同时是一个coordinator，但作为性能考虑，一般大型机群最好将两者分开。</p></li>
<li><p>若coordinator设置成true，则此节点成为一个coordinator。</p></li>
<li><p>若node-scheduler.include-coordinator设置成true，则成为一个worker，两者可以同时设置成true，此节点拥有两种身份。在一个节点上的Presto server即作为coordinator又作为worke将会降低查询性能。因为如果一个服务器作为worker使用，那么大部分的资源都会被worker占用，那么就不会有足够的资源进行关键任务调度、管理和监控查询执行.</p></li>
<li><p>http-server.http.port：指定HTTP server的端口。Presto 使用 HTTP进行内部和外部的所有通讯.</p></li>
<li><p>query.max-memory=10GB：一个单独的任务使用的最大内存 (一个查询计划的某个执行部分会在一个特定的节点上执行)。 这个配置参数限制的GROUP BY语句中的Group的数目、JOIN关联中的右关联表的大小、ORDER BY语句中的行数和一个窗口函数中处理的行数。 该参数应该根据并发查询的数量和查询的复杂度进行调整。如果该参数设置的太低，很多查询将不能执行；但是如果设置的太高将会导致JVM把内存耗光.</p></li>
<li><p>discovery-server.enabled：Presto 通过Discovery 服务来找到集群中所有的节点。为了能够找到集群中所有的节点，每一个Presto实例都会在启动的时候将自己注册到discovery服务。Presto为了简化部署，并且也不想再增加一个新的服务进程，Presto coordinator 可以运行一个内嵌在coordinator 里面的Discovery 服务。这个内嵌的Discovery 服务和Presto共享HTTP server并且使用同样的端口.</p></li>
<li><p>discovery.uri：Discovery server的URI。由于启用了Presto coordinator内嵌的Discovery 服务，因此这个uri就是Presto coordinator的uri。注意：这个URI一定不能以“/“结尾</p></li>
</ol>

<h3 id="toc_11">worker节点配置</h3>

<pre><code>coordinator=false
node-scheduler.include-coordinator=true
http-server.http.port=8585
query.max-memory=5GB
query.max-memory-per-node=1GB
discovery.uri=http://U007:8585
</code></pre>

<h2 id="toc_12">catalog</h2>

<p>hive.properties(hive连接器的配置)</p>

<p>连接hive</p>

<pre><code># 在etc/catalog目录下,新建hive.properties文件,配置上hive的一些信息
[druid@U006 catalog]$ pwd
/home/druid/presto-server-0.161/etc/catalog
[druid@U006 catalog]$ more hive.properties
connector.name=hive-cdh5
hive.metastore.uri=thrift://U006:9083
hive.config.resources=/etc/hadoop/conf.cloudera.yarn/core-site.xml,/etc/hadoop/conf.clouder
a.yarn/hdfs-site.xml
</code></pre>

<p>保证每个节点presto对core-site.xml,hdfs-site.xml两个文件有读权限</p>

<h1 id="toc_13">启动停止presto</h1>

<h2 id="toc_14">单节点启动</h2>

<p>在每个节点依次执行启动脚本</p>

<pre><code># 后台运行
bin/launcher start
# 前台运行
bin/launcher run

# 重启presto
bin/launcher restart

# 停止presto
bin/launcher stop
</code></pre>

<h2 id="toc_15">批量启动停止脚本</h2>

<pre><code>ssh -t ${i} -C &#39;. /usr/local/bin/env.sh &amp;&amp; /usr/local/presto-server-0.161/bin/launcher restart&#39;
</code></pre>

<h1 id="toc_16">监控presto</h1>

<p>启动完成后,在浏览器输入:</p>

<pre><code>http://U007:8585
</code></pre>

<p>这个地址也就是coordinator的discovery.uri</p>

<h1 id="toc_17">cli连接</h1>

<h2 id="toc_18">连接器注意说明</h2>

<ul>
<li><p>cli下载地址(找到自己的版本下载):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli">https://repo1.maven.org/maven2/com/facebook/presto/presto-cli</a></p></li>
<li><p>连接器的配置文件必须是以<code>.properties</code>后缀结尾的,前面的名字就是连接器的catalog名字</p></li>
<li><p>每次新加连接器配置文件后,都需要在presto的所有机器上加上相同的配置文件,然后重启</p></li>
<li><p>要下载对应版本的cli连接器,不然可能不好使.名字类似<code>presto-cli-0.161-executable.jar</code></p></li>
</ul>

<h2 id="toc_19">hive连接器</h2>

<p>配置说明(hive连接器的配置在说catalog的时候已经配置好了,你可以回头看看):</p>

<ul>
<li>connector.name=hive-cdh5(根据你的hive版本来选择)</li>
<li>hive.metastore.uri=thrift://U006:9083(hive的metastore地址)</li>
<li>hive.config.resources=/etc/hadoop/conf.cloudera.yarn/core-(配置文件的地址)</li>
</ul>

<pre><code>chmod +x presto-cli-0.161-executable.jar
./presto-cli-0.161-executable.jar --server U007:8585 --catalog hive --schema default
或者 mv presto-cli-0.161-executable.jar presto都可以
./presto --server U007:8585 --catalog hive --schema default
执行该语句后在 presto shell 中执行: show tables 查看 hive 中的 default 库下的表。如果出现对应的表，表安装验证成功
</code></pre>

<h2 id="toc_20">jmx连接器</h2>

<ul>
<li>JMX提供了有关JVM中运行的Java虚拟机和软件的信息</li>
<li>jmx连接器用于在presto服务器中查询JMX信息</li>
</ul>

<p>在etc/catalog目录下新建<strong>jmx.properties</strong></p>

<pre><code>connector.name=jmx
</code></pre>

<p>现在连接presto cli以启用JMX插件</p>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog jmx --schema jmx
presto:jmx&gt; show schemas from jmx;
       Schema
--------------------
 current
 history
 information_schema
(3 rows)

Query 20171012_063601_00020_yuhat, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:00 [3 rows, 47B] [39 rows/s, 614B/s]
</code></pre>

<h2 id="toc_21">MySQL连接器</h2>

<p>vim etc/catalog/mysql.properties</p>

<pre><code>connector.name=mysql
connection-url=jdbc:mysql://10.10.25.13:3306
connection-user=root
connection-password=wankatest***
</code></pre>

<p>schema 后面跟的mysql的数据库,</p>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog mysql --schema test
presto:test&gt; show tables;
Query 20171012_071844_00002_iz4q8 failed: No worker nodes available

presto:test&gt; show tables;
          Table
--------------------------
 dmp_summary_daily_report
 tb_dmp_stat_appboot
 tb_dmp_stat_asdk_detail
 tb_dmp_stat_device
 tb_leidian1
 tb_leidian2
(6 rows)

Query 20171012_072024_00003_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:01 [6 rows, 190B] [6 rows/s, 196B/s]
</code></pre>

<h2 id="toc_22">kafka连接器</h2>

<p>暂时没这个需求,未测试.</p>

<h2 id="toc_23">系统连接器</h2>

<ul>
<li>系统连接器提供了正在运行的Presto集群的一些信息和指标</li>
<li>那么这个就可以通过标准sql很方便的查询这些信息</li>
<li>系统连接器不需要配置,已经内置了.我们可以很方便的访问名为<code>system</code>的catalog</li>
</ul>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog system
presto&gt; show schemas from system;
       Schema
--------------------
 information_schema
 jdbc
 metadata
 runtime
(4 rows)

Query 20171012_082437_00019_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:00 [4 rows, 57B] [70 rows/s, 997B/s]
</code></pre>

<p>查询有多少个节点</p>

<pre><code>presto&gt; SELECT * FROM system.runtime.nodes;
                  node_id                  |        http_uri         | node_version | coord
-------------------------------------------+-------------------------+--------------+------
 ffffffff-ffff-ffff-ffff-ffffffffffff-u006 | http://10.10.25.13:8585 | 0.161        | false
 ffffffff-ffff-ffff-ffff-ffffffffffff-u007 | http://10.10.25.14:8585 | 0.161        | true
 ffffffff-ffff-ffff-ffff-ffffffffffff-u008 | http://10.10.25.15:8585 | 0.161        | false
(3 rows)

Query 20171012_082952_00023_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
4:08 [3 rows, 228B] [0 rows/s, 0B/s]
</code></pre>

<h2 id="toc_24">JDBC接口</h2>

<h3 id="toc_25">依赖下载安装</h3>

<ul>
<li>下载地址(找到自己的版本下载):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc">https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc</a></li>
<li><code>presto-jdbc-0.161.jar</code>在jar文件下载之后，将其添加到Java应用程序的classpath中。</li>
<li>我不太喜欢用jar包的方式,那么可以在pom文件加入presto-jdbc的依赖</li>
<li><a href="http://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc">http://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc</a> 找到自己相应的版本</li>
</ul>

<pre><code>&lt;!-- https://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.facebook.presto&lt;/groupId&gt;
    &lt;artifactId&gt;presto-jdbc&lt;/artifactId&gt;
    &lt;version&gt;0.161&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Presto支持的URL格式如下：</p>

<pre><code>jdbc:presto://host:port
jdbc:presto://host:port/catalog
jdbc:presto://host:port/catalog/schema
</code></pre>

<p>例如，可以使用下面的URL来连接运行在U007服务器8585端口上的Presto的mysql catalog中的test schema：</p>

<pre><code>jdbc:presto://10.10.25.14:8585/mysql/test
</code></pre>

<p>这个url就是来连接hive catalog中的default schema</p>

<pre><code>jdbc:presto://10.10.25.14:8585/hive/default
</code></pre>

<h3 id="toc_26">Java代码</h3>

<p><strong>读取mysql下的test库下的所有表</strong></p>

<pre><code>public class PrestoJdbcDemo {

  public static void main(String[] args) throws SQLException, ClassNotFoundException {
    Class.forName(&quot;com.facebook.presto.jdbc.PrestoDriver&quot;);
    Connection connection = DriverManager
        .getConnection(&quot;jdbc:presto://10.10.25.14:8585/mysql/test&quot;, &quot;root&quot;, &quot;wankatest***&quot;);
    Statement stmt = connection.createStatement();
    ResultSet rs = stmt.executeQuery(&quot;show tables&quot;);
    while (rs.next()) {
      System.out.println(rs.getString(1));
    }
    rs.close();
    connection.close();

  }
}
</code></pre>

<p><strong>读取hive下的default库下的所有表</strong></p>

<pre><code>public class PrestoJdbcDemo {

  public static void main(String[] args) throws SQLException, ClassNotFoundException {
    Class.forName(&quot;com.facebook.presto.jdbc.PrestoDriver&quot;);
    Connection connection = DriverManager
        .getConnection(&quot;jdbc:presto://10.10.25.14:8585/hive/default&quot;,&quot;root&quot;,null);
    Statement stmt = connection.createStatement();
    ResultSet rs = stmt.executeQuery(&quot;show tables&quot;);
    while (rs.next()) {
      System.out.println(rs.getString(1));
    }
    rs.close();
    connection.close();

  }
}
</code></pre>

<h1 id="toc_27">不同数据源之间的join</h1>

<p>presto的一个特性就是其支持在不同的数据源之间进行join</p>

<p>当连接presto的客户端的时候,也可以不指定连接器</p>

<p>不同的数据源就用catalog名称指定,然后加上库名表明即可.</p>

<pre><code>./presto --server U007:8585
 show tables from mysql.dsp_test;
</code></pre>

<h1 id="toc_28">presto提供的函数和运算符</h1>

<p>参考文档:<a href="http://prestodb-china.com/docs/current/functions.html">http://prestodb-china.com/docs/current/functions.html</a></p>

<h1 id="toc_29">看日志</h1>

<h2 id="toc_30">日志路径</h2>

<p><strong>node.properties</strong>中配置了node.data-dir=/home/druid/data/presto</p>

<pre><code>[druid@U007 presto]$ tree
.
├── etc -&gt; /home/druid/presto-server-0.161/etc
├── plugin -&gt; /home/druid/presto-server-0.161/plugin
└── var
    ├── log
    │   ├── http-request.log
    │   ├── launcher.log
    │   └── server.log
    └── run
        └── launcher.pid

5 directories, 4 files
</code></pre>

<p>出现错误后,我们主要关注var/log目录下的日志.</p>

<p>当服务有问题的时候,看server.log找到报错原因,从而解决问题.</p>

<h1 id="toc_31">常见错误</h1>

<h2 id="toc_32">连接不上连接器</h2>

<p>类似这样的错误</p>

<pre><code>No factory for connector mysql
No factory for connector hive
</code></pre>

<p>如果服务报相关这样的错误,那么就需要关注各个连接器的配置文件是否写对了.各个连接器的配置文件是否在每个presto服务器上都部署了.</p>

<h1 id="toc_33">Presto的实现原理</h1>

<p><img src="http://oz6wyfxp0.bkt.clouddn.com/1512097348.png?imageMogr2/thumbnail/!70p" alt="Presto架构"/></p>

<ul>
<li>Presto的架构</li>
<li>Presto执行原理</li>
</ul>

<p>简单来说:</p>

<ol>
<li>cli客户端把查询需求发送给Coordinator节点.Coordinator节点负责解析sql语句,生成执行计划,分发执行任务给worer节点执行.所以worker节点负责实际执行查询任务.</li>
<li>worker节点启动后向discovery server服务注册,因此coordinator就可以从discovery server获得可以正常工作的worker节点.</li>
</ol>

<p>深入来说:</p>

<ol>
<li>参考网上相关blog.</li>
<li>买书.</li>
<li>看源码!</li>
</ol>

<h1 id="toc_34">参考文档</h1>

<ul>
<li><p><a href="https://prestodb.io">presto官网</a></p></li>
<li><p><a href="https://github.com/prestodb/presto">presto-Github</a></p></li>
<li><p><a href="http://prestodb-china.com">presto-京东维护-中文文档</a></p></li>
<li><p><a href="https://tech.meituan.com/presto.html">Presto实现原理和美团的使用实践</a></p></li>
<li><p><a href="http://getindata.com/tutorial-using-presto-to-combine-data-from-hive-and-mysql-in-one-sql-like-query/">Presto-Hive-Mysql-InteractiveQuery</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch基本概念]]></title>
    <link href="http://dmlcoding.com/15209969513188.html"/>
    <updated>2018-03-14T11:09:11+08:00</updated>
    <id>http://dmlcoding.com/15209969513188.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Node 与 Cluster</h2>

<p>Elastic 本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。</p>

<p>单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">Index</h2>

<p>Elastic 会索引所有字段，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。</p>

<p>所以，Elastic 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。</p>

<p>下面的命令可以查看当前节点的所有 Index。</p>

<pre><code>
$ curl -X GET &#39;http://localhost:9200/_cat/indices?v&#39;

</code></pre>

<h2 id="toc_2">Document</h2>

<p>Index 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。</p>

<p>Document 使用 JSON 格式表示，下面是一个例子。</p>

<pre><code>
 {
   &quot;user&quot;: &quot;张三&quot;,
   &quot;title&quot;: &quot;工程师&quot;,
   &quot;desc&quot;: &quot;数据库管理&quot;
 }
 
</code></pre>

<p>同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。</p>

<h2 id="toc_3">Type</h2>

<p>Document 可以分组，比如<code>weather</code>这个 Index 里面，可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫做 Type，它是虚拟的逻辑分组，用来过滤 Document。</p>

<p>不同的 Type 应该有相似的结构（schema），举例来说，<code>id</code>字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的<a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/mapping.html">一个区别</a>。性质完全不同的数据（比如<code>products</code>和<code>logs</code>）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。</p>

<p>下面的命令可以列出每个 Index 所包含的 Type。</p>

<pre><code> 
 $ curl &#39;localhost:9200/_mapping?pretty=true&#39;
 
</code></pre>

<p>根据<a href="https://www.elastic.co/blog/index-type-parent-child-join-now-future-in-elasticsearch">规划</a>，Elastic 6.x 版只允许每个 Index 包含一个 Type，7.x 版将会彻底移除 Type。</p>

<h2 id="toc_4">Shards &amp; Replicas</h2>

<p>一个index能够存储非常大量的数据，有时可能会超过单个节点硬件的限制。例如。一个index存储一超过1TB的数据，这对于单个节点有可能是负担不起的。为了解决这个问题，es可以讲index分片到不同的shards中。当你创建一个index，你可以定义你想要的shards数量。每一个shard对于自己来说都是一个拥有所有功能并且有独立的index，可以被放到任何node上。<br/>
分片有两个重要的优势：</p>

<ul>
<li>它允许你水平 分割/扩展 你的内容。</li>
<li>它允许你并行处理不同分片上的数据来提高效率。 为了保证高可用性，可以为每个分片节点设置备份节点。</li>
</ul>

<p>对于如何进行分片以及进行聚合操作时文档是怎样merge的，es都自动管理了，并且对用户是透明的。</p>

<p>在网络环境中，当某个服务失效了，failover机制是非常有效的高可用保障。es也提供了对于index分片的复制。<br/>
复制机制有两个重要特点：</p>

<ul>
<li>它提供了高可用性以防一个shard/node失效。值得注意的是，一个shard复制不要和它本身放在同一个节点。</li>
<li>它通过对所有复制shard的并行搜索来提高你系统的吞吐量。</li>
</ul>

<h1 id="toc_5">探索集群</h1>

<p>现在我们已经让节点和集群运行起来了，下一步怎么和es进行沟通交流呢？幸运的是，es提供了一个非常好用的restAPI。通过api我们可以做这些事情：</p>

<ul>
<li>检查集群，节点和index的健康状况，状态和统计数据。</li>
<li>管理集群、节点和index的数据和元数据。</li>
<li>执行CRUD和一些针对index的搜索操作。</li>
<li>执行高级的搜索操作，例如分页，排序，过滤，脚本，聚合以及其他。</li>
</ul>

<h2 id="toc_6">集群的健康监测</h2>

<p>接下来，我们对集群进行一个基本的简单的健康监测。前文提到了，因为es通过restAPI来进行操作，所以可以使用curl或者postman等。<br/>
检测cluster的健康状况，我们可以使用_cat API。</p>

<pre><code>[hadoop@U006 ~]$ curl &#39;localhost:9200/_cat/health?v&#39;
epoch      timestamp cluster            status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1521000953 12:15:53  test_elasticsearch yellow          1         1     39  39    0    0       39             0                  -                 50.0%
</code></pre>

<p>其中status显示了当前的健康状况。<br/>
status的定义如下：</p>

<ul>
<li>green ： everything is ok。</li>
<li>yellow： 所有数据可用，但是一些备份节点的数据尚未被分配。</li>
<li>red ： 一些数据不可用。</li>
</ul>

<h2 id="toc_7">查看节点的状况</h2>

<pre><code>[hadoop@U006 ~]$ curl &#39;localhost:9200/_cat/nodes?v&#39;
host        ip          heap.percent ram.percent load node.role master name
10.10.25.13 10.10.25.13           11          84 2.54 d         *      node1
</code></pre>

<h2 id="toc_8">查询所有的index</h2>

<pre><code>[hadoop@U006 ~]$ curl &#39;localhost:9200/_cat/indices?v&#39;
health status index                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   .marvel-es-1-2017.10.25   1   1      20639          640      7.5mb          7.5mb
yellow open   .marvel-es-1-2017.10.24   1   1     185123          862     66.6mb         66.6mb
yellow open   .marvel-es-1-2017.10.23   1   1     184771          862     66.2mb         66.2mb
yellow open   .marvel-es-1-2017.10.22   1   1     182271          862     65.6mb         65.6mb
yellow open   test_log                  5   1      84539            0     17.2mb         17.2mb
yellow open   website                   5   1          2            0      7.8kb          7.8kb
</code></pre>

<h1 id="toc_9">新建和删除 Index</h1>

<h2 id="toc_10">新建 Index</h2>

<p>新建 Index，可以直接向 Elastic 服务器发出 PUT 请求。下面的例子是新建一个名叫<code>weather</code>的 Index。</p>

<pre><code> 
 $ curl -X PUT &#39;localhost:9200/weather&#39;
 
</code></pre>

<p>服务器返回一个 JSON 对象，里面的<code>acknowledged</code>字段表示操作成功。</p>

<pre><code> 
 {
   &quot;acknowledged&quot;:true,
   &quot;shards_acknowledged&quot;:true
 }
 
</code></pre>

<h2 id="toc_11">删除 Index</h2>

<p>然后，我们发出 DELETE 请求，删除这个 Index。</p>

<pre><code> 
 $ curl -X DELETE &#39;localhost:9200/weather&#39;
 
</code></pre>

<h1 id="toc_12">数据操作</h1>

<h2 id="toc_13">新增记录</h2>

<p>向指定的 /Index/Type 发送 PUT 请求，就可以在 Index 里面新增一条记录。比如，向<code>/accounts/person</code>发送请求，就可以新增一条人员记录。</p>

<pre><code> 
 $ curl -X PUT &#39;localhost:9200/accounts/person/1&#39; -d &#39;
 {
   &quot;user&quot;: &quot;张三&quot;,
   &quot;title&quot;: &quot;工程师&quot;,
   &quot;desc&quot;: &quot;数据库管理&quot;
 }&#39; 
 
</code></pre>

<p>服务器返回的 JSON 对象，会给出 Index、Type、Id、Version 等信息。</p>

<pre><code> 
 {
   &quot;_index&quot;:&quot;accounts&quot;,
   &quot;_type&quot;:&quot;person&quot;,
   &quot;_id&quot;:&quot;1&quot;,
   &quot;_version&quot;:1,
   &quot;result&quot;:&quot;created&quot;,
   &quot;_shards&quot;:{&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0},
   &quot;created&quot;:true
 }
 
</code></pre>

<p>如果你仔细看，会发现请求路径是<code>/accounts/person/1</code>，最后的<code>1</code>是该条记录的 Id。它不一定是数字，任意字符串（比如<code>abc</code>）都可以。</p>

<p>新增记录的时候，也可以不指定 Id，这时要改成 POST 请求。</p>

<pre><code> 
 $ curl -X POST &#39;localhost:9200/accounts/person&#39; -d &#39;
 {
   &quot;user&quot;: &quot;李四&quot;,
   &quot;title&quot;: &quot;工程师&quot;,
   &quot;desc&quot;: &quot;系统管理&quot;
 }&#39;
 
</code></pre>

<p>上面代码中，向<code>/accounts/person</code>发出一个 POST 请求，添加一个记录。这时，服务器返回的 JSON 对象里面，<code>_id</code>字段就是一个随机字符串。</p>

<pre><code> 
 {
   &quot;_index&quot;:&quot;accounts&quot;,
   &quot;_type&quot;:&quot;person&quot;,
   &quot;_id&quot;:&quot;AV3qGfrC6jMbsbXb6k1p&quot;,
   &quot;_version&quot;:1,
   &quot;result&quot;:&quot;created&quot;,
   &quot;_shards&quot;:{&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0},
   &quot;created&quot;:true
 }
 
</code></pre>

<p>注意，如果没有先创建 Index（这个例子是<code>accounts</code>），直接执行上面的命令，Elastic 也不会报错，而是直接生成指定的 Index。所以，打字的时候要小心，不要写错 Index 的名称。</p>

<h2 id="toc_14">查看记录</h2>

<p>向<code>/Index/Type/Id</code>发出 GET 请求，就可以查看这条记录。</p>

<pre><code> 
 $ curl &#39;localhost:9200/accounts/person/1?pretty=true&#39;
 
</code></pre>

<p>上面代码请求查看<code>/accounts/person/1</code>这条记录，URL 的参数<code>pretty=true</code>表示以易读的格式返回。</p>

<p>返回的数据中，<code>found</code>字段表示查询成功，<code>_source</code>字段返回原始记录。</p>

<pre><code> 
 {
   &quot;_index&quot; : &quot;accounts&quot;,
   &quot;_type&quot; : &quot;person&quot;,
   &quot;_id&quot; : &quot;1&quot;,
   &quot;_version&quot; : 1,
   &quot;found&quot; : true,
   &quot;_source&quot; : {
     &quot;user&quot; : &quot;张三&quot;,
     &quot;title&quot; : &quot;工程师&quot;,
     &quot;desc&quot; : &quot;数据库管理&quot;
   }
 }
 
</code></pre>

<p>如果 Id 不正确，就查不到数据，<code>found</code>字段就是<code>false</code>。</p>

<pre><code> 
 $ curl &#39;localhost:9200/weather/beijing/abc?pretty=true&#39;
 
 {
   &quot;_index&quot; : &quot;accounts&quot;,
   &quot;_type&quot; : &quot;person&quot;,
   &quot;_id&quot; : &quot;abc&quot;,
   &quot;found&quot; : false
 }
 
</code></pre>

<h2 id="toc_15">删除记录</h2>

<p>删除记录就是发出 DELETE 请求。</p>

<pre><code> 
 $ curl -X DELETE &#39;localhost:9200/accounts/person/1&#39;
 
</code></pre>

<p>这里先不要删除这条记录，后面还要用到。</p>

<h2 id="toc_16">更新记录</h2>

<p>更新记录就是使用 PUT 请求，重新发送一次数据。</p>

<pre><code> 
 $ curl -X PUT &#39;localhost:9200/accounts/person/1&#39; -d &#39;
 {
     &quot;user&quot; : &quot;张三&quot;,
     &quot;title&quot; : &quot;工程师&quot;,
     &quot;desc&quot; : &quot;数据库管理，软件开发&quot;
 }&#39; 
 
 {
   &quot;_index&quot;:&quot;accounts&quot;,
   &quot;_type&quot;:&quot;person&quot;,
   &quot;_id&quot;:&quot;1&quot;,
   &quot;_version&quot;:2,
   &quot;result&quot;:&quot;updated&quot;,
   &quot;_shards&quot;:{&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0},
   &quot;created&quot;:false
 }
 
</code></pre>

<p>上面代码中，我们将原始数据从&quot;数据库管理&quot;改成&quot;数据库管理，软件开发&quot;。 返回结果里面，有几个字段发生了变化。</p>

<pre><code> 
 &quot;_version&quot; : 2,
 &quot;result&quot; : &quot;updated&quot;,
 &quot;created&quot; : false
 
</code></pre>

<p>可以看到，记录的 Id 没变，但是版本（version）从<code>1</code>变成<code>2</code>，操作类型（result）从<code>created</code>变成<code>updated</code>，<code>created</code>字段变成<code>false</code>，因为这次不是新建记录。</p>

<h1 id="toc_17">数据查询</h1>

<h2 id="toc_18">返回所有记录</h2>

<p>使用 GET 方法，直接请求<code>/Index/Type/_search</code>，就会返回所有记录。</p>

<pre><code>
 $ curl &#39;localhost:9200/accounts/person/_search&#39;
 
 {
   &quot;took&quot;:2,
   &quot;timed_out&quot;:false,
   &quot;_shards&quot;:{&quot;total&quot;:5,&quot;successful&quot;:5,&quot;failed&quot;:0},
   &quot;hits&quot;:{
     &quot;total&quot;:2,
     &quot;max_score&quot;:1.0,
     &quot;hits&quot;:[
       {
         &quot;_index&quot;:&quot;accounts&quot;,
         &quot;_type&quot;:&quot;person&quot;,
         &quot;_id&quot;:&quot;AV3qGfrC6jMbsbXb6k1p&quot;,
         &quot;_score&quot;:1.0,
         &quot;_source&quot;: {
           &quot;user&quot;: &quot;李四&quot;,
           &quot;title&quot;: &quot;工程师&quot;,
           &quot;desc&quot;: &quot;系统管理&quot;
         }
       },
       {
         &quot;_index&quot;:&quot;accounts&quot;,
         &quot;_type&quot;:&quot;person&quot;,
         &quot;_id&quot;:&quot;1&quot;,
         &quot;_score&quot;:1.0,
         &quot;_source&quot;: {
           &quot;user&quot; : &quot;张三&quot;,
           &quot;title&quot; : &quot;工程师&quot;,
           &quot;desc&quot; : &quot;数据库管理，软件开发&quot;
         }
       }
     ]
   }
 }
 
</code></pre>

<p>上面代码中，返回结果的 <code>took</code>字段表示该操作的耗时（单位为毫秒），<code>timed_out</code>字段表示是否超时，<code>hits</code>字段表示命中的记录，里面子字段的含义如下。</p>

<blockquote>
<ul>
<li><code>total</code>：返回记录数，本例是2条。</li>
<li><code>max_score</code>：最高的匹配程度，本例是<code>1.0</code>。</li>
<li><code>hits</code>：返回的记录组成的数组。</li>
</ul>
</blockquote>

<p>返回的记录中，每条记录都有一个<code>_score</code>字段，表示匹配的程序，默认是按照这个字段降序排列。</p>

<h2 id="toc_19">全文搜索</h2>

<p>Elastic 的查询非常特别，使用自己的<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.5/query-dsl.html">查询语法</a>，要求 GET 请求带有数据体。</p>

<pre><code> 
 $ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
 {
   &quot;query&quot; : { &quot;match&quot; : { &quot;desc&quot; : &quot;软件&quot; }}
 }&#39;
 
</code></pre>

<p>上面代码使用 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.5/query-dsl-match-query.html">Match 查询</a>，指定的匹配条件是<code>desc</code>字段里面包含&quot;软件&quot;这个词。返回结果如下。</p>

<pre><code> 
 {
   &quot;took&quot;:3,
   &quot;timed_out&quot;:false,
   &quot;_shards&quot;:{&quot;total&quot;:5,&quot;successful&quot;:5,&quot;failed&quot;:0},
   &quot;hits&quot;:{
     &quot;total&quot;:1,
     &quot;max_score&quot;:0.28582606,
     &quot;hits&quot;:[
       {
         &quot;_index&quot;:&quot;accounts&quot;,
         &quot;_type&quot;:&quot;person&quot;,
         &quot;_id&quot;:&quot;1&quot;,
         &quot;_score&quot;:0.28582606,
         &quot;_source&quot;: {
           &quot;user&quot; : &quot;张三&quot;,
           &quot;title&quot; : &quot;工程师&quot;,
           &quot;desc&quot; : &quot;数据库管理，软件开发&quot;
         }
       }
     ]
   }
 }
 
</code></pre>

<p>Elastic 默认一次返回10条结果，可以通过<code>size</code>字段改变这个设置。</p>

<pre><code> 
 $ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
 {
   &quot;query&quot; : { &quot;match&quot; : { &quot;desc&quot; : &quot;管理&quot; }},
   &quot;size&quot;: 1
 }&#39;
 
</code></pre>

<p>上面代码指定，每次只返回一条结果。</p>

<p>还可以通过<code>from</code>字段，指定位移。</p>

<pre><code> 
 $ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
 {
   &quot;query&quot; : { &quot;match&quot; : { &quot;desc&quot; : &quot;管理&quot; }},
   &quot;from&quot;: 1,
   &quot;size&quot;: 1
 }&#39;
 
</code></pre>

<p>上面代码指定，从位置1开始（默认是从位置0开始），只返回一条结果。</p>

<h2 id="toc_20">逻辑运算</h2>

<p>如果有多个搜索关键字， Elastic 认为它们是<code>or</code>关系。</p>

<pre><code> 
 $ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
 {
   &quot;query&quot; : { &quot;match&quot; : { &quot;desc&quot; : &quot;软件 系统&quot; }}
 }&#39;
 
</code></pre>

<p>上面代码搜索的是<code>软件 or 系统</code>。</p>

<p>如果要执行多个关键词的<code>and</code>搜索，必须使用<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.5/query-dsl-bool-query.html">布尔查询</a>。</p>

<pre><code> 
 $ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
 {
   &quot;query&quot;: {
     &quot;bool&quot;: {
       &quot;must&quot;: [
         { &quot;match&quot;: { &quot;desc&quot;: &quot;软件&quot; } },
         { &quot;match&quot;: { &quot;desc&quot;: &quot;系统&quot; } }
       ]
     }
   }
 }&#39;
 
</code></pre>

<h2 id="toc_21">使用过滤器</h2>

<p>之前提到的score是一个数字，它用来评价当前返回的文档和我们需要的文档的匹配程度。分数越高的匹配程度越好。<br/>
但是查询并不是总需要产生score，尤其是它们只是用来过滤文档的时候。es能够自动的优化查询并不计算这些无用的score。<br/>
上文中的bool query也支持filter语句。filter语句可以在不改变score的情况下使用查询语句来限定文档。</p>

<pre><code>curl -XPOST &#39;localhost:9200/customer/_search?pretty&#39; -d &#39;
{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;must&quot;: { &quot;match_all&quot;: {} },
      &quot;filter&quot;: {
        &quot;range&quot;: {
          &quot;age&quot;: {
            &quot;gte&quot;: 20000,
            &quot;lte&quot;: 30000
          }
        }
      }
    }
  }
}&#39;
</code></pre>

<h2 id="toc_22">执行聚合</h2>

<p>聚合可以对数据进行分组并对分组进行数据统计。类似于SQL中的group by。在es中，可以一次性返回查询结果和聚合结果。</p>

<pre><code>curl -XPOST &#39;localhost:9200/customer/_search?pretty&#39; -d &#39;
{
  &quot;size&quot;: 0,
  &quot;aggs&quot;: {
    &quot;group_by_name&quot;: {
      &quot;terms&quot;: {
        &quot;field&quot;: &quot;name&quot;
      }
    }
  }
}&#39;
</code></pre>

<h1 id="toc_23">参考链接</h1>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html">ElasticSearch 官方手册</a></li>
<li><a href="https://www.elastic.co/blog/a-practical-introduction-to-elasticsearch">A Practical Introduction to Elasticsearch</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[二八定律与长尾理论]]></title>
    <link href="http://dmlcoding.com/15198769093559.html"/>
    <updated>2018-03-01T12:01:49+08:00</updated>
    <id>http://dmlcoding.com/15198769093559.html</id>
    <content type="html"><![CDATA[
<p>刚入广告行业的时候,有时候会听同事们说长尾啥的.一直不太明白长尾是啥意思.最近看一本书,书里面提到了一个二八定律.<br/>
本着好奇的态度,搜索了一下二八定律<a href="%5B%5D()"></a>,没想到顺带都提到了长尾理论.接着这个机会,好好理解一下二八定律与长尾理论.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">什么是二八定律</h1>

<p>我把wiki上的解释摘过来看看.</p>

<p>帕雷托法则（英语：Pareto principle），也称为二八定律或80/20法则，此法则指在众多现象中，80%的结果取决于20%的原因，<br/>
而这一法则在很多方面被广泛的应用。如80%的劳动成果取决于20%的前期努力等等。<br/>
这个法则最初是意大利经济学家维弗雷多·帕雷托在1906年对意大利20%的人口拥有80%的财产的观察而得出的，<br/>
后来管理学思想家约瑟夫·朱兰和其他人把它概括为帕雷托法则。</p>

<p>所以简单说也就是,最重要的</p>

<h1 id="toc_1">什么是长尾理论</h1>

<p>未完待续</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ELK搭建使用]]></title>
    <link href="http://dmlcoding.com/15198769472354.html"/>
    <updated>2018-03-01T12:02:27+08:00</updated>
    <id>http://dmlcoding.com/15198769472354.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>elk搭建记录,学习资料.</p>
</blockquote>

<h1 id="toc_0">ELK学习资料</h1>

<ul>
<li><a href="https://www.gitbook.com/book/chenryn/elk-stack-guide-cn/details">ELKstack 中文指南</a></li>
<li><a href="https://www.gitbook.com/book/looly/elasticsearch-the-definitive-guide-cn/details">Elasticsearch权威指南（中文版）</a></li>
<li>.....</li>
</ul>

<span id="more"></span><!-- more -->

<h1 id="toc_1">ELK下载</h1>

<p>历史版本下载地址 : <a href="https://www.elastic.co/downloads/past-releases">https://www.elastic.co/downloads/past-releases</a><br/>
+ elasticsearch : 2.4.0<br/>
+ logstash : 2.4.0<br/>
+ kibana : 4.6.2</p>

<h1 id="toc_2">安装准备</h1>

<pre><code>[root@U006 opt]# cd /opt
[root@U006 opt]# mkdir elk
[root@U006 opt]# chown -R hadoop:hadoop elk
[root@U006 opt]# su hadoop
[hadoop@U006 opt]$ cd /opt/elk/
[hadoop@U006 elk]$ mkdir jars
[hadoop@U006 jars]$ pwd
/opt/elk/jars

# 上传jar包

[hadoop@U006 jars]$ ll
total 116996
-rw-r--r-- 1 hadoop hadoop 27364449 Sep 18 09:36 elasticsearch-2.4.0.tar.gz
-rw-r--r-- 1 hadoop hadoop 34125464 Sep 18 09:37 kibana-4.6.2-linux-x86_64.tar.gz
-rw-r--r-- 1 hadoop hadoop 58310656 Sep 18 09:41 logstash-2.4.0.tar.gz

</code></pre>

<pre><code>[hadoop@U006 elk]$ ll
total 16
drwxrwxr-x  6 hadoop hadoop 4096 Sep 18 09:45 elasticsearch-2.4.0
drwxrwxr-x  2 hadoop hadoop 4096 Sep 18 09:40 jars
drwxrwxr-x 11 hadoop hadoop 4096 Oct 21  2016 kibana-4.6.2-linux-x86_64
drwxrwxr-x  5 hadoop hadoop 4096 Sep 18 09:47 logstash-2.4.0
</code></pre>

<h1 id="toc_3">安装elasticsearch</h1>

<blockquote>
<p>测试环境搭建,单节点为例</p>
</blockquote>

<pre><code># 解压elasticsearch
tar -zxvf elasticsearch-2.4.0.tar.gz -C /opt/elk/
</code></pre>

<h2 id="toc_4">修改配置文件config/elasticsearch.yml</h2>

<pre><code>[hadoop@U006 config]$ pwd
/opt/elk/elasticsearch-2.4.0/config

[hadoop@U006 config]$ ll
total 8
-rw-rw-r-- 1 hadoop hadoop 3192 Aug 24  2016 elasticsearch.yml
-rw-rw-r-- 1 hadoop hadoop 2571 Aug 24  2016 logging.yml

[hadoop@U006 elasticsearch-2.4.0]$ vim config/elasticsearch.yml

# 打开这三个配置的注释
 cluster.name: test_elasticsearch # es集群名字
 node.name: node1 # 该节点在es中的名字
 network.host: 0.0.0.0 # 任意节点可以访问
</code></pre>

<h2 id="toc_5">启动es</h2>

<pre><code>./bin/elasticsearch
./bin/elasticsearch -d #后台启动
</code></pre>

<pre><code>[hadoop@U006 elasticsearch-2.4.0]$ ./bin/elasticsearch
[2017-09-18 10:08:50,300][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in
[2017-09-18 10:08:51,697][INFO ][node                     ] [node1] version[2.4.0], pid[3098], build[ce9f0c7/2016-08-29T09:14:17Z]
[2017-09-18 10:08:51,697][INFO ][node                     ] [node1] initializing ...
[2017-09-18 10:08:52,332][INFO ][plugins                  ] [node1] modules [lang-groovy, reindex, lang-expression], plugins [], sites []
[2017-09-18 10:08:52,363][INFO ][env                      ] [node1] using [1] data paths, mounts [[/home (/dev/mapper/VolGroup-lv_home)]], net usable_space [195.3gb], net total_space [857.4gb], spins? [possibly], types [ext4]
[2017-09-18 10:08:52,363][INFO ][env                      ] [node1] heap size [989.8mb], compressed ordinary object pointers [true]
[2017-09-18 10:08:54,472][INFO ][node                     ] [node1] initialized
[2017-09-18 10:08:54,473][INFO ][node                     ] [node1] starting ...
[2017-09-18 10:08:54,675][INFO ][transport                ] [node1] publish_address {10.10.25.13:9300}, bound_addresses {[::]:9300}
[2017-09-18 10:08:54,685][INFO ][discovery                ] [node1] test_elasticsearch/oAbAZR_tTaGAJU-5yd3BqQ
[2017-09-18 10:08:57,757][INFO ][cluster.service          ] [node1] new_master {node1}{oAbAZR_tTaGAJU-5yd3BqQ}{10.10.25.13}{10.10.25.13:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2017-09-18 10:08:57,798][INFO ][http                     ] [node1] publish_address {10.10.25.13:9200}, bound_addresses {[::]:9200}
[2017-09-18 10:08:57,799][INFO ][node                     ] [node1] started
[2017-09-18 10:08:57,811][INFO ][gateway                  ] [node1] recovered [0] indices into cluster_state
</code></pre>

<p>打开<a href="http://10.10.25.13:9200/">http://10.10.25.13:9200/</a> 将会看到以下内容.返回数据中包含配置的cluster.name和node.name,以及es的版本等信息.<br/>
<img src="media/15198769472354/es-startresult.png" alt="es-startresult"/></p>

<h2 id="toc_6">安装插件</h2>

<h3 id="toc_7">elasticsearch-head 插件安装</h3>

<h4 id="toc_8">下载安装</h4>

<pre><code># 进入bin目录下
[hadoop@U006 bin]$ ./plugin install mobz/elasticsearch-head
-&gt; Installing mobz/elasticsearch-head...
Trying https://github.com/mobz/elasticsearch-head/archive/master.zip ...
Downloading ...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................DONE
Verifying https://github.com/mobz/elasticsearch-head/archive/master.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
Installed head into /opt/elk/elasticsearch-2.4.0/plugins/head

</code></pre>

<h4 id="toc_9">使用说明</h4>

<blockquote>
<p>head插件是一个用浏览器跟ES集群交互的插件，可以查看集群状态、集群的doc内容、执行搜索和普通的Rest请求等。</p>
</blockquote>

<p>在浏览器中直接访问接口 <a href="http://10.10.25.13:9200/_plugin/head/">http://10.10.25.13:9200/_plugin/head/</a> ,可以看到es集群状态<br/>
<img src="media/15198769472354/es-plugin-head.png" alt="es-plugin-head"/></p>

<h3 id="toc_10">安装Marvel插件</h3>

<h4 id="toc_11">下载安装</h4>

<pre><code># ./bin/plugin install license
# ./bin/plugin install marvel-agent

[hadoop@U006 elasticsearch-2.4.0]$ ./bin/plugin install license
-&gt; Installing license...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.4.0/license-2.4.0.zip ...
Downloading .......DONE
Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.4.0/license-2.4.0.zip checksums if available ...
Downloading .DONE
Installed license into /opt/elk/elasticsearch-2.4.0/plugins/license
[hadoop@U006 elasticsearch-2.4.0]$ ./bin/plugin install marvel-agent
-&gt; Installing marvel-agent...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.4.0/marvel-agent-2.4.0.zip ...
Downloading ..........DONE
Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.4.0/marvel-agent-2.4.0.zip checksums if available ...
Downloading .DONE
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission setFactory
* javax.net.ssl.SSLPermission setHostnameVerifier
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
Installed marvel-agent into /opt/elk/elasticsearch-2.4.0/plugins/marvel-agent


# 安装的插件都会出现在es的plugins目录下
[hadoop@U006 elasticsearch-2.4.0]$ ll
total 56
drwxrwxr-x 2 hadoop hadoop  4096 Sep 18 09:45 bin
drwxrwxr-x 3 hadoop hadoop  4096 Sep 18 10:05 config
drwxrwxr-x 3 hadoop hadoop  4096 Sep 18 09:59 data
drwxrwxr-x 2 hadoop hadoop  4096 Sep 18 09:45 lib
-rw-rw-r-- 1 hadoop hadoop 11358 Aug 24  2016 LICENSE.txt
drwxrwxr-x 2 hadoop hadoop  4096 Sep 18 09:59 logs
drwxrwxr-x 5 hadoop hadoop  4096 Aug 29  2016 modules
-rw-rw-r-- 1 hadoop hadoop   150 Aug 24  2016 NOTICE.txt
drwxrwxr-x 3 hadoop hadoop  4096 Sep 18 10:15 plugins
-rw-rw-r-- 1 hadoop hadoop  8700 Aug 24  2016 README.textile
[hadoop@U006 elasticsearch-2.4.0]$ cd plugins/
[hadoop@U006 plugins]$ ll
total 12
drwxrwxr-x 6 hadoop hadoop 4096 Sep 18 10:15 head
drwxrwxr-x 2 hadoop hadoop 4096 Sep 18 10:34 license
drwxrwxr-x 2 hadoop hadoop 4096 Sep 18 10:34 marvel-agent
</code></pre>

<h4 id="toc_12">使用说明</h4>

<blockquote>
<p>Marvel是Elasticsearch的管理和监控工具，在开发环境下免费使用。它包含了一个叫做Sense的交互式控制台，<br/>
使用户方便的通过浏览器直接与Elasticsearch进行交互。<br/>
marvel插件主要会和kibana进行配置使用,待会看kibana也需要安装marvel插件</p>
</blockquote>

<h2 id="toc_13">注意</h2>

<ul>
<li>如何之前在config/elasticsearch.yml的文件中,没有修改network.host项.那么你只能用localhost或者127.0.0.1访问es了.</li>
<li>注意配置yml结尾的配置文件都需要冒号后面加空格才行</li>
</ul>

<h1 id="toc_14">安装kibana</h1>

<h2 id="toc_15">解压安装</h2>

<pre><code>tar -zxvf kibana-4.6.2-linux-x86_64.tar.gz -C /opt/elk/

</code></pre>

<h2 id="toc_16">修改config/kibana.yml的elasticsearch.url属性即可。</h2>

<p><img src="media/15198769472354/kibana-config.png" alt="kibana-config"/></p>

<h2 id="toc_17">安装插件</h2>

<h3 id="toc_18">安装Marvel插件</h3>

<blockquote>
<p>在安装es的时候,已经给es安装了marvel插件,现在给kibana也安装上marvel插件</p>
</blockquote>

<h4 id="toc_19">下载</h4>

<pre><code>[hadoop@U006 kibana-4.6.2-linux-x86_64]$ bin/kibana plugin --install elasticsearch/marvel/latest
Installing marvel
Attempting to transfer from https://download.elastic.co/elasticsearch/marvel/marvel-latest.tar.gz
.....
Transfer complete
Extracting plugin archive
Extraction complete
Optimizing and caching browser bundles...
Plugin installation complete

</code></pre>

<h4 id="toc_20">启动验证</h4>

<pre><code>bin/elasticsearch
bin/kibana

</code></pre>

<p>查看<a href="http://10.10.25.13:5601/app/marvel">http://10.10.25.13:5601/app/marvel</a> 页面：<br/>
<img src="media/15198769472354/kibana-marvel1.png" alt="kibana-marve"/><br/>
<img src="media/15198769472354/kibana-marvel2.png" alt="kibana-marve"/></p>

<h3 id="toc_21">安装sense插件</h3>

<blockquote>
<p>Sense是flask写的elasticsearch查询工具。<br/>
支持es查询语言自动提示，es结构自动提示，支持两种主题，支持查询历史记录，支持快捷键。</p>
</blockquote>

<h4 id="toc_22">下载</h4>

<pre><code>[hadoop@U006 kibana-4.6.2-linux-x86_64]$ ./bin/kibana plugin --install elastic/sense
Installing sense
Attempting to transfer from https://download.elastic.co/elastic/sense/sense-latest.tar.gz
.....
Transfer complete
Extracting plugin archive
Extraction complete
Optimizing and caching browser bundles...
Plugin installation complete

</code></pre>

<h4 id="toc_23">使用说明</h4>

<p>启动es和kibana<br/>
查看<a href="http://10.10.25.13:5601/app/sense">http://10.10.25.13:5601/app/sense</a> 页面：<br/>
<img src="media/15198769472354/kibana-sense.png" alt="kibana-sense"/></p>

<h1 id="toc_24">安装logstash</h1>

<h2 id="toc_25">解压安装即可</h2>

<pre><code>tar -zxvf logstash-2.4.0.tar.gz -C /opt/elk/

</code></pre>

<h2 id="toc_26">配置logstash的配置文件</h2>

<p>此文件input为从log4j接收日志.output为输出到es集群,进行搜索.</p>

<p>字段具体意思可以看官网 <a href="https://www.elastic.co/guide/en/logstash/current/plugins-inputs-log4j.html">https://www.elastic.co/guide/en/logstash/current/plugins-inputs-log4j.html</a></p>

<pre><code>[hadoop@U006 logstash-2.4.0]$ vim logstash_log4j_to_es.conf
input {
        log4j {
                 mode =&gt; &quot;server&quot;
                 host =&gt; &quot;10.10.25.13&quot;
                 port =&gt; 4567
                 type =&gt; &quot;log4j&quot;
         }
}
output{
        elasticsearch{
                action =&gt; &quot;index&quot;
                hosts =&gt; &quot;10.10.25.13:9200&quot;
                index =&gt; &quot;test_log&quot;
        }
}

</code></pre>

<p><img src="media/15198769472354/logstash-file1.png" alt="logstash-file1"/></p>

<h2 id="toc_27">启动logstash</h2>

<pre><code>bin/logstash agent -f logstash_log4j_to_es.conf
# 或者
bin/logstash -f logstash_log4j_to_es.conf

[hadoop@U006 logstash-2.4.0]$ bin/logstash -f logstash_log4j_to_es.conf
Settings: Default pipeline workers: 24
log4j:WARN No appenders could be found for logger (org.apache.http.client.protocol.RequestAuthCache).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Pipeline main started
</code></pre>

<h1 id="toc_28">ELK框架的综合实际使用</h1>

<blockquote>
<p>在工作中,我们会使用ELK对业务日志进行收集分析.<br/>
也就是把项目中的log4j日志用logstash进行收集,然后输出到es中进行索引搜索.最后使用Kibana进行可视化的搜索展示.</p>
</blockquote>

<h2 id="toc_29">启动elk</h2>

<pre><code>bin/elasticsearch
bin/kibana
bin/logstash -f logstash_log4j_to_es.conf
</code></pre>

<p>打开：<a href="http://10.10.25.13:5601">http://10.10.25.13:5601</a><br/>
<img src="media/15198769472354/kibana-index.png" alt="kibana-index"/></p>

<p>如图所以,这里有个WARN警告:没有默认的索引模式,需要创建一个才能继续.<br/>
那么我们就创建一个索引.</p>

<h2 id="toc_30">创建索引</h2>

<p>Kibana界面日志检索只有当第一条日志通过Logstash进入ElasticSearch后，才能配置Kibana索引。</p>

<p>1、在“Index name or pattern”项下，填入一个elasticsearch的索引名，也即是Logstash配置文件中output项下的index对应的名称；在你这里应该是将“logstash-* ” 改成“test_log”<br/>
2、在“Time-field name”，选用默认的配置：“@timestamp”<br/>
3、点击“create”即可</p>

<p><img src="media/15198769472354/kibana-index2.png" alt="kibana-index2"/></p>

<h2 id="toc_31">log4j日志接入</h2>

<h3 id="toc_32">编写log4j的测试代码</h3>

<pre><code>public class TestFunc {

  Logger logger = LoggerFactory.getLogger(TestFunc.class);

  @Test
  public void testlog4j() throws Exception {
    while (true) {
      long s_time = System.currentTimeMillis();
      logger.info(&quot;当前时间戳: &quot;+s_time+&quot; i am info info hadoop&quot;);
      logger.warn(&quot;当前时间戳: &quot;+s_time+&quot; i am info warn spark hushiwei nice&quot;);
      logger.error(&quot;当前时间戳: &quot;+s_time+&quot; i am info error elk&quot;);
      Thread.sleep(1000L);

    }
  }
  }
</code></pre>

<h3 id="toc_33">log4j的配置</h3>

<p>log4j.properties<br/>
remotehost填写logstash的服务器地址.也就是那个input项里面的地址.<br/>
<code><br/>
log4j.rootLogger=INFO,socket<br/>
log4j.appender.socket=org.apache.log4j.net.SocketAppender<br/>
log4j.appender.socket.RemoteHost=10.10.25.13<br/>
log4j.appender.socket.Port=4567<br/>
log4j.appender.socket.LocationInfo=true<br/>
</code></p>

<h2 id="toc_34">执行代码,输出log4j日志,观察kibana页面变化</h2>

<p>索引日志里面的信息</p>

<p><img src="media/15198769472354/kibana-index3.png" alt="kibana-index3"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python读取文件编码及内容]]></title>
    <link href="http://dmlcoding.com/15198744609518.html"/>
    <updated>2018-03-01T11:21:00+08:00</updated>
    <id>http://dmlcoding.com/15198744609518.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>当你不知道文件的具体编码方式的时候,如何正确的读取文件内容呢?</p>
</blockquote>

<h1 id="toc_0">报错日志</h1>

<p>当我们不知道文件编码方式的时候,贸然的读取文件,有时候就会出现这些问题</p>

<span id="more"></span><!-- more -->

<pre><code>UnicodeDecodeError: &#39;gbk&#39; codec can&#39;t decode byte
......
</code></pre>

<h1 id="toc_1">解决办法</h1>

<p>所以就是编码方式不对,那么需要先能识别文件的编码文件,然后根据此编码方式进行对文件编码，最后返回文件内容。<br/>
可以借助一个第三方库<code>chardet</code><br/>
```</p>

<h1 id="toc_2">安装chardet</h1>

<p>pip install chardet<br/>
```</p>

<h1 id="toc_3">正确实例</h1>

<pre><code>with open(&quot;your_file&quot;, &#39;rb&#39;) as fp:
    file_data = fp.read()
    result = chardet.detect(file_data)
    file_content = file_data.decode(encoding=result[&#39;encoding&#39;])

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[spark开发中遇到的问题]]></title>
    <link href="http://dmlcoding.com/15198733795152.html"/>
    <updated>2018-03-01T11:02:59+08:00</updated>
    <id>http://dmlcoding.com/15198733795152.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">spark连接mysql</h1>

<h2 id="toc_1">问题描述</h2>

<pre><code>总是报no suitable driver以及   jdbc.mysql.driver类似这样的错误
</code></pre>

<span id="more"></span><!-- more -->

<h2 id="toc_2">解决办法1</h2>

<pre><code>1.提交任务的时候带上这个，手动指定mysql jar包的位置
                    SPARK_CLASSPATH=/usr/local/spark-1.4.1-bin-hadoop2.6/lib/mysql-connector-java-5.1.38.jar ./bin/spark-submit --class sparkDemo /root/data/demon-parent-1.0-SNAPSHOT-jar-with-dependencies.jar hdfs://192.168.119.100:9000/examples/custom.txt
</code></pre>

<h2 id="toc_3">解决办法2</h2>

<pre><code>修改了这个配置SPARK_HOME/conf/spark-env.sh文件，在里面加上了这个参数，就OK了

export SPARK_CLASSPATH=$SPATH_CLASSPATH:/usr/hdp/2.4.0.0-169/spark/lib/mysql-connector-java-5.1.38.jar

</code></pre>

<h1 id="toc_4">在spark中使用hive抛出错误</h1>

<h1 id="toc_5">报错日志</h1>

<pre><code>17/08/09 12:11:51 WARN DataNucleus.Persistence: Error creating validator of type org.datanucleus.properties.CorePropertyValidator
ClassLoaderResolver for class &quot;&quot; gave error on creation : {1}
org.datanucleus.exceptions.NucleusUserException: ClassLoaderResolver for class &quot;&quot; gave error on creation : {1}
    at org.datanucleus.NucleusContext.getClassLoaderResolver(NucleusContext.java:1087)
    at org.datanucleus.PersistenceConfiguration.validatePropertyValue(PersistenceConfiguration.java:797)
    at org.datanucleus.PersistenceConfiguration.setProperty(PersistenceConfiguration.java:714)
    at org.datanucleus.PersistenceConfiguration.setPersistenceProperties(PersistenceConfiguration.java:693)
    at org.datanucleus.NucleusContext.&lt;init&gt;(NucleusContext.java:273)
    at org.datanucleus.NucleusContext.&lt;init&gt;(NucleusContext.java:247)
    at org.datanucleus.NucleusContext.&lt;init&gt;(NucleusContext.java:225)
    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.&lt;init&gt;(JDOPersistenceManagerFactory.java:416)
    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:301)
    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
</code></pre>

<h1 id="toc_6">问题分析</h1>

<p>看日志应该是缺少了hive的一些包,在网上搜了一下,是下面几个包<br/>
```<br/>
[hadoop@U007 lib]\( pwd<br/>
/opt/spark-1.6.0/lib<br/>
[hadoop@U007 lib]\) ll<br/>
total 305220<br/>
-rw-r--r-- 1 hadoop hadoop    339666 Apr 15  2016 datanucleus-api-jdo-3.2.6.jar<br/>
-rw-r--r-- 1 hadoop hadoop   1890075 Apr 15  2016 datanucleus-core-3.2.10.jar<br/>
-rw-r--r-- 1 hadoop hadoop   1809447 Apr 15  2016 datanucleus-rdbms-3.2.9.jar<br/>
...</p>

<pre><code>
所以在提交spark任务的时候,把这几个包加入到classpath中即可

# 解决办法
在提交spark的脚本中加上这几个jar包和hive-site.xml文件
如下

</code></pre>

<p>nohup spark-submit \<br/>
 --master yarn \<br/>
 --deploy-mode cluster \<br/>
 --class ${className} \<br/>
 --driver-memory 4g \<br/>
 --executor-memory 2g \<br/>
 --executor-cores 4 \<br/>
 --num-executors 4 \<br/>
 --jars ./lib/datanucleus-api-jdo-3.2.6.jar,./lib/datanucleus-core-3.2.10.jar,./lib/datanucleus-rdbms<br/>
-3.2.9.jar  \<br/>
 --files ./lib/hive-site.xml \<br/>
 ./app-jar-with-dependencies.jar \</p>

<pre><code>
加上--jars 和 --files即可

# 在spark中将数据插入hive动态分区
## 问题描述
当我用standalone以及yarn-client模式进行提交任务的时候,不会报错.但是当我改成yarn-cluster模式进行提交任务,有时候就会报下面的错
## 报错日志
</code></pre>

<p>17/08/09 10:08:01 ERROR scheduler.JobScheduler: Error running job streaming job 1502188440000 ms.0<br/>
java.lang.NoSuchMethodException: org.apache.hadoop.hive.ql.metadata.Hive.loadDynamicPartitions(org.apache.hadoop.fs.Path, java.lang.String, java.util.Map, boolean, int, boolean, boolean)<br/>
    at java.lang.Class.getMethod(Class.java:1670)<br/>
    at org.apache.spark.sql.hive.client.Shim.findMethod(HiveShim.scala:114)<br/>
    at org.apache.spark.sql.hive.client.Shim_v0_12.loadDynamicPartitionsMethod\(lzycompute(HiveShim.scala:168)<br/>
    at org.apache.spark.sql.hive.client.Shim_v0_12.loadDynamicPartitionsMethod(HiveShim.scala:167)<br/>
    at org.apache.spark.sql.hive.client.Shim_v0_12.loadDynamicPartitions(HiveShim.scala:261)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper\)\(anonfun\)loadDynamicPartitions\(1.apply\)mcV\(sp(ClientWrapper.scala:560)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper\)\(anonfun\)loadDynamicPartitions\(1.apply(ClientWrapper.scala:560)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper\)\(anonfun\)loadDynamicPartitions\(1.apply(ClientWrapper.scala:560)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper\)\(anonfun\)withHiveState\(1.apply(ClientWrapper.scala:279)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper.liftedTree1\)1(ClientWrapper.scala:226)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:225)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:268)<br/>
    at org.apache.spark.sql.hive.client.ClientWrapper.loadDynamicPartitions(ClientWrapper.scala:559)<br/>
    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult\(lzycompute(InsertIntoHiveTable.scala:225)<br/>
    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult(InsertIntoHiveTable.scala:127)<br/>
    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.doExecute(InsertIntoHiveTable.scala:276)<br/>
    at org.apache.spark.sql.execution.SparkPlan\)\(anonfun\)execute\(5.apply(SparkPlan.scala:132)<br/>
    at org.apache.spark.sql.execution.SparkPlan\)\(anonfun\)execute\(5.apply(SparkPlan.scala:130)<br/>
    at org.apache.spark.rdd.RDDOperationScope\).withScope(RDDOperationScope.scala:150)<br/>
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)<br/>
    at org.apache.spark.sql.execution.QueryExecution.toRdd\(lzycompute(QueryExecution.scala:55)<br/>
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)<br/>
    at org.apache.spark.sql.DataFrame.&lt;init&gt;(DataFrame.scala:145)<br/>
    at org.apache.spark.sql.DataFrame.&lt;init&gt;(DataFrame.scala:130)<br/>
    at org.apache.spark.sql.DataFrame\).apply(DataFrame.scala:52)<br/>
    at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:817)<br/>
```</p>

<h2 id="toc_7">分析</h2>

<p>用client模式的时候,是在13上运行的.没有问题<br/>
用cluster模式的时候,有时候报错,有时候没有报错<br/>
那不禁让我猜想,为啥cluster模式时而报错时而不报错呢?</p>

<p>然后我用client模式,在14上提交,不出我所料,基本上每个job都抛出了那个错误.<br/>
所以定位到问题就是,除了13这个节点外,别的节点缺少了什么包,导致抛出了错误.<br/>
因为抛出来的错误是java.lang.NoSuchMethodException:,所以肯定是缺少了什么包.<br/>
之前cluster模式时而报错时而不报错的原因肯定是,当不报错的时候,正好driver端是在13上</p>

<p>现在的问题就是找出别的机器缺少什么包了.</p>

<p>然后我在spark的环境变量里面发现了这个参数<br/>
<code><br/>
spark.sql.hive.metastore.jars : /usr/lib/hive/lib/*:/opt/spark-1.6.0/lib/spark-assembly-1.6.0-hadoop2.4.0.jar<br/>
</code></p>

<p>我去,13上有这个/usr/lib/hive/lib/* 路径<br/>
14和15上都没有,,,<br/>
问题找到了</p>

<h2 id="toc_8">解决办法1</h2>

<p>把13上这个路径/usr/lib/hive/lib/* 拷贝到14和15上,各自都有一份.这样无论driver端在哪里,都能找到相应的jar包.<br/>
就这样愉快的解决了.<br/>
所以遇到问题,慢慢分析,不要像无头苍蝇一样.<br/>
在网上搜的解决办法,都无法解决这个问题.所以有时候,具体问题具体分析,要慢慢的分析到出错原因.找到了原因,bug就能迎刃而解.</p>

<h2 id="toc_9">解决办法2</h2>

<p>在spark的配置文件中把 <code>spark.sql.hive.metastore.jars</code> 给删了.因为你总不能在每个节点上去拷贝hive的一些依赖吧,如果以后hive升级了,还得替换hive的jar包,太麻烦.所以改成下面的解决办法更好.</p>

<p>在pom文件中加上hive的依赖</p>

<pre><code>&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.10 --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-mllib_2.10 --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-mllib_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming-kafka_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;mysql&lt;/groupId&gt;
    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;version&gt;5.1.32&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
    &lt;version&gt;0.13.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
    &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;
    &lt;version&gt;0.13.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<h1 id="toc_10">sparkstreaming读取kafka数据</h1>

<h2 id="toc_11">问题描述Couldn&#39;t find leaders for Set</h2>

<p>SparkStreaming程序从Kafka读数据的程序运行期间报了描述中的异常.<br/>
通过监控分析发现,是由于有一个Broker挂掉了。可是对应Topic的replica设置的2，就算挂掉一个，应该有replica顶上啊。<br/>
后来发现，这是由于存在Partition的Replica没有跟Leader保持同步更新，也就是通常所说的“没追上”。 查看某个Topic是否存在没追上的情况：</p>

<p>查看某个Topic是否存在没追上的情况：<br/>
<code><br/>
kafka-topics.sh --describe --zookeeper XXX --topic XXX<br/>
</code></p>

<h2 id="toc_12">报错日志</h2>

<pre><code>17/10/13 09:41:13 ERROR DirectKafkaInputDStream: ArrayBuffer(java.nio.channels.ClosedChannelException, org.apache.spark.SparkException: Couldn&#39;t find leader offsets for Set([dsp_request_event,2]))
17/10/13 09:41:13 ERROR StreamingContext: Error starting the context, marking it as stopped
org.apache.spark.SparkException: ArrayBuffer(java.nio.channels.ClosedChannelException, org.apache.spark.SparkException: Couldn&#39;t find leader offsets for Set([dsp_request_event,2]))
    at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.latestLeaderOffsets(DirectKafkaInputDStream.scala:123)
    at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:145)
    at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:352)
    at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:352)
</code></pre>

<h2 id="toc_13">解决办法</h2>

<p>观察其中的Replicas和Isr是否一致，如果出现Isr少于Replicas，则对应Partition存在没追上的情况<br/>
解决方法：<br/>
增大num.replica.fetchers的值，此参数是Replicas从Leader同步数据的线程数，默认为1，增大此参数即增大了同步IO。经过测试，增大此值后，不再有追不上的情况<br/>
确定问题已解决的方法：<br/>
启动出现问题的SparkStreaming程序，在程序正常计算的状态下，kill掉任意一个Broker后，再观察运行情况。在增大同步线程数之前，kill后SparkStreaming会报同样的异常，而增大后程序依然正常运行，问题解决。</p>

<p>参考:<a href="http://blog.csdn.net/yanshu2012/article/details/53995159">http://blog.csdn.net/yanshu2012/article/details/53995159</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[诗经 <<伐檀>>]]></title>
    <link href="http://dmlcoding.com/15198767503226.html"/>
    <updated>2018-03-01T11:59:10+08:00</updated>
    <id>http://dmlcoding.com/15198767503226.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15198767503226/fatan.png" alt="fatan"/></p>

<span id="more"></span><!-- more -->

<blockquote>
<p>抄一首诗,换个心情...</p>
</blockquote>

<h1 id="toc_0">伐檀</h1>

<p>坎坎伐檀兮，置之河之干兮，河水清且涟猗。<br/>
不稼不穑，胡取禾三百廛[1]兮？<br/>
不狩不猎，胡瞻尔庭有县[2]貆兮？<br/>
彼君子兮，不素餐兮！</p>

<p>坎坎伐辐兮，置之河之侧兮，河水清且直猗。<br/>
不稼不穑，胡取禾三百亿兮？<br/>
不狩不猎，胡瞻尔庭有县特[3]兮？<br/>
彼君子兮，不素食兮！</p>

<p>坎坎伐轮兮，置之河之漘[4]兮，河水清且沦猗。<br/>
不稼不穑，胡取禾三百囷[5]兮？<br/>
不狩不猎，胡瞻尔庭有县鹑兮？<br/>
彼君子兮，不素飧兮！</p>

<h1 id="toc_1">伐檀白话</h1>

<p>砍伐檀树声坎坎啊，<br/>
棵棵放倒堆河边啊，<br/>
河水清清微波转哟。<br/>
不播种来不收割，<br/>
为何三百捆禾往家搬啊？<br/>
不冬狩来不夜猎，<br/>
为何见你庭院猪獾悬啊？<br/>
那些老爷君子啊，<br/>
不会白吃闲饭啊！</p>

<p>砍下檀树做车辐啊，<br/>
放在河边堆一处啊。<br/>
河水清清直流注哟。<br/>
不播种来不收割，<br/>
为何三百捆禾要独取啊？<br/>
不冬狩来不夜猎，<br/>
为何见你庭院兽悬柱啊？<br/>
那些老爷君子啊，<br/>
不会白吃饱腹啊！</p>

<p>砍下檀树做车轮啊，<br/>
棵棵放倒河边屯啊。<br/>
河水清清起波纹啊。<br/>
不播种来不收割，<br/>
为何三百捆禾要独吞啊？<br/>
不冬狩来不夜猎，<br/>
为何见你庭院挂鹌鹑啊？<br/>
那些老爷君子啊，<br/>
可不白吃腥荤啊！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ITerm2下使用ssh访问Linux(包括堡垒机)]]></title>
    <link href="http://dmlcoding.com/15198752296399.html"/>
    <updated>2018-03-01T11:33:49+08:00</updated>
    <id>http://dmlcoding.com/15198752296399.html</id>
    <content type="html"><![CDATA[
<p>mac下没有xshell,虽然有SecurtCRT,但是真的太丑了.我还是比较喜欢用Iterm2来进行远程连接.<br/>
这样不可避免的会碰到要记录远程密码,如果每次都输入,那就太麻烦了.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">Iterm2下使用ssh访问Linux</h1>

<p>通过情况下,Iterm2访问远程Linux使用ssh命令,如下:<br/>
<code><br/>
ssh &lt;用户名&gt;@&lt;ip&gt;<br/>
</code><br/>
然后输入访问密码即可登录进去.有时候远程访问的默认端口如果不是22,那就需要额外加上<code>-p</code>参数跟上远程访问端口进行登录了.<br/>
很明显如果每次都要输入访问密码,那在开发过程中是相当的不方便的.</p>

<p>这里有两个方式实现免密登录.都是用Iterm2的Profiles功能加上脚本来实现.</p>

<h2 id="toc_1">方式1:使用spawn脚本文件</h2>

<p>将远程访问的相关内容写成一个脚本,然后在Profile里面调用即可.<br/>
<code><br/>
cd /Users/hushiwei/.ssh/<br/>
$ touch filename<br/>
</code></p>

<p>脚本内容<br/>
```</p>

<h1 id="toc_2">!/usr/bin/expect -f</h1>

<p>set user &lt;用户名&gt;<br/>
  set host <ip地址><br/>
  set password &lt;密码&gt;<br/>
  set timeout -1</p>

<p>spawn ssh \(user@\)host<br/>
  expect &quot;<em>assword:</em>&quot;<br/>
  send &quot;$password\r&quot;<br/>
  interact<br/>
  expect eof<br/>
```</p>

<p>如何调用呢?<br/>
在command中使用命令.command在哪看下面的图你就知道了.<br/>
<code><br/>
expect &lt;保存的脚本完整路径&gt;<br/>
</code></p>

<h2 id="toc_3">方式2:使用sshpass(推荐方式)</h2>

<p>brew安装sshpass<br/>
<code><br/>
brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb<br/>
</code></p>

<p>然后把密码写入到一个文件中<br/>
<code><br/>
hushiwei@localhost  ~/sshpass  pwd<br/>
/Users/hushiwei/sshpass<br/>
hushiwei@localhost  ~/sshpass  more pass<br/>
passwd123<br/>
</code><br/>
参考图中进行配置<br/>
<img src="media/15198752296399/sshpass.png" alt="sshpass"/></p>

<p>command写上命令<br/>
<code><br/>
/usr/local/bin/sshpass -f /Users/hushiwei/sshpass/pass ssh -p22 用户名@密码<br/>
</code><br/>
然后在iterm2的菜单栏选择Profiles,然后点击刚刚的配置,即可免密自动登录到服务器上</p>

<ul>
<li>注意:首先用命令行登录一次</li>
</ul>

<h1 id="toc_4">iterm2登录堡垒机</h1>

<p>通过SSH和密钥文件(.pem格式)登录服务器［可能是堡垒机］</p>

<h2 id="toc_5">首先修改下密钥文件权限</h2>

<pre><code>sudo chmod 600 /Users/hushiwei/sshpass/Jumpserver/hushiwei.pem
</code></pre>

<h2 id="toc_6">其次，终端可直接命令连接</h2>

<pre><code>ssh -i /Users/hushiwei/sshpass/Jumpserver/hushiwei.pem hushiwei@xxx.xxx.xxx.xxx
</code></pre>

<p>注：首次连接时，会弹出密钥文件密码输入框，可以输入并保存！</p>

<p>除了直接命令连接外，也可参考上面Profiles功能，配置好，直接在Profile里调用！简单脚本如下：</p>

<h2 id="toc_7">配置Profile脚本自动登录堡垒机</h2>

<p>脚本文件 vim jumpserver<br/>
```<br/>
hushiwei@localhost  ~/sshpass/Jumpserver  more jumpserver</p>

<h1 id="toc_8">!/usr/bin/expect -f</h1>

<p>set user hushiwei<br/>
 set host xxx.xxx.xxx.xxx<br/>
 set empath /Users/hushiwei/sshpass/Jumpserver/hushiwei.pem<br/>
 set timeout -1</p>

<p>spawn ssh -i \(empath \)user@$host<br/>
 interact<br/>
 expect eof<br/>
```</p>

<p>命令行执行<br/>
```<br/>
hushiwei@localhost  ~/sshpass/Jumpserver  expect /Users/hushiwei/sshpass/Jumpserver/jumpserver<br/>
spawn ssh -i /Users/hushiwei/sshpass/Jumpserver/hushiwei.pem <a href="mailto:hushiwei@xxx.xxx.xxx.xxx">hushiwei@xxx.xxx.xxx.xxx</a><br/>
Last login: Fri Aug 18 11:06:16 2017 from xxx.xxx.xxx.xxx</p>

<h3 id="toc_9">欢迎使用Jumpserver开源跳板机系统</h3>

<pre><code>   1) 输入 ID 直接登录 或 输入部分 IP,主机名,备注 进行搜索登录(如果唯一).
   2) 输入 / + IP, 主机名 or 备注 搜索. 如: /ip
   3) 输入 P/p 显示您有权限的主机.
   4) 输入 G/g 显示您有权限的主机组.
   5) 输入 G/g + 组ID 显示该组下主机. 如: g1
   6) 输入 E/e 批量执行命令.
   7) 输入 U/u 批量上传文件.
   8) 输入 D/d 批量下载文件.
   9) 输入 H/h 帮助.
   0) 输入 Q/q 退出.
</code></pre>

<p>Opt or ID&gt;:<br/>
<code><br/>
参考上面的Profile功能,配置好,直接在Profile里调用即可<br/>
</code></p>

<h1 id="toc_10">在Command里面写入以下即可</h1>

<p>expect /Users/hushiwei/sshpass/Jumpserver/jumpserver<br/>
```</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[监控SparkStreaming程序脚本]]></title>
    <link href="http://dmlcoding.com/15198749505894.html"/>
    <updated>2018-03-01T11:29:10+08:00</updated>
    <id>http://dmlcoding.com/15198749505894.html</id>
    <content type="html"><![CDATA[
<p>虽然Spark on yarn非常的稳定,一般情况下是不会出问题的.但是我们的SparkStreaming程序是一直运行着出实时报表的.<br/>
我们必须得对SparkStreaming程序进行监控,在程序退出后,能够及时的重启.<br/>
基于此需求,我想到了通过调用yarn的rest接口来获取提交到yarn上的任务</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">思路</h1>

<p>调用yarn提供的rest接口来获取所有正在运行的任务<br/>
<code><br/>
curl --compressed -H &quot;Accept: application/json&quot; -X GET &quot;http://master:8088/ws/v1/cluster/apps?states=RUNNING&quot;<br/>
</code><br/>
如果对别的接口有兴趣,可以看看官网.<br/>
+ <a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html</a><br/>
+ <a href="http://www.winseliu.com/blog/2014/12/07/hadoop-mr-rest-api/">http://www.winseliu.com/blog/2014/12/07/hadoop-mr-rest-api/</a></p>

<h1 id="toc_1">脚本</h1>

<p>脚本也很简单,简单看看就明白了</p>

<pre><code># -*- coding: utf-8 -*-
&#39;&#39;&#39;
    Created by hushiwei on 2018/1/5.
    监控SparkStreaming程序
    一旦挂了,执行重启,同时发送邮件和微信报警
&#39;&#39;&#39;

import os
import subprocess
import json
import logging
import time
import urllib2
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.header import Header

wechats = &quot;HuShiwei&quot;
sendEmails = [&#39;hsw_v5@163.com&#39;, &#39;xxxx@gm825.com&#39;]

urlRun = &#39;curl --compressed -H &quot;Accept: application/json&quot; -X GET &quot;http://u007:8089/ws/v1/cluster/apps?states=RUNNING&quot;&#39;
urlAcc = &#39;curl --compressed -H &quot;Accept: application/json&quot; -X GET &quot;http://u007:8089/ws/v1/cluster/apps?states=ACCEPTED&quot;&#39;

monitorPrograms = {
    &quot;com.xxxx.streaming.ADXStreaming&quot;: &quot;/home/hadoop/statistics/ad/adxstreaming/start_adx_streaming_yarn.sh&quot;,
    &quot;com.xxxx.online.streaming.DSPStreaming&quot;: &quot;/home/hadoop/statistics/ad/dsp_ad_puton/dsp_ad_puton_streaming/start_dsp_streaming_yarn_test.sh&quot;,
    &quot;com.xxxx.streaming.CPDAppStreaming&quot;: &quot;/home/hadoop/statistics/ad/dsp_app_promotion/start_dsp_app_promotion_yarn.sh&quot;
}


class WeChat(object):
    &#39;&#39;&#39;
    发送微信工具类
    &#39;&#39;&#39;

    def __init__(self, corpid, corpsecret, tokenpath):
        self.corpid = corpid
        self.corpsecret = corpsecret
        self.tokenpath = tokenpath
        self.logger = logging.getLogger(&#39;wechat&#39;)

    def saveToken(self):
        &#39;&#39;&#39;
        :return:
        &#39;&#39;&#39;
        try:
            with open(self.tokenpath, &#39;r&#39;) as f:
                token = f.read()
                if len(token) &lt; 10:
                    token = self.getToken()
                    self.logger.info(&quot;Can not get token from %s,prepare to get token on api which token is %s&quot; % (
                        self.tokenpath, token))
                    return token
                else:
                    return token
        except IOError:
            token = self.getToken()
            self.logger.info(
                &quot;Can not get token from %s,prepare to get token on api which token is %s&quot; % (self.tokenpath, token))
            return token

    def getToken(self):
        Url = &#39;https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=%s&amp;corpsecret=%s&#39; % (self.corpid, self.corpsecret)
        req = urllib2.Request(Url)
        result = urllib2.urlopen(req)
        json_access_token = json.loads(result.read())
        access_token = json_access_token[&#39;access_token&#39;]

        with open(self.tokenpath, &#39;w&#39;) as f:
            f.write(access_token)
        return access_token

    def setMessage(self, wechatids, text):
        token = self.saveToken()
        message = self.makeMessage(text)
        submiturl = &#39;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token={0}&#39;.format(token)
        data = {&quot;touser&quot;: wechatids, &quot;msgtype&quot;: &quot;text&quot;, &quot;agentid&quot;: &quot;1000002&quot;, &quot;text&quot;: {&quot;content&quot;: message}, &quot;safe&quot;: &quot;0&quot;}
        data = json.dumps(data, ensure_ascii=False)

        send_request = urllib2.Request(submiturl, data)

        self.logger.info(&quot;Send wechat %s&quot; % text)

        response = json.loads(urllib2.urlopen(send_request).read())

        if response[&#39;errcode&#39;] == 42001 or response[&#39;errcode&#39;] == 40014:
            self.logger.info(&quot;Send wechat errorcode : %s&quot; % response[&#39;errcode&#39;])
            os.remove(self.tokenpath)
            self.setMessage(wechatids, text)

    def makeMessage(self, text):
        def date():
            date = time.strftime(&#39;%m-%d %H:%M:%S&#39;, time.localtime())
            return date

        return &quot;%s \nCall Time:%s&quot; % (text, date())


class Message(object):
    &#39;&#39;&#39;
    构造邮箱发送的内容
    &#39;&#39;&#39;

    def format_str(self, strs):
        if not isinstance(strs, unicode):
            strs = unicode(strs)
        return strs

    def __init__(self, from_user, to_user, subject, content, with_attach=False):
        &#39;&#39;&#39;

        :param from_user: 谁发过来的邮件
        :param to_user: 发给谁
        :param subject: 邮件主题
        :param content: 邮件内容
        :param with_attach: 邮件是否包含附件
        &#39;&#39;&#39;

        if with_attach:
            self._message = MIMEMultipart()
            self._message.attach(MIMEText(content, &#39;plain&#39;, &#39;utf-8&#39;))
        else:
            self._message = MIMEText(content, &#39;plain&#39;, &#39;utf-8&#39;)

        self._message[&#39;Subject&#39;] = Header(subject, &#39;utf-8&#39;)
        self._message[&#39;From&#39;] = Header(self.format_str(from_user), &#39;utf-8&#39;)
        self._message[&#39;To&#39;] = Header(self.format_str(to_user), &#39;utf-8&#39;)
        self._with_attach = with_attach

    def attach(self, file_path):
        if self._with_attach == False:
            print &quot;Please init the Message with attr &#39;with_attach = True&#39;&quot;
            exit(1)
        if os.path.isfile(file_path) == False:
            print &quot;The file doesn`t exist!&quot;
            exit(1)
        atta = MIMEText(open(file_path, &#39;rb&#39;).read(), &#39;base64&#39;, &#39;utf-8&#39;)
        atta[&#39;Content-Type&#39;] = &#39;application/octet-stream&#39;
        atta[&#39;Content-Disposition&#39;] = &#39;attachment; filename=&quot;%s&quot;&#39; % Header(os.path.basename(file_path), &#39;utf-8&#39;)
        self._message.attach(atta)

    def getMessage(self):
        return self._message.as_string()


class SMTPClient(object):
    &#39;&#39;&#39;
    发送邮件工具类
    &#39;&#39;&#39;

    def __init__(self, hostname, port, user, passwd):
        &#39;&#39;&#39;
        初始化相关参数
        :param hostname: QQ邮箱:smtp.qq.com
        :param port: QQ邮箱ssl加密端口:465
        :param user: QQ邮箱账号
        :param passwd: QQ邮箱授权秘钥,在web qq邮箱上获取
        &#39;&#39;&#39;
        self._HOST = hostname
        self._PORT = port
        self._USER = user
        self._PASS = passwd

    def send(self, receivers, msg):
        &#39;&#39;&#39;
        发送邮件方法
        :param receivers: 邮件接收者,可以是多个.为列表
        :param msg: 发送的邮件内容
        :return:
        &#39;&#39;&#39;
        if isinstance(msg, Message) == False:
            print &quot;Error Message Instance!&quot;
            exit(1)
        try:
            smtpObj = smtplib.SMTP_SSL(self._HOST, self._PORT)
            smtpObj.connect(self._HOST)
            smtpObj.login(self._USER, self._PASS)
            smtpObj.sendmail(self._USER, receivers, msg.getMessage())
            return (1, &quot;邮件发送成功&quot;)
        except smtplib.SMTPException, e:
            return (0, &quot;Error: 无法发送邮件%s&quot; % e)


def run_it(cmd):
    &#39;&#39;&#39;
    通过python执行shell命令
    :param cmd:
    :return:
    &#39;&#39;&#39;
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True,
                         stderr=subprocess.PIPE)
    # print (&#39;running:%s&#39; % cmd)
    out, err = p.communicate()
    if p.returncode != 0:
        print (&quot;Non zero exit code:%s executing: %s \nerr course ---&gt; %s&quot; % (p.returncode, cmd, err))
    return out


def reStartSparkScript(scriptPath):
    &#39;&#39;&#39;
    执行spark脚本
    1.cd到脚本所在路径
    2.在改路径执行脚本
    :param scripyPath:
    :return:
    &#39;&#39;&#39;
    logger = logging.getLogger(&quot;Main&quot;)
    scriptDir, script = os.path.split(scriptPath)
    os.chdir(scriptDir)
    run_it(&quot;sh %s&quot; % script)
    logger.info(&quot;exec [ %s ] on [ %s ] &quot; % (script, scriptDir))


def collectMonitorStatus(yarnRestApi):
    &#39;&#39;&#39;
    从Yarn的Running接口或者Accept接口中获取我们需要监控的程序状态
    :param str: yarn的running接口或者accept接口
    :return:
    &#39;&#39;&#39;
    strUrl = run_it(yarnRestApi)
    result = []
    obj = json.loads(strUrl)
    if obj[&#39;apps&#39;] is None:
        return result
    else:
        apps = obj[&#39;apps&#39;][&#39;app&#39;]
        result = [(app[&#39;name&#39;], app[&#39;state&#39;]) for app in apps if app[&#39;name&#39;] in monitorPrograms]
        return result


def checkMonitorApps():
    &#39;&#39;&#39;
    调用yarn的running接口和accept接口
    判断这里面是否有我们需要监控的spark程序
        如果没有就执行报警和重启
    :return:
    &#39;&#39;&#39;

    logging.basicConfig(level=logging.INFO,
                        format=&#39;%(asctime)s - %(message)s&#39;,
                        datefmt=&#39;%Y-%m-%d %H:%M:%S&#39;)

    logger = logging.getLogger(&quot;Main&quot;)

    smpt_client = SMTPClient(&#39;smtp.qq.com&#39;, 465, &#39;694244330@qq.com&#39;, &#39;xxxxxx&#39;)
    wechat_client = WeChat(&#39;xxxxxxxxxxxxxx&#39;, &#39;xxxxxxxxxxxxxxxxxxxxxxxxx&#39;, &#39;/tmp/token.txt&#39;)

    runningStatus = collectMonitorStatus(urlRun)
    acceptStatus = collectMonitorStatus(urlAcc)

    runningAcceptApps = dict(runningStatus + acceptStatus)

    logger.info(&quot;SparkStreaming ON Yarn Running And Accept ===&gt;%s &quot; % str(runningAcceptApps))

    for monitor in monitorPrograms:
        if monitor not in runningAcceptApps:
            logging.info(&quot;[ %s ] is not running or accept,prepare to restart!&quot; % monitor)
            msg = Message(&quot;694244330@qq.com&quot;, &quot;hushwiei&quot;, monitor, &#39;%s is failed, prepare to resart! -- %s&#39; % (
                monitor, time.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;, time.localtime())))
            smpt_client.send(sendEmails, msg)
            wechat_client.setMessage(wechats, &quot;%s is not running or accept,prepare to restart!&quot; % monitor)
            reStartSparkScript(monitorPrograms[monitor])


def main():
    checkMonitorApps()


if __name__ == &#39;__main__&#39;:
    main()


</code></pre>

]]></content>
  </entry>
  
</feed>
