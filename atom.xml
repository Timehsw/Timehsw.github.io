<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Time渐行渐远]]></title>
  <link href="http://dmlcoding.com/atom.xml" rel="self"/>
  <link href="http://dmlcoding.com/"/>
  <updated>2018-03-29T18:30:13+08:00</updated>
  <id>http://dmlcoding.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Spark Metrics分析]]></title>
    <link href="http://dmlcoding.com/15223141973169.html"/>
    <updated>2018-03-29T17:03:17+08:00</updated>
    <id>http://dmlcoding.com/15223141973169.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">MetricsSystem类注释翻译</h2>

<p><img src="media/15223141973169/15223151043399.jpg" alt=""/></p>

<span id="more"></span><!-- more -->

<p>Spark Metrics System 结合源代码由特定的&quot;实例&quot;创建.<br/>
Sink,定期的拉取source中的度量数据(metrics data)到目标sink中.</p>

<p>&quot;实例&quot;指定&quot;谁&quot;(角色)使用metrics system.在spark中有几个角色,比如master,worker,executor,client driver.这些角色会创建度量系统进行监控.<br/>
所以实例代表这些角色.目前在spark中,已经实现了几个实例:master,worker,executor,driver,application.</p>

<p>&quot;Source&quot;指定&quot;从哪里&quot;(来源)去收集指标数据.在度量系统中,这存在两种来源:<br/>
1.Spark内部源,如MasterSource,WorkerSource等.它们将收集Spark组件的内部状态,这些源与实例相关,并将在创建特定度量标准系统后添加.<br/>
2.通用源,比如JvmSource,这些将收集低级别的状态,由配置进行配置并通过反射加载.</p>

<p>&quot;Sink&quot;指定将Metrics data 输出到哪里(目的地).多个sinks可以共存并且metrics可以都flush到这些sinks中.</p>

<p>Metrics配置格式如下所示</p>

<pre><code>[instance].[sink|source].[name].[options] = xxxx
</code></pre>

<p>[instance]可以是&quot;master&quot;,&quot;worker&quot;,&quot;executor&quot;,&quot;driver&quot;,&quot;applications&quot;这意味着只有指定的实例才具有这个属性.可以用&quot;*&quot;来替换实例名,这意味着所有的这些实例将拥有此属性.</p>

<p>[sink|source]表示此属性是属于source还是sink.这个字段只能是source或者sink.</p>

<p>[name] 指定source或者sink的名字.如果它是自定义的.</p>

<p>[options] 代表这个source或者sink的具体属性</p>

<h2 id="toc_1">Metrics系统</h2>

<p>Spark拥有一个基于Coda Hale Metrics Library的可配置Metrics系统,这个Metrics系统通过配置文件进行配置。</p>

<p>Spark的Metrics系统允许用户把Spark metrics信息报告到各种各样的sink包含HTTP和 JMX、CSV文件。</p>

<p>Spark的metrics系统解耦到每个Spark组件的实例中。每个实例里，你可以配置一组sink（metrics被报告到的地方）。</p>

<p>注：Coda Hale Metrics使用的Yammer.com开发的Metrics框架，官方网址是<a href="https://github.com/dropwizard/metrics">https://github.com/dropwizard/metrics</a> ，相关文档可在<a href="https://dropwizard.github.io/metrics">https://dropwizard.github.io/metrics</a> 查看。</p>

<h2 id="toc_2">配置文件</h2>

<p>默认的配置文件为“$SPARK_HOME/conf/metrics.properties”，Spark启动时候会自动加载它。</p>

<p>如果想修改配置文件位置，可以使用java的运行时属性<code>-Dspark.metrics.conf=xxx</code>进行修改。。</p>

<h2 id="toc_3">Spark的Metrics系统支持的实例：</h2>

<ul>
<li><p>master：Spark standalone模式的 master进程。</p></li>
<li><p>applications：master进程里的一个组件，为各种应用作汇报.</p></li>
<li><p>worker：Spark standalone模式的一个worker进程。</p></li>
<li><p>executor：一个Spark executor.</p></li>
<li><p>driver：Spark driver进程(该进程指创建SparkContext的那个).</p></li>
</ul>

<h2 id="toc_4">Spark的Metrics系统支持的Sink：</h2>

<p>　　Sink指定metrics信息发送到哪，每个instance可以设置一个或多个Sink。</p>

<p>　　Sink源码位于包org.apache.spark.metrics.sink中。</p>

<h3 id="toc_5">ConsoleSink</h3>

<p>　　记录Metrics信息到Console中。</p>

<table>
<thead>
<tr>
<th>名称</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>class</td>
<td>org.apache.spark.metrics.sink.ConsoleSink</td>
<td>Sink类</td>
</tr>
<tr>
<td>period</td>
<td>10</td>
<td>轮询间隔</td>
</tr>
<tr>
<td>unit</td>
<td>seconds</td>
<td>轮询间隔的单位</td>
</tr>
</tbody>
</table>

<h3 id="toc_6">CSVSink</h3>

<p>　　定期的把Metrics信息导出到CSV文件中。</p>

<table>
<thead>
<tr>
<th>名称</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>class</td>
<td>org.apache.spark.metrics.sink.CsvSink</td>
<td>Sink类</td>
</tr>
<tr>
<td>period</td>
<td>10</td>
<td>轮询间隔</td>
</tr>
<tr>
<td>unit</td>
<td>seconds</td>
<td>轮询间隔的单位</td>
</tr>
<tr>
<td>directory</td>
<td>/tmp</td>
<td>CSV文件存储的位置</td>
</tr>
</tbody>
</table>

<h3 id="toc_7">JmxSink</h3>

<p>可以通过JMX方式访问Mertics信息</p>

<table>
<thead>
<tr>
<th>名称</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>class</td>
<td>org.apache.spark.metrics.sink.JmxSink</td>
<td>Sink类</td>
</tr>
</tbody>
</table>

<h3 id="toc_8">MetricsServlet</h3>

<table>
<thead>
<tr>
<th>名称</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>class</td>
<td>org.apache.spark.metrics.sink.MetricsServlet</td>
<td>Sink类</td>
</tr>
<tr>
<td>path</td>
<td>VARIES*</td>
<td>Path prefix from the web server root</td>
</tr>
<tr>
<td>sample</td>
<td>false</td>
<td>Whether to show entire set of samples for histograms (&#39;false&#39; or &#39;true&#39;) ｜</td>
</tr>
</tbody>
</table>

<p>　　除master之外所有实例的默认路径为“/metrics/json”。</p>

<p>　　master有两个路径: “/metrics/aplications/json” App的信息、 “/metrics/master/json” Master的信息</p>

<h3 id="toc_9">GraphiteSink</h3>

<table>
<thead>
<tr>
<th>名称</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>class</td>
<td>org.apache.spark.metrics.sink.GraphiteSink</td>
<td>Sink类</td>
</tr>
<tr>
<td>host</td>
<td>NONE</td>
<td>Graphite服务器主机名</td>
</tr>
<tr>
<td>port</td>
<td>NONE</td>
<td>Graphite服务器端口</td>
</tr>
<tr>
<td>period</td>
<td>10</td>
<td>轮询间隔</td>
</tr>
<tr>
<td>unit</td>
<td>seconds</td>
<td>轮询间隔的单位</td>
</tr>
<tr>
<td>prefix</td>
<td>EMPTY STRING</td>
<td>Prefix to prepend to metric name</td>
</tr>
</tbody>
</table>

<h3 id="toc_10">GangliaSink</h3>

<p>由于Licene限制，默认没有放到默认的build里面。需要自己打包</p>

<table>
<thead>
<tr>
<th>名称</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>class</td>
<td>org.apache.spark.metrics.sink.GangliaSink</td>
<td>Sink类</td>
</tr>
<tr>
<td>host</td>
<td>NONE</td>
<td>Ganglia 服务器的主机名或multicast group</td>
</tr>
<tr>
<td>port</td>
<td>NONE</td>
<td>Ganglia服务器的端口</td>
</tr>
<tr>
<td>period</td>
<td>10</td>
<td>轮询间隔</td>
</tr>
<tr>
<td>unit</td>
<td>seconds</td>
<td>轮询间隔的单位</td>
</tr>
<tr>
<td>ttl</td>
<td>1</td>
<td>TTL of messages sent by Ganglia</td>
</tr>
<tr>
<td>mode</td>
<td>multicast</td>
<td>Ganglia网络模式(&#39;unicast&#39; or &#39;multicast&#39;)</td>
</tr>
</tbody>
</table>

<h2 id="toc_11">source：</h2>

<p>第一种为Spark内部source，MasterSource、WorkerSource等，它们会接收Spark组件的内部状态；<br/>
　　第二种为通用source，如：JvmSource，它收集低级别的状态</p>

<h2 id="toc_12">示例</h2>

<h3 id="toc_13">通过类名为所有实例开启ConsoleSink</h3>

<pre><code>*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink

ConsoleSink的轮询周期 *.sink.console.period=10

*.sink.console.unit=seconds

**Master实例重置轮询周期**

master.sink.console.period=15

master.sink.console.unit=seconds
</code></pre>

<h3 id="toc_14">通过类名为所有实例开启JmxSink</h3>

<pre><code>*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink
</code></pre>

<h3 id="toc_15">为所有实例开启CsvSink</h3>

<pre><code>*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink
</code></pre>

<p><strong>CsvSink的轮询周期</strong></p>

<pre><code>*.sink.csv.period=1

*.sink.csv.unit=minutes
</code></pre>

<p>Polling directory for CsvSink *.sink.csv.directory=/tmp/</p>

<p><strong>Worker实例重置轮询周期</strong></p>

<pre><code>worker.sink.csv.period=10

worker.sink.csv.unit=minutes
</code></pre>

<h3 id="toc_16">为master和worker、driver、executor开启jvm source</h3>

<pre><code>
master.source.jvm.class=org.apache.spark.metrics.source.JvmSource

worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource

driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource

executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
</code></pre>

<h2 id="toc_17">参考文档：</h2>

<pre><code>${SPARK_HOME}/conf/metrics.properties.template
http://spark.apache.org/docs/latest/monitoring.html
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop RPC运行机制]]></title>
    <link href="http://dmlcoding.com/15222053353935.html"/>
    <updated>2018-03-28T10:48:55+08:00</updated>
    <id>http://dmlcoding.com/15222053353935.html</id>
    <content type="html"><![CDATA[
<p>RPC是一种通过网络从远程计算机上请求服务的机制，封装了具体实现，使用户不需要了解底层网络技术。目前存在许多开源RPC框架，比较有名的有Thrift、Protocol Buffers和Avro。Hadoop RPC与他们一样，均由两部分组成：对象序列化和远程过程调用</p>

<p>什么是远程过程调用?</p>

<span id="more"></span><!-- more -->

<p>首先从字面上解释，“过程”在Java中指的就是对象中的方法，“远程”是指不同机器上的进程（狭义），或者不同的进程（广义）（为了简单，下文不对这种情况进行说明）。因此，RPC就是允许程序调用其他机器上的对象方法。</p>

<p>RPC是属于典型的C/S结构，提供服务的一方称为server，请求服务的一方称为client。server端提供对象方法供client端调用，被调用的对象方法的执行发生在server端。</p>

<p>上面这样解释其实已经很明白了,为了更直观的理解.<br/>
那么就不废话了,直接上一个例子,用代码说话.</p>

<h1 id="toc_0">RPC 实例</h1>

<blockquote>
<p>简单来说,就是在client调用远程服务器里面的方法就像在本地调用自己的方法一样.</p>
</blockquote>

<p>我们的实例用hadoop rpc框架,那么我们需要引入对应的依赖</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
        &lt;version&gt;2.7.1&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Hadoop RPC主要由三个大类组成，即RPC、Client和Server，分别对应对外编程接口、客户端实现和服务器实现</p>

<p>代码分为两部分</p>

<ul>
<li>服务端的逻辑代码(server-rpc)</li>
<li>客户端调用服务端的逻辑代码(client-rpc)</li>
</ul>

<p><img src="media/15222053353935/15222165453483.jpg" alt=""/></p>

<h3 id="toc_1">服务端server-rpc</h3>

<p>RPCLearningServiceInterface <br/>
定义一个接口,我们想要客户端调用的方法</p>

<pre><code>/**
 * Created by hushiwei on 2018/3/28.
 * desc : RPC协议是client端和server端之间的通信接口，它定义了server端对外提供的服务接口。
 */
public interface RPCLearningServiceInterface {

    // 协议版本号,不同版本号的client和sever之间不能相互通信
    static final long versionID = 1L;

    // 登录方法
    String loging(String name);

    // 乘法
    String multip(int a, int b);
}


</code></pre>

<p>RPCLearningServiceImpl</p>

<p>接口的具体实现类</p>

<pre><code>/**
 * Created by hushiwei on 2018/3/28.
 * desc :Hadoop RPC协议通常是一个Java接口，定义了server端对外提供的服务接口，需要在server端进行实现
 *
 * server端的协议实现中不需要关注Socket通信
 *
 */
public class RPCLearningServiceImpl implements RPCLearningServiceInterface {
    @Override
    public String loging(String name) {
        System.out.println(&quot;Exec Login function ...&quot;);
        return &quot;Hello, &quot; + name + &quot;. Welcome you!&quot;;
    }

    @Override
    public String multip(int a, int b) {
        System.out.println(&quot;Prepare to exec multip function in server ....&quot;);
        return a + &quot; * &quot; + b + &quot; = &quot; + a * b;
    }
}

</code></pre>

<p>RpcLearningServer</p>

<p>server-rpc的启动类</p>

<pre><code>
/**
 * Created by hushiwei on 2018/3/28.
 * desc :
 *
 * java -classpath MapReduce-1.0-SNAPSHOT-jar-with-dependencies.jar com.hushiwei.mr.rpc.rpcserver.RpcLearningServer
 */
public class RpcLearningServer {
    public static void main(String[] args) throws IOException {

        // 创建RPC.Builder实例builder,用于构造RPC server
        RPC.Builder builder = new RPC.Builder(new Configuration());

        // 向builder传递一些必要的参数,如主机,端口号,真实业务逻辑实例,协议接口
        builder.setBindAddress(&quot;localhost&quot;).setPort(45666)
                .setInstance(new RPCLearningServiceImpl())
                .setProtocol(RPCLearningServiceInterface.class);

        // 构造rpc server
        RPC.Server server = builder.build();

        // 启动server
        server.start();

    }
}

</code></pre>

<h3 id="toc_2">客户端client-rpc</h3>

<p>ClientController</p>

<p>客户端的代码里面调用了服务端提供的api</p>

<pre><code>/**
 * Created by hushiwei on 2018/3/28.
 * desc : ClientController is a RPC client
 */
public class ClientController {
    public static void main(String[] args) throws IOException {

        // 通过rpc拿到loginServiceinterface的代理对象
        RPCLearningServiceInterface service = RPC.getProxy(RPCLearningServiceInterface.class, 1L, new InetSocketAddress(&quot;localhost&quot;, 45666), new Configuration());

        // 像本地调用一样,就可以调用服务端的方法
        String result = service.loging(&quot;Hushiwei&quot;);
        System.out.println(&quot;rpc client result: &quot; + result);

        String res = service.multip(6, 8);
        System.out.println(res);


        // 关闭连接
        RPC.stopProxy(service);
    }
}


</code></pre>

<h3 id="toc_3">本地执行</h3>

<ul>
<li>为了方便,我在本机进行测试,服务器地址写的localhost</li>
<li>我在idea中进行开发的,直接run起来</li>
</ul>

<p>运行<code>RpcLearningServer</code>类,启动rpc服务<br/>
<img src="media/15222053353935/15222166749600.jpg" alt=""/></p>

<p>运行<code>ClientController</code>类,启动client,进行远程调用<br/>
<img src="media/15222053353935/15222167326547.jpg" alt=""/></p>

<p>此时,看服务端的输出<br/>
<img src="media/15222053353935/15222167785896.jpg" alt=""/></p>

<p>这个也说明了,远程调用过程是在服务端执行的.</p>

<h3 id="toc_4">远程执行</h3>

<p>刚刚在本地执行的,为了更说明问题.我把服务器地址从<code>localhost</code>改成服务器的主机名<code>u006</code>,然后代码编译打包,上传到服务器上.<br/>
在远程服务器上启动rpc-server.然后再本地执行客户端的代码.<br/>
这样更能清楚的明白rpc的运行过程了</p>

<pre><code>java -classpath MapReduce-1.0-SNAPSHOT-jar-with-dependencies.jar com.hushiwei.mr.rpc.rpcserver.RpcLearningServer
</code></pre>

<p><img src="media/15222053353935/15222171711365.jpg" alt=""/></p>

<p>在服务器上启动了rpc-server,现在客户端还没启动,服务器上也没有任何输出.</p>

<p>接着在本地执行客户端代码<br/>
<img src="media/15222053353935/15222172448137.jpg" alt=""/></p>

<p>紧跟着,服务器上也有相应的输出了.<br/>
<img src="media/15222053353935/15222172751812.jpg" alt=""/></p>

<p>补充:<br/>
在服务器上启动rpcserver后,可以用<code>jps -m</code>查看这个服务器的启动情况</p>

<pre><code>[hadoop@U006 ~]$ jps -m
30432 Jps -m
6979 RpcLearningServer
</code></pre>

<p>或者查看代码里面用的端口是否被占用中</p>

<pre><code>[hadoop@U006 ~]$ netstat -natp | grep 45666
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 ::ffff:10.10.25.13:45666    :::*                        LISTEN      6979/java
</code></pre>

<p>是不是很熟悉,我们曾经安装hadoop后,用<code>jps -m</code>也可以看到hadoop里面的一些进程.比如<br/>
<code>NameNode</code>、<code>DataNode</code>、<code>ResourceManager</code>、<code>NodeManager</code>.</p>

<p>所以这些进程都是作为一个RPCserver,并对外提供RPC服务的.</p>

<h1 id="toc_5">总结</h1>

<p>定义RPC协议,实现RPC协议<br/>
<img src="media/15222053353935/15222165453483.jpg" alt=""/></p>

<p>构造并启动RPC服务<br/>
<img src="media/15222053353935/15222174334242.jpg" alt=""/></p>

<p>构成RPC客户端并发送RPC请求<br/>
<img src="media/15222053353935/15222173920962.jpg" alt=""/></p>

<p>使用Hadoop RPC的4个步骤</p>

<ul>
<li>定义RPC协议</li>
<li>实现RPC协议</li>
<li>构造并启动RPC server</li>
<li>构造RPC client并发送RPC请求</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[概率分布函数和概率密度函数]]></title>
    <link href="http://dmlcoding.com/15221149143615.html"/>
    <updated>2018-03-27T09:41:54+08:00</updated>
    <id>http://dmlcoding.com/15221149143615.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>研究一个随机变量,不只要看它能取哪些值,更重要的是它取各种值的概率如何!</p>
</blockquote>

<ul>
<li>概率分布函数</li>
<li>概率密度函数</li>
</ul>

<h2 id="toc_0">离散型随机变量</h2>

<p>在理解这两个词之前</p>

<span id="more"></span><!-- more --> 

<ul>
<li>概率分布函数</li>
<li>概率密度函数</li>
</ul>

<p>先理解这两个词</p>

<ul>
<li>概率函数: 就是用函数的形式来表达概率.也叫分布律.</li>
<li>概率分布: 表达随机变量所有可能取值的分布情况.注意:是所有的取值</li>
<li>分布函数: 即,概率分布函数,</li>
</ul>

<p><img src="media/15221149143615/15221166166372.png" alt=""/></p>

<p>概率分布函数: 它是概率函数各个取值的累加结果.也叫累积概率函数</p>

<ul>
<li>分布函数</li>
</ul>

<h2 id="toc_1">连续型随机变量</h2>

<ul>
<li>概率密度函数: 等同于概率函数,why?</li>
</ul>

<p>在连续型随机变量中为啥要将概率函数叫成概率密度函数呢?<br/>
参考陈希孺老师所著的《概率论与数理统计》<br/>
<img src="media/15221149143615/15221159306143.png" alt=""/><br/>
<img src="media/15221149143615/15221159405543.png" alt=""/></p>

<p>概率密度函数用数学公式表示就是一个定积分的函数，定积分在数学中是用来求面积的，而在这里，你就把概率表示为面积即可！<br/>
<img src="media/15221149143615/15221159485806.png" alt=""/></p>

<p>左边是F(x)<code>连续型随机变量分布函数</code>画出的图形，右边是f(x)<code>连续型随机变量的概率密度函数</code>画出的图像，它们之间的关系就是，概率密度函数是分布函数的导函数。</p>

<p>两张图一对比，你就会发现，如果用右图中的面积来表示概率，利用图形就能很清楚的看出，哪些取值的概率更大！这样看起来是不是特别直观，特别爽！！所以，我们在表示连续型随机变量的概率时，用f(x)概率密度函数来表示，是非常好的！</p>

<h1 id="toc_2">参考</h1>

<p><a href="https://www.jianshu.com/p/b570b1ba92bb">应该如何理解概率分布函数和概率密度函数?</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[分布式系统的一致性]]></title>
    <link href="http://dmlcoding.com/15220339795263.html"/>
    <updated>2018-03-26T11:12:59+08:00</updated>
    <id>http://dmlcoding.com/15220339795263.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15220339795263/15221454798314.jpg" alt=""/></p>

<p>先上个图,未完待续......</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[docker 错误]]></title>
    <link href="http://dmlcoding.com/15217143404677.html"/>
    <updated>2018-03-22T18:25:40+08:00</updated>
    <id>http://dmlcoding.com/15217143404677.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">net/http: TLS handshake timeout</h2>

<p>下载镜像的时候出现<code>net/http: TLS handshake timeout</code>,<br/>
原因是docker默认镜像拉取地址为国外仓库下载速度较慢，则会报错“net/http: TLS handshake timeout”。</p>

<span id="more"></span><!-- more -->

<p>此时，只需要将拉取地址改为国内镜像仓库即可。</p>

<p>标准格式为：</p>

<pre><code>$ docker pull registry.docker-cn.com/myname/myrepo:mytag
</code></pre>

<p>例：</p>

<pre><code>$ docker pull registry.docker-cn.com/library/ubuntu:16.04
</code></pre>

<p>为了永久性保留更改，您可以修改 /etc/docker/daemon.json 文件并添加上 registry-mirrors 键值。</p>

<pre><code>{
  &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]
}
</code></pre>

<p>修改保存后重启 Docker 以使配置生效。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mac 搭建 区块链开发环境]]></title>
    <link href="http://dmlcoding.com/15216998348495.html"/>
    <updated>2018-03-22T14:23:54+08:00</updated>
    <id>http://dmlcoding.com/15216998348495.html</id>
    <content type="html"><![CDATA[
<ul>
<li><a href="https://docs.docker.com/docker-for-mac/install/">官方安装文档</a></li>
<li><a href="https://docs.docker.com/docker-for-mac/">官方使用文档</a></li>
</ul>

<span id="more"></span><!-- more -->

<h1 id="toc_0">用docker下载ubuntu镜像</h1>

<pre><code># 下载ubuntu镜像
docker pull ubuntu
# 进入镜像中
docker run -it ubuntu /bin/bash
# 查看ubuntu的发行版本
cat /etc/issue

</code></pre>

<p>新开一个终端查看</p>

<pre><code> hushiwei@hsw  ~/Docker  docker images
REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE
ubuntu                latest              f975c5035748        2 weeks ago         112MB
</code></pre>

<h2 id="toc_1">进入镜像后,即生成了一个容器.更新容易里面的系统</h2>

<p>跟新apt-get,安装常用开发工具</p>

<pre><code>apt-get update
apt-get install vim
apt-get install sudo
</code></pre>

<h2 id="toc_2">提高安全意识,添加普通用户</h2>

<pre><code>adduser deploy
su deploy
</code></pre>

<h2 id="toc_3">给普通用户sudo权限</h2>

<pre><code># root下执行
chmod 777 /etc/sudoers
vim /etc/sudoers
</code></pre>

<p>在/etc/sudoers文件里面添加一行,表示给deploy用户sudo权限,并且不需要密码<br/>
<img src="media/15216998348495/15217671764613.jpg" alt=""/></p>

<p>把这个文件的权限改回去</p>

<pre><code>chmod 440 /etc/sudoers
</code></pre>

<h1 id="toc_4">在容器中进行区块链环境的搭建</h1>

<p>注意:目前为止,我们还没退出容器过.</p>

<h2 id="toc_5">安装curl,nodejs</h2>

<pre><code>sudo apt-get install curl

# 安装nodejs的版本
root@a43d121b81a0:~# curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -
# 完成后输出
## Run `apt-get install nodejs` (as root) to install Node.js v8.x LTS Carbon and npm

# 然后安装nodejs
apt-get install nodejs

# 查看版本
node -v

npm -v
</code></pre>

<h2 id="toc_6">安装testrpc,truffle</h2>

<pre><code># 安装淘宝镜像,速度快
npm install -g cnpm --registry=https://registry.npm.taobao.org

sudo cnpm install -g ethereumjs-testrpc
# 完成后输入testrpc,进行校验

sudo cnpm install -g truffle

</code></pre>

<h1 id="toc_7">提交安装好环境的容器</h1>

<p>在开一个窗口,输入<code>docker ps</code> 找到正在运行的容器的<code>CONTAINER ID</code><br/>
刚刚在这个容器里面安装好了环境,那么我们需要保存一下,所以用docker commit提交这个<br/>
容器,成为镜像<br/>
<code><br/>
docker commit a43d121b81a0 testrpc-truffle-env:v1<br/>
</code><br/>
提交后,再查看images,比之前的ubuntu大了不少.毕竟装了那么多工具进去了<br/>
<code><br/>
 hushiwei@hsw  ~/Docker  docker images<br/>
REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE<br/>
testrpc-truffle-env   v1                  a2073b0b64cc        6 minutes ago       503MB<br/>
ubuntu                latest              f975c5035748        2 weeks ago         112MB<br/>
</code></p>

<h2 id="toc_8">退出再进去</h2>

<p>现在就算退出运行中的容器,之前安装的工具也都还在了.<br/>
执行下面的命令即可进入刚刚提交的镜像中.<code>a2073b0b64cc</code>是刚刚提交的镜像的id</p>

<pre><code>docker run -it a2073b0b64cc /bin/bash
</code></pre>

<h1 id="toc_9">安装idea插件进行开发</h1>

<p>在idea的Plugins中安装插件<code>Intellij-Solidity</code></p>

<h1 id="toc_10">docker挂载本地目录进行开发</h1>

<p>将本地的<code>/Users/hushiwei/BlockChain/ethereum</code>目录,与容器中的<code>/home/deploy</code>进行共享</p>

<pre><code>docker run -it -v /Users/hushiwei/BlockChain/ethereum:/home/deploy a2073b0b64cc /bin/bash

</code></pre>

<h1 id="toc_11">生成truffle代码</h1>

<p>在容器的<code>/home/deploy/demo3</code>目录下执行<code>truffle init</code> 生成项目</p>

<p><img src="media/15216998348495/15217712425759.jpg" alt=""/></p>

<p>本机对应的目录也可以看到<br/>
<img src="media/15216998348495/15217712751901.jpg" alt=""/></p>

<h2 id="toc_12">用idea打开ethereum目录作为项目.</h2>

<p><img src="media/15216998348495/15217713264100.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LeetCode解题思路]]></title>
    <link href="http://dmlcoding.com/15216328231070.html"/>
    <updated>2018-03-21T19:47:03+08:00</updated>
    <id>http://dmlcoding.com/15216328231070.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">Easy</h1>

<span id="more"></span><!-- more -->

<h2 id="toc_1"><u>01</u>TwoSum</h2>

<h2 id="toc_2"><u>07</u>ReverseInteger</h2>

<h2 id="toc_3"><u>09</u>PalindromeNumber</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Macbook 区块链开发]]></title>
    <link href="http://dmlcoding.com/15216132281232.html"/>
    <updated>2018-03-21T14:20:28+08:00</updated>
    <id>http://dmlcoding.com/15216132281232.html</id>
    <content type="html"><![CDATA[
<p>基本概念就不说了,网上一大堆.</p>

<h2 id="toc_0">客户端安装</h2>

<pre><code>brew tap ethereum/ethereum

brew install ethereum

</code></pre>

<span id="more"></span><!-- more -->

<p>很是花了一些时间呢<br/>
<code><br/>
  /usr/local/Cellar/ethereum/1.8.2: 9 files, 54.7MB, built in 26 minutes 26 seconds<br/>
</code></p>

<pre><code>
geth -h
geth console

</code></pre>

<pre><code>
 hushiwei@hsw  ~/BlockChain  geth console
INFO [03-21|15:05:35] Maximum peer count                       ETH=25 LES=0 total=25
INFO [03-21|15:05:35] Starting peer-to-peer node               instance=Geth/v1.8.2-stable/darwin-amd64/go1.10
INFO [03-21|15:05:35] Allocated cache and file handles         database=/Users/hushiwei/Library/Ethereum/geth/chaindata cache=768 handles=1024
INFO [03-21|15:05:35] Initialised chain configuration          config=&quot;{ChainID: 1 Homestead: 1150000 DAO: 1920000 DAOSupport: true EIP150: 2463000 EIP155: 2675000 EIP158: 2675000 Byzantium: 4370000 Constantinople: &lt;nil&gt; Engine: ethash}&quot;
INFO [03-21|15:05:35] Disk storage enabled for ethash caches   dir=/Users/hushiwei/Library/Ethereum/geth/ethash count=3
INFO [03-21|15:05:35] Disk storage enabled for ethash DAGs     dir=/Users/hushiwei/.ethash                      count=2
INFO [03-21|15:05:35] Initialising Ethereum protocol           versions=&quot;[63 62]&quot; network=1
INFO [03-21|15:05:35] Loaded most recent local header          number=0 hash=d4e567…cb8fa3 td=17179869184
INFO [03-21|15:05:35] Loaded most recent local full block      number=0 hash=d4e567…cb8fa3 td=17179869184
INFO [03-21|15:05:35] Loaded most recent local fast block      number=0 hash=d4e567…cb8fa3 td=17179869184
INFO [03-21|15:05:35] Loaded local transaction journal         transactions=0 dropped=0
INFO [03-21|15:05:35] Regenerated local transaction journal    transactions=0 accounts=0
INFO [03-21|15:05:35] Starting P2P networking
INFO [03-21|15:05:37] UDP listener up                          self=enode://239405879cbf4f9d8cfb1dafca4bab87b8f7ce27fbdb486347d7215b1de56663805d520d44044bb5ed324a79e41fbdd0be2adbbe5f31684cf528bf2faef4796f@[::]:30303
INFO [03-21|15:05:37] RLPx listener up                         self=enode://239405879cbf4f9d8cfb1dafca4bab87b8f7ce27fbdb486347d7215b1de56663805d520d44044bb5ed324a79e41fbdd0be2adbbe5f31684cf528bf2faef4796f@[::]:30303
INFO [03-21|15:05:37] IPC endpoint opened                      url=/Users/hushiwei/Library/Ethereum/geth.ipc
Welcome to the Geth JavaScript console!

instance: Geth/v1.8.2-stable/darwin-amd64/go1.10
 modules: admin:1.0 debug:1.0 eth:1.0 miner:1.0 net:1.0 personal:1.0 rpc:1.0 txpool:1.0 web3:1.0

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[大数据框架-开源协议]]></title>
    <link href="http://dmlcoding.com/15209079363420.html"/>
    <updated>2018-03-13T10:25:36+08:00</updated>
    <id>http://dmlcoding.com/15209079363420.html</id>
    <content type="html"><![CDATA[
<p>知道我们经常用的开源框架的开源协议么?</p>

<span id="more"></span><!-- more -->

<table>
<thead>
<tr>
<th>开源项目</th>
<th>开源协议</th>
</tr>
</thead>

<tbody>
<tr>
<td>Superset</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Yanagishima</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Kibana</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Spark</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Tensorflow</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>PlanOut</td>
<td>BSD License</td>
</tr>
<tr>
<td>Hadoop</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Hive</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Sqoop</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Impala</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Presto</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Hbase</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Mysql</td>
<td>GPL license</td>
</tr>
<tr>
<td>Elasticsearch</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>CDH</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Zookeeper</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Hue</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Oozie</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Flume</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Kafka</td>
<td>Apache License 2.0</td>
</tr>
<tr>
<td>Logstash</td>
<td>Apache License 2.0</td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zookeeper笔记]]></title>
    <link href="http://dmlcoding.com/15205689785115.html"/>
    <updated>2018-03-09T12:16:18+08:00</updated>
    <id>http://dmlcoding.com/15205689785115.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">基础知识</h2>

<h3 id="toc_1">znode</h3>

<span id="more"></span><!-- more -->

<h3 id="toc_2">基础api</h3>

<pre><code># 创建一个test_node节点,并包含数据testdata
[zk: localhost:2181(CONNECTED) 12] create /test_note testdata
Created /test_note
[zk: localhost:2181(CONNECTED) 13] ls /test_note
[]
[zk: localhost:2181(CONNECTED) 14] get /test_note
testdata
cZxid = 0x1cc0015709f
ctime = Fri Mar 09 12:15:41 CST 2018
mZxid = 0x1cc0015709f
mtime = Fri Mar 09 12:15:41 CST 2018
pZxid = 0x1cc0015709f
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 8
numChildren = 0

[zk: localhost:2181(CONNECTED) 15] delete /test_note
[zk: localhost:2181(CONNECTED) 16] get /test_note
Node does not exist: /test_note

</code></pre>

<h2 id="toc_3">主-从模式的例子</h2>

<pre><code># -e 表示临时性
# 创建一个临时性的znode,节点名称为master_example ,数据为master1.example.com:2223
# 一个临时节点会在会话过期或关闭时自动被删除
create -e /master_example &quot;master1.example.com:2223&quot;

# 列出目录树
ls /

# 获取master_example znode的元数据和数据
get /master_example
</code></pre>

<p>如果再次执行创建master_example的命令会报<code>Node already exists: /master_example</code></p>

<pre><code># 在master_example上设置一个监视点
stat /master_example true
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kafka-sparkstreaming自己维护offset]]></title>
    <link href="http://dmlcoding.com/15203071412162.html"/>
    <updated>2018-03-06T11:32:21+08:00</updated>
    <id>http://dmlcoding.com/15203071412162.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">auto.offset.reset</h2>

<p>By default, it will start consuming from the latest offset of each Kafka partition. If you set configuration auto.offset.reset in Kafka parameters to smallest, then it will start consuming from the smallest offset.</p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">OffsetRange</h2>

<p>topic主题,分区ID,起始offset,结束offset</p>

<h2 id="toc_2">重写思路</h2>

<p>因为spark源码中<code>KafkaCluster</code>类被限制在<strong>[spark]</strong>包下,所以我们如果想要在项目中调用这个类,那么只能在项目中也新建包<code>org.apache.spark.streaming.kafka</code>.然后再该包下面写调用的逻辑.这里面就可以引用<code>KafkaCluster</code>类了.这个类里面封装了很多实用的方法,比如:获取主题和分区,获取offset等等...</p>

<p>这些api,spark里面都有现成的,我们现在就是需要组织起来!</p>

<pre><code>offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
</code></pre>

<p>简单说一下<br/>
1. 在zookeeper上读取offset前先根据实际情况更新<code>fromOffsets</code><br/>
    1.1 如果是一个新的groupid,那么会从最新的开始读<br/>
    1.2 如果是存在的groupid,根据配置<code>auto.offset.reset</code><br/>
        1.2.1 <code>smallest</code> : 那么会从开始读,获取最开始的offset.<br/>
        1.2.2 <code>largest</code> : 那么会从最新的位置开始读取,获取最新的offset.</p>

<ol>
<li>根据topic获取topic和该topics下所有的partitions</li>
</ol>

<pre><code>val partitionsE = kc.getPartitions(topics)
</code></pre>

<ol>
<li>传入上面获取到的topics和该分区所有的partitions</li>
</ol>

<pre><code>val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions)
</code></pre>

<ol>
<li>获取到该topic下所有分区的offset了.最后还是调用spark中封装好了的api</li>
</ol>

<pre><code>KafkaUtils.createDirectStream[K, V, KD, VD, (K, V, String)](
                ssc, kafkaParams, consumerOffsets, (mmd: MessageAndMetadata[K, V]) =&gt; ( mmd.key, mmd.message, mmd.topic))
</code></pre>

<ol>
<li>更新zookeeper中的kafka消息的偏移量</li>
</ol>

<pre><code>kc.setConsumerOffsets(groupId, Map((topicAndPartition, offsets.untilOffset)))
</code></pre>

<h2 id="toc_3">问题</h2>

<p>sparkstreaming-kafka的源码中是自己把offset维护在kafka集群中了?</p>

<pre><code>./kafka-consumer-groups.sh --bootstrap-server 10.10.25.13:9092 --describe  --group heheda
</code></pre>

<p>因为用命令行工具可以查到,这个工具可以查到基于javaapi方式的offset,查不到在zookeeper中的</p>

<p>网上的自己维护offset,是把offset维护在zookeeper中了?<br/>
用这个方式产生的groupid,在命令行工具中查不到,但是也是调用的源码中的方法呢?<br/>
难道spark提供了这个方法,但是自己却没有用是吗?</p>

<h2 id="toc_4">自己维护和用原生的区别</h2>

<p>区别只在于,自己维护offset,会先去zk中获取offset,逻辑处理完成后再更新zk中的offset.<br/>
然而,在代码层面,区别在于调用了不同的<code>KafkaUtils.createDirectStream</code></p>

<h3 id="toc_5">自己维护</h3>

<p>自己维护的offset,这个方法会传入offset.因为在此之前我们已经从zk中获取到了起始offset</p>

<pre><code>KafkaUtils.createDirectStream[K, V, KD, VD, (K, V, String)](
                ssc, kafkaParams, consumerOffsets, (mmd: MessageAndMetadata[K, V]) =&gt; ( mmd.key, mmd.message, mmd.topic))
</code></pre>

<h3 id="toc_6">原生的</h3>

<p>接受的是一个topic,底层会根据topic去获取存储在kafka中的起始offset</p>

<pre><code>KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, myTopic)
</code></pre>

<p>接下来这个方法里面会调用<code>getFromOffsets</code>来获取起始的offset</p>

<pre><code>val kc = new KafkaCluster(kafkaParams)
val fromOffsets = getFromOffsets(kc, kafkaParams, topics)
</code></pre>

<h2 id="toc_7">代码</h2>

<p>这个代码,网上很多,GitHub上也有现成的了.这里我就不贴出来了!<br/>
这里主要还是学习代码的实现思路!</p>

<h2 id="toc_8">如何引用spark源码中限制了包的代码</h2>

<ol>
<li>新建和源码中同等的包名,如上所述.</li>
<li>把你需要的源码拷贝一份出来,但是可能源码里面又引用了别的,这个不一定好使.</li>
<li>在你需要引用的那个类里,把这个类的包名改成与你需要引用的包名一样.最简单的办法了</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[查看linux系统负荷]]></title>
    <link href="http://dmlcoding.com/15202200627560.html"/>
    <updated>2018-03-05T11:21:02+08:00</updated>
    <id>http://dmlcoding.com/15202200627560.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">uptime</h1>

<pre><code>~ uptime
17:54:40 up 846 days, 55 min, 10 users,  load average: 0.36, 0.33, 0.48
</code></pre>

<span id="more"></span><!-- more -->

<ul>
<li>这行信息的后半部部分,显示 **load average.  **它的意思是系统的平均负荷.</li>
<li>通过里面的三个数字,我们可以从中判断系统负荷是大还是小</li>
<li>这3个数字,分别代表1分钟,5分钟,15分钟内系统的平均负荷.</li>
<li>当CPU完全空闲的时候,平均负荷为0;当CPU工作量饱和的时候,平均负荷为1(假设只有一个核)</li>
<li>那么很显然,load average的值越低,比如0.2或者0.3,就说明电脑的工作量越小,系统负荷比较轻</li>
</ul>

<p>参考文档</p>

<p><a href="http://blog.scoutapp.com/articles/2009/07/31/understanding-load-averages">http://blog.scoutapp.com/articles/2009/07/31/understanding-load-averages</a></p>

<p>总结:</p>

<p>假设服务器有16个核.</p>

<p>那么当load average的值小于16的时候,表示现在系统负荷轻.当load average的值大于16就表示负荷过大了.</p>

<p>查看服务器有多少个CPU核心</p>

<p><u><strong>grep -c &#39;model name&#39; /proc/cpuinfo</strong></u></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MacbookPro 修改应用的默认语言]]></title>
    <link href="http://dmlcoding.com/15202199373374.html"/>
    <updated>2018-03-05T11:18:57+08:00</updated>
    <id>http://dmlcoding.com/15202199373374.html</id>
    <content type="html"><![CDATA[
<p>当我把macbook pro的默认系统语言从中文改成英文后,其他的应用的语言也都变成了英文了.</p>

<p>但是比如地图应用,我还是希望用中文呢?</p>

<h1 id="toc_0">将地图Maps应用的语言改成中文</h1>

<span id="more"></span><!-- more -->

<p>第一步</p>

<pre><code>You don&#39;t need to change language setting. 
You can simply set it in the Maps App&#39;s Menu at [View]-&gt; [Labels]-&gt; [Always Show Labels in English].
 Then cancel it.
</code></pre>

<p>第二步</p>

<pre><code>defaults write com.apple.Maps AppleLanguages &#39;(zh-CN)&#39;
</code></pre>

<hr/>

<pre><code># 英文 -&gt; 简体中文
defaults write com.google.Chrome AppleLanguages &#39;(zh-CN)&#39;

# 简体中文 -&gt; 英文
defaults write com.google.Chrome AppleLanguages &#39;(en-US)&#39;

# 英文优先，简体中文第二。反之改一下顺序
defaults write com.google.Chrome AppleLanguages &quot;(en-US,zh-CN)&quot;
</code></pre>

<h1 id="toc_1">Calendar</h1>

<pre><code>defaults write com.apple.iCal AppleLanguages &#39;(zh-CN)&#39;
</code></pre>

<p>那么问题来了,从上面可以知道只要修改每个应用的系统语言即可</p>

<h1 id="toc_2">如何找到应用的包名</h1>

<p>以<strong>滴答清单</strong>为例</p>

<pre><code># 在&quot;&quot;双引号中输入应用名称,执行下面的命令即可返回应用的包名
hushiwei@hsw ~  osascript -e &#39;id of app &quot;Wunderlist&quot;&#39;
com.wunderkinder.wunderlistdesktop
</code></pre>

<p>知道了应用的包名,那么就可以执行上面的命令来修改应用的语言了</p>

<pre><code>defaults write com.wunderkinder.wunderlistdesktop AppleLanguages &#39;(zh-CN)&#39;
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[思考]]></title>
    <link href="http://dmlcoding.com/15199545224004.html"/>
    <updated>2018-03-02T09:35:22+08:00</updated>
    <id>http://dmlcoding.com/15199545224004.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">为什么有些人能够突飞猛进?</h2>

<ul>
<li>智商高</li>
<li>有想法,有思路</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kafka-sparkstreaming长时间运行后异常退出原因分析]]></title>
    <link href="http://dmlcoding.com/15199040835190.html"/>
    <updated>2018-03-01T19:34:43+08:00</updated>
    <id>http://dmlcoding.com/15199040835190.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">报错信息</h2>

<pre><code>Couldn`t find leader offsets for set([xxxxxxxxxxx,xx])

# 或者
numRecords must not be negative
</code></pre>

<span id="more"></span><!-- more -->

<h2 id="toc_1">分析过程</h2>

<p>从博客中已经找不到直接的解决方案，那么只能从源码入手，从而看看能不能定位到问题所在。<br/>
从这一行代码开始跟踪</p>

<pre><code>KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, myTopic)

</code></pre>

<p>发现了,先调用<code>getFromOffsets(kc, kafkaParams, topics)</code>获取到了起始offset.然后创建了一个<br/>
<code>DirectKafkaInputDStream</code>对象.点进去</p>

<pre><code>  def createDirectStream[
    K: ClassTag,
    V: ClassTag,
    KD &lt;: Decoder[K]: ClassTag,
    VD &lt;: Decoder[V]: ClassTag] (
      ssc: StreamingContext,
      kafkaParams: Map[String, String],
      topics: Set[String]
  ): InputDStream[(K, V)] = {
    val messageHandler = (mmd: MessageAndMetadata[K, V]) =&gt; (mmd.key, mmd.message)
    val kc = new KafkaCluster(kafkaParams)
    val fromOffsets = getFromOffsets(kc, kafkaParams, topics)
    new DirectKafkaInputDStream[K, V, KD, VD, (K, V)](
      ssc, kafkaParams, fromOffsets, messageHandler)
  }

</code></pre>

<p>找到这个对象里面的<code>compute</code>方法,重点从这个方法的逻辑开始分析</p>

<pre><code>  override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {
    val untilOffsets = clamp(latestLeaderOffsets(maxRetries))
    val rdd = KafkaRDD[K, V, U, T, R](
      context.sparkContext, kafkaParams, currentOffsets, untilOffsets, messageHandler)

    // Report the record number and metadata of this batch interval to InputInfoTracker.
    val offsetRanges = currentOffsets.map { case (tp, fo) =&gt;
      val uo = untilOffsets(tp)
      OffsetRange(tp.topic, tp.partition, fo, uo.offset)
    }
    val description = offsetRanges.filter { offsetRange =&gt;
      // Don&#39;t display empty ranges.
      offsetRange.fromOffset != offsetRange.untilOffset
    }.map { offsetRange =&gt;
      s&quot;topic: ${offsetRange.topic}\tpartition: ${offsetRange.partition}\t&quot; +
        s&quot;offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}&quot;
    }.mkString(&quot;\n&quot;)
    // Copy offsetRanges to immutable.List to prevent from being modified by the user
    val metadata = Map(
      &quot;offsets&quot; -&gt; offsetRanges.toList,
      StreamInputInfo.METADATA_KEY_DESCRIPTION -&gt; description)
    val inputInfo = StreamInputInfo(id, rdd.count, metadata)
    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)

    currentOffsets = untilOffsets.map(kv =&gt; kv._1 -&gt; kv._2.offset)
    Some(rdd)
  }
</code></pre>

<p>分析过程不仔细说了,感兴趣的在compute方法处打个断点,跟踪一下相关变量的值,应该就能明白一个大概了.</p>

<h2 id="toc_2">解决办法</h2>

<p>综上所述,解决办法为添加以下参数:</p>

<pre><code>--conf spark.streaming.kafka.maxRetries=50 \
--conf spark.yarn.maxAppAttempts=10 \
--conf spark.yarn.am.attemptFailuresValidityInterval=1h \
</code></pre>

<p>注意:<br/>
1. spark.yarn.maxAppAttempts参数不能超过hadoop集群yarn.resourcemanager.am.max-attempts的最大值，如果超过将会取两者较小值<br/>
2. 由于kafka切换leader导致的数据丢失,然后造成sparkstreaming程序报错,异常退出的情况下.我这通通常都是重启kafka,删除checkpoint,再重启sparkstreaming了.<br/>
3. 后面我们不用原生的方式来存储offset.而是自己维护offset,那么以后更新迭代后就不用再删除checkpoint了,对业务的影响也最小了.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch：用Curator辅助Marvel，实现自动删除旧marvel索引]]></title>
    <link href="http://dmlcoding.com/15198768495613.html"/>
    <updated>2018-03-01T12:00:49+08:00</updated>
    <id>http://dmlcoding.com/15198768495613.html</id>
    <content type="html"><![CDATA[
<p>Marvel几乎是所有Elasticsearch用户的标配。Marvel保留观测数据的代价是，<br/>
它默认每天会新建一个index，命名规律像是这样：.marvel-2017-12-10。<br/>
marvel自建的索引一天可以产生大概500M的数据，而且将会越来越多，占的容量也将越来越大。<br/>
有没有什么办法能让它自动过期？比如说只保留最近两天的观测数据，其他的都抛弃掉。</p>

<p>当然有办法,curator就可以帮你实现.</p>

<span id="more"></span><!-- more -->

<h1 id="toc_0">curator是什么？</h1>

<p>它是一个命令，可以帮助你管理你在Elasticsearch中的索引，帮你删除，关闭（close），<br/>
打开（open）它们。当然这是比较片面的说法，更完整的说明见：<br/>
<a href="https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html">https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html</a></p>

<h1 id="toc_1">实践</h1>

<p>我们集群里面安装的Elasticsearch的版本是2.1.1.<br/>
按照官网,我装了最新的5.x版本,显示版本不对.<br/>
按照 <a href="http://blog.csdn.net/hereiskxm/article/details/47423715">http://blog.csdn.net/hereiskxm/article/details/47423715</a>  这个博客,我装了3.3.0版本.<br/>
显示也不对.</p>

<p>然后我搜了一下,感觉应该装一个中间的版本,因此我安装了4.0.0版本<br/>
<code><br/>
pip install elasticsearch-curator (4.0.0)<br/>
</code></p>

<p>然后我看了一下这个版本提供的参数</p>

<pre><code> curator --help
Usage: curator [OPTIONS] ACTION_FILE

  Curator for Elasticsearch indices.

  See http://elastic.co/guide/en/elasticsearch/client/curator/current

Options:
  --config PATH  Path to configuration file. Default: ~/.curator/curator.yml
  --dry-run      Do not perform any changes.
  --version      Show the version and exit.
  --help         Show this message and exit.
</code></pre>

<p>和我安装最新的5.X的版本看起来是一致的.正好在这个站点看到配置的办法<br/>
<a href="https://stackoverflow.com/questions/33430055/removing-old-indices-in-elasticsearch/42268400#42268400">https://stackoverflow.com/questions/33430055/removing-old-indices-in-elasticsearch/42268400#42268400</a></p>

<p>之前在博客里面看到的那个3.3.0版本,还不兼容呢.</p>

<h2 id="toc_2">用法</h2>

<blockquote>
<p>目的是删除2天前以.marvel开头的索引</p>
</blockquote>

<p>新建目录 /opt/curator</p>

<pre><code>~ pwd
/opt/curator
~ ll
total 12
-rw-r--r-- 1 root root  184 Dec 12 10:48 config_file.yml
-rw-r--r-- 1 root root 1311 Dec 12 10:37 delete_marvel_indices.yml
drwxr-xr-x 2 root root 4096 Dec 12 10:49 logs
</code></pre>

<h2 id="toc_3">config_file.yml</h2>

<pre><code># 记住,这个logfile得提前新建好.不然会启动报错.
vim config_file.yml

---
client:
  hosts:
   - 10.10.25.217
  port: 9200
logging:
  loglevel: INFO
  logfile: &quot;/opt/curator/logs/actions.log&quot;
  logformat: default
  blacklist: [&#39;elasticsearch&#39;, &#39;urllib3&#39;]

</code></pre>

<h2 id="toc_4">delete_marvel_indices.yml</h2>

<p>删除以.marvel前缀且是2天之前的索引</p>

<h2 id="toc_5">```</h2>

<h1 id="toc_6">Remember, leave a key empty if there is no value.  None will be a string,</h1>

<h1 id="toc_7">not a Python &quot;NoneType&quot;</h1>

<h1 id="toc_8">Also remember that all examples have &#39;disable_action&#39; set to True.  If you</h1>

<h1 id="toc_9">want to use this action as a template, be sure to set this to False after</h1>

<h1 id="toc_10">copying it.</h1>

<p>actions:<br/>
  1:<br/>
    action: delete_indices<br/>
    description: &gt;-<br/>
      Delete indices older than 30 days (based on index name), for rc- prefixed indices.<br/>
    options:<br/>
      ignore_empty_list: True<br/>
      timeout_override:<br/>
      continue_if_exception: False<br/>
      disable_action: False<br/>
    filters:<br/>
    - filtertype: pattern<br/>
      kind: prefix<br/>
      value: rc-<br/>
      exclude:<br/>
    - filtertype: age<br/>
      source: name<br/>
      direction: older<br/>
      timestring: &#39;%Y.%m.%d&#39;<br/>
      unit: days<br/>
      unit_count: 30<br/>
      exclude:<br/>
  2:<br/>
    action: delete_indices<br/>
    description: &gt;-</p>

<pre><code>  Delete indices older than 2 days (based on index name), for .marvel prefixed indices.
options:
  ignore_empty_list: True
  timeout_override:
  continue_if_exception: False
  disable_action: False
filters:
- filtertype: pattern
  kind: prefix
  value: .marvel
  exclude:
- filtertype: age
  source: name
  direction: older
  timestring: &#39;%Y.%m.%d&#39;
  unit: days
  unit_count: 2
  exclude:
</code></pre>

<pre><code>
配置完成.

## 执行命令

</code></pre>

<p>curator --config config_file.yml [--dry-run] delete_marvel_indices.yml<br/>
```</p>

<p>注意:<br/>
1. --dry-run 是可选参数,加上后不会真的删除,只会执行逻辑.你可以通过看日志来判断是否正确.<br/>
确认正确后,去掉--dry-run参数,再执行命令,既是真正的执行删除了.<br/>
2. 如果没有在config_file.yml里面配置logfile参数,那么日志会在console打印出来.</p>

<h1 id="toc_11">配置日常任务</h1>

<p>很明显,我们需要自动化这个过程,让它每天自动执行,因此写一个脚本,让crontab每天自动调用即可</p>

<pre><code>#!/bin/bash

curator --config /opt/curator/config_file.yml  /opt/curator/delete_marvel_indices.yml

echo &quot;delete success&quot;

</code></pre>

<p>配置crontab</p>

<pre><code># 每天2点执行删除脚本
0 2 * * * source /etc/profile;bash /opt/curator/delete_marvel_daily.sh &gt; /opt/curator/delete.log 2&gt;&amp;1

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[机器学习实战之朴素贝叶斯]]></title>
    <link href="http://dmlcoding.com/15198746079468.html"/>
    <updated>2018-03-01T11:23:27+08:00</updated>
    <id>http://dmlcoding.com/15198746079468.html</id>
    <content type="html"><![CDATA[
<p>朴素贝叶斯就是利用先验知识来解决后验概率，因为训练集中我们已经知道了每个单词在类别0和1中的概率，即p(w|c),<br/>
我们就是要利用这个知识去解决在出现这些单词的组合情况下，类别更可能是0还是1,即p(c|w)。<br/>
如果说之前的训练样本少，那么这个p(w|c)就更可能不准确，所以样本越多我们会觉得这个p(w|c)越可信。</p>

<span id="more"></span><!-- more -->

<pre><code class="language-python">import os
import sys
from numpy import *
sys.path.append(os.getcwd())
</code></pre>

<pre><code class="language-python">import bayes
</code></pre>

<pre><code class="language-python"># 返回实验样本和类别标签(侮辱类和非侮辱类)
listOPosts,listClasses=bayes.loadDataSet()
</code></pre>

<pre><code class="language-python"># 创建词汇表
# 将实验样本里面的词汇进行去重
def createVocabList(dataSet):
    vocabSet=set([])
    for document in dataSet:
        vocabSet=vocabSet|set(document)
    return list(vocabSet)
</code></pre>

<pre><code class="language-python">myVocabList=createVocabList(listOPosts)
</code></pre>

<pre><code class="language-python"># 将词汇转成特征向量
# 也就是将每一行样本转成特征向量
# 向量的每一元素为1或者0,分别表示词汇表中的单词在输入文档中是否出现
def setOfWordsVec(vocabList,inputSet):
    returnVec=[0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)]=1
        else:print &quot;the word:%s is not in my Vocabulary!&quot; % word
    return returnVec
</code></pre>

<h1 id="toc_0">训练算法:从词向量计算概率</h1>

<p>朴素贝叶斯分类器训练函数</p>

<pre><code class="language-python"># 输入为文档矩阵以及由每篇文档类别标签所构成的向量
def trainNB0(trainMatrix,trainCategory):
    # 文档总数,有几篇文档,在这里也就是有几个一维数组
    numTrainDocs=len(trainMatrix)
    # 每篇文档里面的单词数,也就是一维数组的长度
    numWords=len(trainMatrix[0])
    # 因为就只有0和1两个分类,将类别列表求和后,就是其中一个类别的个数
    # 然后numTrainDocs也就是文档总数,这样相除后就是这个类别的概率了
    pAbusive=sum(trainCategory)/float(numTrainDocs)
    # 以下两行,初始化概率
    p0Num=ones(numWords);p1Num=ones(numWords)
    p0Denom=2.0;p1Denom=2.0

    # 依次遍历所有的文档
    for i in range(numTrainDocs):
        # 判断这个文档所属类别
        if trainCategory[i]==1:
            # 数组与数组相加,这里就是统计每个词在这个分类里面出现的次数
            p1Num+=trainMatrix[i]
            # 统计该类别下,这些词语一共出现了多少次
            p1Denom+=sum(trainMatrix[i])
        else:
            p0Num+=trainMatrix[i]
            p0Denom+=sum(trainMatrix[i])
    # 通过求对数避免数据下溢出
    p1Vect=log(p1Num/p1Denom)
    p0Vect=log(p0Num/p0Denom)
    return p0Vect,p1Vect,pAbusive
</code></pre>

<pre><code class="language-python"># 所有文档的特征向量
trainMat=[]
</code></pre>

<pre><code class="language-python"># 将文档的每一行,转成词向量,然后追加到trainMat中
for postinDoc in listOPosts:
    trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc))
</code></pre>

<pre><code class="language-python">p0V,p1V,pAb=trainNB0(trainMat,listClasses)
</code></pre>

<pre><code class="language-python">pAb
</code></pre>

<pre><code>0.5
</code></pre>

<pre><code class="language-python">p0V
</code></pre>

<pre><code>array([-2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,
       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -2.56494936,
       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,
       -2.15948425, -3.25809654, -3.25809654, -2.56494936, -3.25809654,
       -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,
       -2.56494936, -3.25809654, -2.56494936, -3.25809654, -2.56494936,
       -2.56494936, -1.87180218])
</code></pre>

<pre><code class="language-python">p1V
</code></pre>

<pre><code>array([-3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,
       -2.35137526, -2.35137526, -2.35137526, -3.04452244, -1.94591015,
       -3.04452244, -2.35137526, -2.35137526, -3.04452244, -1.94591015,
       -3.04452244, -1.65822808, -3.04452244, -2.35137526, -3.04452244,
       -3.04452244, -3.04452244])
</code></pre>

<pre><code class="language-python"># 朴素贝叶斯分类函数
def classifyNB(vec2classify,p0Vec,p1Vec,pClass1):
    #元素相乘
    p1=sum(vec2classify*p1Vec)+log(pClass1)
    p0=sum(vec2classify*p0Vec)+log(pClass1)
    if p1&gt;p0:
        return 1
    else:
        return 0
</code></pre>

<pre><code class="language-python">def testingNB():
    listOPosts,listClasses=bayes.loadDataSet()
    myVocabList=createVocabList(listOPosts)
    trainMat=[]
    for postinDoc in listOPosts:
        trainMat.append(setOfWordsVec(myVocabList,postinDoc))

    p0V,p1V,pAb=trainNB0(array(trainMat),array(listClasses))

    testEntry=[&#39;love&#39;,&#39;my&#39;,&#39;dalmation&#39;]
    thisDoc=array(setOfWordsVec(myVocabList,testEntry))
    print testEntry,&#39;classified as : &#39;,classifyNB(thisDoc,p0V,p1V,pAb)

    testEntry=[&#39;stupid&#39;,&#39;garbage&#39;]
    thisDoc=array(setOfWordsVec(myVocabList,testEntry))
    print testEntry,&#39;classified as : &#39;,classifyNB(thisDoc,p0V,p1V,pAb)
</code></pre>

<pre><code class="language-python">testingNB()
</code></pre>

<pre><code>[&#39;love&#39;, &#39;my&#39;, &#39;dalmation&#39;] classified as :  0
[&#39;stupid&#39;, &#39;garbage&#39;] classified as :  1
</code></pre>

<h1 id="toc_1">使用朴素贝叶斯过滤垃圾邮件</h1>

<ul>
<li>收集数据：提供文本文件</li>
<li>准备数据：将文本文件解析成词条向量</li>
<li>分析数据：检查词条确保解析的正确性</li>
<li>训练算法：使用我们之前建立的trainNB0()函数</li>
<li>测试算法：使用classifyNB()</li>
</ul>

<pre><code class="language-python">mySent=&#39;This book is the best book on Python or M.L. I have ever laid eyes upon.&#39;
</code></pre>

<pre><code class="language-python"># 文件解析及完整的垃圾邮件测试函数
def textParse(bigString):
    import re
    listOfTokens=re.split(r&#39;\W*&#39;,bigString)
    return [tok.lower() for tok in listOfTokens if len(tok)&gt;2]
</code></pre>

<pre><code class="language-python">def spamTest():
    docList=[];classList=[];fullText=[]
    for i in range(1,26):
        # 导入邮件文本，并解析成词条
        wordList=textParse(open(&#39;email/spam/%d.txt&#39; %i).read())
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(1)

        wordList=textParse(open(&#39;email/ham/%d.txt&#39; %i).read())
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(0)
    # 生成词汇表
    vocabList=createVocabList(docList)

    # 随机构建训练集、测试集
    trainingSet=range(50);testSet=[]
    for i in range(10):
        randIndex=int(random.uniform(0,len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])

    # 生成测试集的特征向量
    trainMat=[];trainClasses=[]
    for docIndex in trainingSet:
        trainMat.append(setOfWordsVec(vocabList,docList[docIndex]))
        trainClasses.append(classList[docIndex])

    p0V,p1V,pSpam=trainNB0(trainMat,trainClasses)

    # 测试集，测试错误率
    errorCount=0
    for docIndex in testSet:
        wordVector=setOfWordsVec(vocabList,docList[docIndex])
        if classifyNB(wordVector,p0V,p1V,pSpam)!=classList[docIndex]:
            errorCount+=1
    print &#39;the error rate is : &#39;,float(errorCount)/len(testSet)
</code></pre>

<pre><code class="language-python">spamTest()
</code></pre>

<pre><code>the error rate is :  0.2
</code></pre>

<h1 id="toc_2">使用朴素贝叶斯分类器从个人广告中获取区域倾向</h1>

<ul>
<li>收集数据:从RSS源收集内容,这里需要对RSS源构建一个接口</li>
<li>准备数据:将文本文件解析成词条向量</li>
<li>分析数据:检查词条确保解析的正确性</li>
<li>训练算法:使用我们之前建立的trainNB0()函数</li>
<li>测试算法:观察错误率,确保分类器可用.可以修改切分程序,以降低错误率,提高分类结果.</li>
<li>使用算法:构建一个完整的程序,封装所有内容.给定两个RSS源,该程序会显示最常用的公共词.</li>
</ul>

<p>下面将使用来自不同城市的广告训练一个分类器，然后观察分类器的效果。我们的目的并不是使用该分类器进行分类，而是通过观察单词和条件概率值来发现与特定城市相关的内容。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Presto的学习笔记]]></title>
    <link href="http://dmlcoding.com/15198745641385.html"/>
    <updated>2018-03-01T11:22:44+08:00</updated>
    <id>http://dmlcoding.com/15198745641385.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0">是什么?可以做什么?</h1>

<ol>
<li>Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节。</li>
<li>Presto支持在线数据查询，包括Hive, Cassandra, 关系数据库以及专有数据存储。 一条Presto查询可以将多个数据源的数据进行合并，可以跨越整个组织进行分析。</li>
<li>作为Hive和Pig（Hive和Pig都是通过MapReduce的管道流来完成HDFS数据的查询）的替代者，Presto不仅可以访问HDFS，也可以操作不同的数据源，包括：RDBMS和其他的数据源（例如：Cassandra）。</li>
<li>查询后的数据自动分页,这个很不错.</li>
</ol>

<span id="more"></span><!-- more -->

<h1 id="toc_1">源码编译</h1>

<p>下载<strong>presto</strong>源码包地址:<a href="https://github.com/prestodb/presto/releases">https://github.com/prestodb/presto/releases</a></p>

<p>安装文档地址(注意这个中文文档的版本是0.100):</p>

<p>注意:</p>

<ul>
<li>jdk得是1.8以上</li>
<li>我是用的presto0.161</li>
</ul>

<pre><code>tar -xzvf presto-0.161.tar.gz

# 编译
./mvnw clean install -DskipTests
</code></pre>

<p>在pom.xml文件中加入阿里云的仓库,加速下载依赖</p>

<pre><code>&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;nexus-aliyun&lt;/id&gt;
        &lt;name&gt;Nexus aliyun&lt;/name&gt;
        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
</code></pre>

<p>编译报错</p>

<pre><code>[ERROR] Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.13:revision (default) on project presto-spi: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml -&gt; [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.13:revision (default) on project presto-spi: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:185)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:181)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.maven.plugin.MojoExecutionException: .git directory could not be found! Please specify a valid [dotGitDirectory] in your pom.xml
    at pl.project13.maven.git.GitCommitIdMojo.throwWhenRequiredDirectoryNotFound(GitCommitIdMojo.java:432)
    at pl.project13.maven.git.GitCommitIdMojo.execute(GitCommitIdMojo.java:337)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
</code></pre>

<p>解决办法</p>

<pre><code>pom文件中加入这个插件
&lt;pluginManagement&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;pl.project13.maven&lt;/groupId&gt;
                &lt;artifactId&gt;git-commit-id-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;skip&gt;true&lt;/skip&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/pluginManagement&gt;
</code></pre>

<h1 id="toc_2">部署包安装</h1>

<h2 id="toc_3">下载地址</h2>

<p>下载presto-cli(记住要下载的presto-cli-xxxx-executable.jar):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli">https://repo1.maven.org/maven2/com/facebook/presto/presto-cli</a></p>

<p>下载部署包的地址:<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-server/">https://repo1.maven.org/maven2/com/facebook/presto/presto-server/</a></p>

<h2 id="toc_4">服务器说明</h2>

<p>机器共有三台U006,U007,U008</p>

<ul>
<li>coordinator

<ul>
<li>U007</li>
</ul></li>
<li>discovery

<ul>
<li>U007</li>
</ul></li>
<li>worker

<ul>
<li>U006</li>
<li>U008</li>
</ul></li>
</ul>

<p>coordinator和worker的其他配置都是一样的,除了config.properties不一样.具体哪里不一样,看下面的配置说明.</p>

<h1 id="toc_5">配置说明</h1>

<p>以下配置均是presto0.161版本的.如果你的版本不一样,启动后如果报错了,那么可能是配置文件里面的参数和版本对应不上,请找相应版本的配置.</p>

<p>在presto-server-0.161目录下新建etc目录,下面的配置文件均在此etc目录下</p>

<pre><code>[druid@U007 presto-server-0.161]$ tree etc/
etc/
├── catalog
│   ├── hive.properties
│   └── jmx.properties
├── config.properties
├── jvm.config
├── log.properties
└── node.properties
</code></pre>

<h2 id="toc_6">jvm.config</h2>

<blockquote>
<p>包含一系列在启动JVM的时候需要使用的命令行选项。这份配置文件的格式是：一系列的选项，每行配置一个单独的选项。由于这些选项不在shell命令中使用。 因此即使将每个选项通过空格或者其他的分隔符分开，java程序也不会将这些选项分开，而是作为一个命令行选项处理,信息如下：</p>
</blockquote>

<p>VM 系统属性 <code>HADOOP_USER_NAME</code> 来指定用户名</p>

<pre><code>-server
-Xmx16G
-XX:+UseG1GC
-XX:G1HeapRegionSize=32M
-XX:+UseGCOverheadLimit
-XX:+ExplicitGCInvokesConcurrent
-XX:+HeapDumpOnOutOfMemoryError
-XX:OnOutOfMemoryError=kill -9 %p
-DHADOOP_USER_NAME=hdfs
</code></pre>

<h2 id="toc_7">log.properties</h2>

<blockquote>
<p>这个配置文件中允许你根据不同的日志结构设置不同的日志级别。每个logger都有一个名字（通常是使用logger的类的全标示类名）. Loggers通过名字中的“.“来表示层级和集成关系，信息如下：</p>
</blockquote>

<pre><code>com.facebook.presto=INFO
</code></pre>

<ul>
<li>配置日志等级，类似于log4j。四个等级：DEBUG,INFO,WARN,ERROR</li>
</ul>

<h2 id="toc_8">node.properties</h2>

<blockquote>
<p>包含针对于每个节点的特定的配置信息。 一个节点就是在一台机器上安装的Presto实例，<strong>etc/node.properties</strong>配置文件至少包含如下配置信息</p>
</blockquote>

<pre><code>node.environment=production
node.id=ffffffff-ffff-ffff-ffff-ffffffffffff # 每个节点的node.id一定要不一样
node.data-dir=/home/druid/data/presto # 计算临时存储目录,presto得有读写权限
</code></pre>

<p>说明:</p>

<ol>
<li>node.environment： 集群名称, 所有在同一个集群中的Presto节点必须拥有相同的集群名称.</li>
<li>node.id： 每个Presto节点的唯一标示。每个节点的node.id都必须是唯一的。在Presto进行重启或者升级过程中每个节点的node.id必须保持不变。如果在一个节点上安装多个Presto实例（例如：在同一台机器上安装多个Presto节点），那么每个Presto节点必须拥有唯一的node.id.</li>
<li>node.data-dir： 数据存储目录的位置（操作系统上的路径）, Presto将会把日期和数据存储在这个目录下</li>
</ol>

<h2 id="toc_9">config.properties</h2>

<h3 id="toc_10">coordinator 主节点配置</h3>

<pre><code>coordinator=true
node-scheduler.include-coordinator=false
http-server.http.port=8585
query.max-memory=10GB
discovery-server.enabled=true
discovery.uri=http://U007:8585
</code></pre>

<p>说明:</p>

<ol>
<li><p>coordinator表示此节点是否作为一个coordinator。每个节点可以是一个worker，也可以同时是一个coordinator，但作为性能考虑，一般大型机群最好将两者分开。</p></li>
<li><p>若coordinator设置成true，则此节点成为一个coordinator。</p></li>
<li><p>若node-scheduler.include-coordinator设置成true，则成为一个worker，两者可以同时设置成true，此节点拥有两种身份。在一个节点上的Presto server即作为coordinator又作为worke将会降低查询性能。因为如果一个服务器作为worker使用，那么大部分的资源都会被worker占用，那么就不会有足够的资源进行关键任务调度、管理和监控查询执行.</p></li>
<li><p>http-server.http.port：指定HTTP server的端口。Presto 使用 HTTP进行内部和外部的所有通讯.</p></li>
<li><p>query.max-memory=10GB：一个单独的任务使用的最大内存 (一个查询计划的某个执行部分会在一个特定的节点上执行)。 这个配置参数限制的GROUP BY语句中的Group的数目、JOIN关联中的右关联表的大小、ORDER BY语句中的行数和一个窗口函数中处理的行数。 该参数应该根据并发查询的数量和查询的复杂度进行调整。如果该参数设置的太低，很多查询将不能执行；但是如果设置的太高将会导致JVM把内存耗光.</p></li>
<li><p>discovery-server.enabled：Presto 通过Discovery 服务来找到集群中所有的节点。为了能够找到集群中所有的节点，每一个Presto实例都会在启动的时候将自己注册到discovery服务。Presto为了简化部署，并且也不想再增加一个新的服务进程，Presto coordinator 可以运行一个内嵌在coordinator 里面的Discovery 服务。这个内嵌的Discovery 服务和Presto共享HTTP server并且使用同样的端口.</p></li>
<li><p>discovery.uri：Discovery server的URI。由于启用了Presto coordinator内嵌的Discovery 服务，因此这个uri就是Presto coordinator的uri。注意：这个URI一定不能以“/“结尾</p></li>
</ol>

<h3 id="toc_11">worker节点配置</h3>

<pre><code>coordinator=false
node-scheduler.include-coordinator=true
http-server.http.port=8585
query.max-memory=5GB
query.max-memory-per-node=1GB
discovery.uri=http://U007:8585
</code></pre>

<h2 id="toc_12">catalog</h2>

<p>hive.properties(hive连接器的配置)</p>

<p>连接hive</p>

<pre><code># 在etc/catalog目录下,新建hive.properties文件,配置上hive的一些信息
[druid@U006 catalog]$ pwd
/home/druid/presto-server-0.161/etc/catalog
[druid@U006 catalog]$ more hive.properties
connector.name=hive-cdh5
hive.metastore.uri=thrift://U006:9083
hive.config.resources=/etc/hadoop/conf.cloudera.yarn/core-site.xml,/etc/hadoop/conf.clouder
a.yarn/hdfs-site.xml
</code></pre>

<p>保证每个节点presto对core-site.xml,hdfs-site.xml两个文件有读权限</p>

<h1 id="toc_13">启动停止presto</h1>

<h2 id="toc_14">单节点启动</h2>

<p>在每个节点依次执行启动脚本</p>

<pre><code># 后台运行
bin/launcher start
# 前台运行
bin/launcher run

# 重启presto
bin/launcher restart

# 停止presto
bin/launcher stop
</code></pre>

<h2 id="toc_15">批量启动停止脚本</h2>

<pre><code>ssh -t ${i} -C &#39;. /usr/local/bin/env.sh &amp;&amp; /usr/local/presto-server-0.161/bin/launcher restart&#39;
</code></pre>

<h1 id="toc_16">监控presto</h1>

<p>启动完成后,在浏览器输入:</p>

<pre><code>http://U007:8585
</code></pre>

<p>这个地址也就是coordinator的discovery.uri</p>

<h1 id="toc_17">cli连接</h1>

<h2 id="toc_18">连接器注意说明</h2>

<ul>
<li><p>cli下载地址(找到自己的版本下载):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli">https://repo1.maven.org/maven2/com/facebook/presto/presto-cli</a></p></li>
<li><p>连接器的配置文件必须是以<code>.properties</code>后缀结尾的,前面的名字就是连接器的catalog名字</p></li>
<li><p>每次新加连接器配置文件后,都需要在presto的所有机器上加上相同的配置文件,然后重启</p></li>
<li><p>要下载对应版本的cli连接器,不然可能不好使.名字类似<code>presto-cli-0.161-executable.jar</code></p></li>
</ul>

<h2 id="toc_19">hive连接器</h2>

<p>配置说明(hive连接器的配置在说catalog的时候已经配置好了,你可以回头看看):</p>

<ul>
<li>connector.name=hive-cdh5(根据你的hive版本来选择)</li>
<li>hive.metastore.uri=thrift://U006:9083(hive的metastore地址)</li>
<li>hive.config.resources=/etc/hadoop/conf.cloudera.yarn/core-(配置文件的地址)</li>
</ul>

<pre><code>chmod +x presto-cli-0.161-executable.jar
./presto-cli-0.161-executable.jar --server U007:8585 --catalog hive --schema default
或者 mv presto-cli-0.161-executable.jar presto都可以
./presto --server U007:8585 --catalog hive --schema default
执行该语句后在 presto shell 中执行: show tables 查看 hive 中的 default 库下的表。如果出现对应的表，表安装验证成功
</code></pre>

<h2 id="toc_20">jmx连接器</h2>

<ul>
<li>JMX提供了有关JVM中运行的Java虚拟机和软件的信息</li>
<li>jmx连接器用于在presto服务器中查询JMX信息</li>
</ul>

<p>在etc/catalog目录下新建<strong>jmx.properties</strong></p>

<pre><code>connector.name=jmx
</code></pre>

<p>现在连接presto cli以启用JMX插件</p>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog jmx --schema jmx
presto:jmx&gt; show schemas from jmx;
       Schema
--------------------
 current
 history
 information_schema
(3 rows)

Query 20171012_063601_00020_yuhat, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:00 [3 rows, 47B] [39 rows/s, 614B/s]
</code></pre>

<h2 id="toc_21">MySQL连接器</h2>

<p>vim etc/catalog/mysql.properties</p>

<pre><code>connector.name=mysql
connection-url=jdbc:mysql://10.10.25.13:3306
connection-user=root
connection-password=wankatest***
</code></pre>

<p>schema 后面跟的mysql的数据库,</p>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog mysql --schema test
presto:test&gt; show tables;
Query 20171012_071844_00002_iz4q8 failed: No worker nodes available

presto:test&gt; show tables;
          Table
--------------------------
 dmp_summary_daily_report
 tb_dmp_stat_appboot
 tb_dmp_stat_asdk_detail
 tb_dmp_stat_device
 tb_leidian1
 tb_leidian2
(6 rows)

Query 20171012_072024_00003_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:01 [6 rows, 190B] [6 rows/s, 196B/s]
</code></pre>

<h2 id="toc_22">kafka连接器</h2>

<p>暂时没这个需求,未测试.</p>

<h2 id="toc_23">系统连接器</h2>

<ul>
<li>系统连接器提供了正在运行的Presto集群的一些信息和指标</li>
<li>那么这个就可以通过标准sql很方便的查询这些信息</li>
<li>系统连接器不需要配置,已经内置了.我们可以很方便的访问名为<code>system</code>的catalog</li>
</ul>

<pre><code>[druid@U007 presto-server-0.161]$ ./presto --server U007:8585 --catalog system
presto&gt; show schemas from system;
       Schema
--------------------
 information_schema
 jdbc
 metadata
 runtime
(4 rows)

Query 20171012_082437_00019_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
0:00 [4 rows, 57B] [70 rows/s, 997B/s]
</code></pre>

<p>查询有多少个节点</p>

<pre><code>presto&gt; SELECT * FROM system.runtime.nodes;
                  node_id                  |        http_uri         | node_version | coord
-------------------------------------------+-------------------------+--------------+------
 ffffffff-ffff-ffff-ffff-ffffffffffff-u006 | http://10.10.25.13:8585 | 0.161        | false
 ffffffff-ffff-ffff-ffff-ffffffffffff-u007 | http://10.10.25.14:8585 | 0.161        | true
 ffffffff-ffff-ffff-ffff-ffffffffffff-u008 | http://10.10.25.15:8585 | 0.161        | false
(3 rows)

Query 20171012_082952_00023_iz4q8, FINISHED, 2 nodes
Splits: 2 total, 2 done (100.00%)
4:08 [3 rows, 228B] [0 rows/s, 0B/s]
</code></pre>

<h2 id="toc_24">JDBC接口</h2>

<h3 id="toc_25">依赖下载安装</h3>

<ul>
<li>下载地址(找到自己的版本下载):<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc">https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc</a></li>
<li><code>presto-jdbc-0.161.jar</code>在jar文件下载之后，将其添加到Java应用程序的classpath中。</li>
<li>我不太喜欢用jar包的方式,那么可以在pom文件加入presto-jdbc的依赖</li>
<li><a href="http://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc">http://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc</a> 找到自己相应的版本</li>
</ul>

<pre><code>&lt;!-- https://mvnrepository.com/artifact/com.facebook.presto/presto-jdbc --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.facebook.presto&lt;/groupId&gt;
    &lt;artifactId&gt;presto-jdbc&lt;/artifactId&gt;
    &lt;version&gt;0.161&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Presto支持的URL格式如下：</p>

<pre><code>jdbc:presto://host:port
jdbc:presto://host:port/catalog
jdbc:presto://host:port/catalog/schema
</code></pre>

<p>例如，可以使用下面的URL来连接运行在U007服务器8585端口上的Presto的mysql catalog中的test schema：</p>

<pre><code>jdbc:presto://10.10.25.14:8585/mysql/test
</code></pre>

<p>这个url就是来连接hive catalog中的default schema</p>

<pre><code>jdbc:presto://10.10.25.14:8585/hive/default
</code></pre>

<h3 id="toc_26">Java代码</h3>

<p><strong>读取mysql下的test库下的所有表</strong></p>

<pre><code>public class PrestoJdbcDemo {

  public static void main(String[] args) throws SQLException, ClassNotFoundException {
    Class.forName(&quot;com.facebook.presto.jdbc.PrestoDriver&quot;);
    Connection connection = DriverManager
        .getConnection(&quot;jdbc:presto://10.10.25.14:8585/mysql/test&quot;, &quot;root&quot;, &quot;wankatest***&quot;);
    Statement stmt = connection.createStatement();
    ResultSet rs = stmt.executeQuery(&quot;show tables&quot;);
    while (rs.next()) {
      System.out.println(rs.getString(1));
    }
    rs.close();
    connection.close();

  }
}
</code></pre>

<p><strong>读取hive下的default库下的所有表</strong></p>

<pre><code>public class PrestoJdbcDemo {

  public static void main(String[] args) throws SQLException, ClassNotFoundException {
    Class.forName(&quot;com.facebook.presto.jdbc.PrestoDriver&quot;);
    Connection connection = DriverManager
        .getConnection(&quot;jdbc:presto://10.10.25.14:8585/hive/default&quot;,&quot;root&quot;,null);
    Statement stmt = connection.createStatement();
    ResultSet rs = stmt.executeQuery(&quot;show tables&quot;);
    while (rs.next()) {
      System.out.println(rs.getString(1));
    }
    rs.close();
    connection.close();

  }
}
</code></pre>

<h1 id="toc_27">不同数据源之间的join</h1>

<p>presto的一个特性就是其支持在不同的数据源之间进行join</p>

<p>当连接presto的客户端的时候,也可以不指定连接器</p>

<p>不同的数据源就用catalog名称指定,然后加上库名表明即可.</p>

<pre><code>./presto --server U007:8585
 show tables from mysql.dsp_test;
</code></pre>

<h1 id="toc_28">presto提供的函数和运算符</h1>

<p>参考文档:<a href="http://prestodb-china.com/docs/current/functions.html">http://prestodb-china.com/docs/current/functions.html</a></p>

<h1 id="toc_29">看日志</h1>

<h2 id="toc_30">日志路径</h2>

<p><strong>node.properties</strong>中配置了node.data-dir=/home/druid/data/presto</p>

<pre><code>[druid@U007 presto]$ tree
.
├── etc -&gt; /home/druid/presto-server-0.161/etc
├── plugin -&gt; /home/druid/presto-server-0.161/plugin
└── var
    ├── log
    │   ├── http-request.log
    │   ├── launcher.log
    │   └── server.log
    └── run
        └── launcher.pid

5 directories, 4 files
</code></pre>

<p>出现错误后,我们主要关注var/log目录下的日志.</p>

<p>当服务有问题的时候,看server.log找到报错原因,从而解决问题.</p>

<h1 id="toc_31">常见错误</h1>

<h2 id="toc_32">连接不上连接器</h2>

<p>类似这样的错误</p>

<pre><code>No factory for connector mysql
No factory for connector hive
</code></pre>

<p>如果服务报相关这样的错误,那么就需要关注各个连接器的配置文件是否写对了.各个连接器的配置文件是否在每个presto服务器上都部署了.</p>

<h1 id="toc_33">Presto的实现原理</h1>

<p><img src="http://oz6wyfxp0.bkt.clouddn.com/1512097348.png?imageMogr2/thumbnail/!70p" alt="Presto架构"/></p>

<ul>
<li>Presto的架构</li>
<li>Presto执行原理</li>
</ul>

<p>简单来说:</p>

<ol>
<li>cli客户端把查询需求发送给Coordinator节点.Coordinator节点负责解析sql语句,生成执行计划,分发执行任务给worer节点执行.所以worker节点负责实际执行查询任务.</li>
<li>worker节点启动后向discovery server服务注册,因此coordinator就可以从discovery server获得可以正常工作的worker节点.</li>
</ol>

<p>深入来说:</p>

<ol>
<li>参考网上相关blog.</li>
<li>买书.</li>
<li>看源码!</li>
</ol>

<h1 id="toc_34">参考文档</h1>

<ul>
<li><p><a href="https://prestodb.io">presto官网</a></p></li>
<li><p><a href="https://github.com/prestodb/presto">presto-Github</a></p></li>
<li><p><a href="http://prestodb-china.com">presto-京东维护-中文文档</a></p></li>
<li><p><a href="https://tech.meituan.com/presto.html">Presto实现原理和美团的使用实践</a></p></li>
<li><p><a href="http://getindata.com/tutorial-using-presto-to-combine-data-from-hive-and-mysql-in-one-sql-like-query/">Presto-Hive-Mysql-InteractiveQuery</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[字符串编码转换]]></title>
    <link href="http://dmlcoding.com/15219482100406.html"/>
    <updated>2018-03-25T11:23:30+08:00</updated>
    <id>http://dmlcoding.com/15219482100406.html</id>
    <content type="html"><![CDATA[
<ul>
<li>乱码之类的几乎都是由汉字引起的。</li>
<li>任何平台的任何编码 都能和 Unicode 互相转换</li>
<li>UTF-8 与 GBK 互相转换，那就先把UTF-8转换成Unicode，再从Unicode转换成GBK，反之同理。</li>
</ul>

<span id="more"></span><!-- more -->

<h2 id="toc_0">decode与encode</h2>

<ul>
<li><code>decode</code>的作用是将其他编码的字符串转换成 Unicode 编码</li>
<li><code>encode</code>的作用是将 Unicode 编码转换成其他编码的字符串</li>
<li>一句话：UTF-8是对Unicode字符集进行编码的一种编码方式</li>
</ul>

<h2 id="toc_1">utf-8转gbk</h2>

<pre><code class="language-python"># 这是一个 UTF-8 编码的字符串
utf8Str = &quot;你好地球&quot;

# 1. 将 UTF-8 编码的字符串 转换成 Unicode 编码
unicodeStr = utf8Str.decode(&quot;UTF-8&quot;)

# 2. 再将 Unicode 编码格式字符串 转换成 GBK 编码
gbkData = unicodeStr.encode(&quot;GBK&quot;)

</code></pre>

<h2 id="toc_2">gbk转utf-8</h2>

<pre><code>
# 1. 再将 GBK 编码格式字符串 转化成 Unicode
unicodeStr = gbkData.decode(&quot;gbk&quot;)

# 2. 再将 Unicode 编码格式字符串转换成 UTF-8
utf8Str = unicodeStr.encode(&quot;UTF-8&quot;)

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[json和jsonPATH[整理自网络]]]></title>
    <link href="http://dmlcoding.com/15219476833792.html"/>
    <updated>2018-03-25T11:14:43+08:00</updated>
    <id>http://dmlcoding.com/15219476833792.html</id>
    <content type="html"><![CDATA[
<p>JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。</p>

<p>JSON和XML的比较可谓不相上下。</p>

<span id="more"></span><!-- more -->

<p>Python 2.7中自带了JSON模块，直接<code>import json</code>就可以使用了。</p>

<p>官方文档：<a href="http://docs.python.org/library/json.html">http://docs.python.org/library/json.html</a></p>

<p>Json在线解析网站：<a href="http://www.json.cn/#">http://www.json.cn/#</a></p>

<h1 id="toc_0">JSON</h1>

<p>json简单说就是javascript中的对象和数组，所以这两种结构就是对象和数组两种结构，通过这两种结构可以表示各种复杂的结构</p>

<ol>
<li><p>对象：对象在js中表示为<code>{ }</code>括起来的内容，数据结构为 <code>{ key：value, key：value, ... }</code>的键值对的结构，在面向对象的语言中，key为对象的属性，value为对应的属性值，所以很容易理解，取值方法为 对象.key 获取属性值，这个属性值的类型可以是数字、字符串、数组、对象这几种。</p></li>
<li><p>数组：数组在js中是中括号<code>[ ]</code>括起来的内容，数据结构为 <code>[&quot;Python&quot;, &quot;javascript&quot;, &quot;C++&quot;, ...]</code>，取值方式和所有语言中一样，使用索引获取，字段值的类型可以是 数字、字符串、数组、对象几种。</p></li>
</ol>

<h2 id="toc_1">导入json模块</h2>

<pre><code>import json
</code></pre>

<p>json模块提供了四个功能：<code>dumps</code>、<code>dump</code>、<code>loads</code>、<code>load</code>，用于字符串 和 python数据类型间进行转换。</p>

<h2 id="toc_2">1. json.loads()</h2>

<p>把Json格式字符串解码转换成Python对象 从json到python的类型转化对照如下：</p>

<p><img src="media/15219476833792/15219477820474.jpg" alt=""/></p>

<pre><code># json_loads.py

import json

strList = &#39;[1, 2, 3, 4]&#39;

strDict = &#39;{&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;大猫&quot;}&#39;

json.loads(strList) 
# [1, 2, 3, 4]

json.loads(strDict) # json数据自动按Unicode存储
# {u&#39;city&#39;: u&#39;\u5317\u4eac&#39;, u&#39;name&#39;: u&#39;\u5927\u732b&#39;}

</code></pre>

<h2 id="toc_3">2. json.dumps()</h2>

<p>实现python类型转化为json字符串，返回一个str对象 把一个Python对象编码转换成Json字符串</p>

<p>从python原始类型向json类型的转化对照如下：</p>

<p><img src="media/15219476833792/15219478024682.jpg" alt=""/></p>

<pre><code># json_dumps.py

import json
import chardet

listStr = [1, 2, 3, 4]
tupleStr = (1, 2, 3, 4)
dictStr = {&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;大猫&quot;}

json.dumps(listStr)
# &#39;[1, 2, 3, 4]&#39;
json.dumps(tupleStr)
# &#39;[1, 2, 3, 4]&#39;

# 注意：json.dumps() 序列化时默认使用的ascii编码
# 添加参数 ensure_ascii=False 禁用ascii编码，按utf-8编码
# chardet.detect()返回字典, 其中confidence是检测精确度

json.dumps(dictStr) 
# &#39;{&quot;city&quot;: &quot;\\u5317\\u4eac&quot;, &quot;name&quot;: &quot;\\u5927\\u5218&quot;}&#39;

chardet.detect(json.dumps(dictStr))
# {&#39;confidence&#39;: 1.0, &#39;encoding&#39;: &#39;ascii&#39;}

print json.dumps(dictStr, ensure_ascii=False) 
# {&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;大刘&quot;}

chardet.detect(json.dumps(dictStr, ensure_ascii=False))
# {&#39;confidence&#39;: 0.99, &#39;encoding&#39;: &#39;utf-8&#39;}

</code></pre>

<p><strong><em>chardet是一个非常优秀的编码识别模块，可通过pip安装</em></strong></p>

<h2 id="toc_4">3. json.dump()</h2>

<p>将Python内置类型序列化为json对象后写入文件</p>

<pre><code># json_dump.py

import json

listStr = [{&quot;city&quot;: &quot;北京&quot;}, {&quot;name&quot;: &quot;大刘&quot;}]
json.dump(listStr, open(&quot;listStr.json&quot;,&quot;w&quot;), ensure_ascii=False)

dictStr = {&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;大刘&quot;}
json.dump(dictStr, open(&quot;dictStr.json&quot;,&quot;w&quot;), ensure_ascii=False)

</code></pre>

<h2 id="toc_5">4. json.load()</h2>

<p>读取文件中json形式的字符串元素 转化成python类型</p>

<pre><code># json_load.py

import json

strList = json.load(open(&quot;listStr.json&quot;))
print strList

# [{u&#39;city&#39;: u&#39;\u5317\u4eac&#39;}, {u&#39;name&#39;: u&#39;\u5927\u5218&#39;}]

strDict = json.load(open(&quot;dictStr.json&quot;))
print strDict
# {u&#39;city&#39;: u&#39;\u5317\u4eac&#39;, u&#39;name&#39;: u&#39;\u5927\u5218&#39;}

</code></pre>

<h1 id="toc_6">JsonPath</h1>

<p>JsonPath 是一种信息抽取类库，是从JSON文档中抽取指定信息的工具，提供多种语言实现版本，包括：Javascript, Python， PHP 和 Java。</p>

<p>JsonPath 对于 JSON 来说，相当于 XPATH 对于 XML。</p>

<blockquote>
<p>下载地址：<a href="https://pypi.python.org/pypi/jsonpath/">https://pypi.python.org/pypi/jsonpath</a></p>

<p>安装方法：点击<code>Download URL</code>链接下载jsonpath，解压之后执行<code>python setup.py install</code></p>

<p>官方文档：<a href="http://goessner.net/articles/JsonPath/">http://goessner.net/articles/JsonPath</a></p>
</blockquote>

<h2 id="toc_7">JsonPath与XPath语法对比：</h2>

<p>Json结构清晰，可读性高，复杂度低，非常容易匹配，下表中对应了XPath的用法。</p>

<table>
<thead>
<tr>
<th style="text-align: center">XPath</th>
<th>JSONPath</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: center"><code>/</code></td>
<td><code>$</code></td>
<td>根节点</td>
</tr>
<tr>
<td style="text-align: center"><code>.</code></td>
<td><code>@</code></td>
<td>现行节点</td>
</tr>
<tr>
<td style="text-align: center"><code>/</code></td>
<td><code>.</code>or<code>[]</code></td>
<td>取子节点</td>
</tr>
<tr>
<td style="text-align: center"><code>..</code></td>
<td>n/a</td>
<td>取父节点，Jsonpath未支持</td>
</tr>
<tr>
<td style="text-align: center"><code>//</code></td>
<td><code>..</code></td>
<td>就是不管位置，选择所有符合条件的条件</td>
</tr>
<tr>
<td style="text-align: center"><code>*</code></td>
<td><code>*</code></td>
<td>匹配所有元素节点</td>
</tr>
<tr>
<td style="text-align: center"><code>@</code></td>
<td>n/a</td>
<td>根据属性访问，Json不支持，因为Json是个Key-value递归结构，不需要。</td>
</tr>
<tr>
<td style="text-align: center"><code>[]</code></td>
<td><code>[]</code></td>
<td>迭代器标示（可以在里边做简单的迭代操作，如数组下标，根据内容选值等）</td>
</tr>
<tr>
<td style="text-align: center"></td>
<td></td>
<td><code>[,]</code></td>
</tr>
<tr>
<td style="text-align: center"><code>[]</code></td>
<td><code>?()</code></td>
<td>支持过滤操作.</td>
</tr>
<tr>
<td style="text-align: center">n/a</td>
<td><code>()</code></td>
<td>支持表达式计算</td>
</tr>
<tr>
<td style="text-align: center"><code>()</code></td>
<td>n/a</td>
<td>分组，JsonPath不支持</td>
</tr>
</tbody>
</table>

<h2 id="toc_8">示例：</h2>

<p>我们以拉勾网城市JSON文件 <a href="http://www.lagou.com/lbs/getAllCitySearchLabels.json">http://www.lagou.com/lbs/getAllCitySearchLabels.json</a> 为例，获取所有城市。</p>

<pre><code># jsonpath_lagou.py

import urllib2
import jsonpath
import json
import chardet

url = &#39;http://www.lagou.com/lbs/getAllCitySearchLabels.json&#39;
request =urllib2.Request(url)
response = urllib2.urlopen(request)
html = response.read()

# 把json格式字符串转换成python对象
jsonobj = json.loads(html)

# 从根节点开始，匹配name节点
citylist = jsonpath.jsonpath(jsonobj,&#39;$..name&#39;)

print citylist
print type(citylist)
fp = open(&#39;city.json&#39;,&#39;w&#39;)

content = json.dumps(citylist, ensure_ascii=False)
print content

fp.write(content.encode(&#39;utf-8&#39;))
fp.close()

</code></pre>

<h2 id="toc_9">注意事项：</h2>

<p>json.loads() 是把 Json格式字符串解码转换成Python对象，如果在json.loads的时候出错，要注意被解码的Json字符的编码。</p>

<p>如果传入的字符串的编码不是UTF-8的话，需要指定字符编码的参数 <code>encoding</code></p>

<pre><code>dataDict = json.loads(jsonStrGBK);

</code></pre>

<ul>
<li><p>dataJsonStr是JSON字符串，假设其编码本身是非UTF-8的话而是GBK 的，那么上述代码会导致出错，改为对应的：</p>

<pre><code>  dataDict = json.loads(jsonStrGBK, encoding=&quot;GBK&quot;);

</code></pre></li>
<li><p>如果 dataJsonStr通过encoding指定了合适的编码，但是其中又包含了其他编码的字符，则需要先去将dataJsonStr转换为Unicode，然后再指定编码格式调用json.loads()</p></li>
</ul>

<pre><code>``` python

</code></pre>

<p>dataJsonStrUni = dataJsonStr.decode(&quot;GB2312&quot;); dataDict = json.loads(dataJsonStrUni, encoding=&quot;GB2312&quot;);</p>

<pre><code>

</code></pre>

]]></content>
  </entry>
  
</feed>
