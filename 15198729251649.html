<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  Spark算子详解 - Time渐行渐远
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="Time渐行渐远" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:dmlcoding.com ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_blank" href="collections.html">COLLECTION</a></li>
        
        <li id=""><a target="_blank" href="tools.html">TOOLS</a></li>
        
        <li id=""><a target="_blank" href="about.html">ABOUT</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; Time渐行渐远</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="BigData.html">BigData</a></li>
        
            <li><a href="Python.html">Python</a></li>
        
            <li><a href="Linux.html">Linux</a></li>
        
            <li><a href="Life.html">Life</a></li>
        
            <li><a href="Spark.html">Spark</a></li>
        
            <li><a href="Yarn.html">Yarn</a></li>
        
            <li><a href="Scala.html">Scala</a></li>
        
            <li><a href="VPS.html">VPS</a></li>
        
            <li><a href="MachineLearning.html">MachineLearning</a></li>
        
            <li><a href="Writer.html">Writer</a></li>
        
            <li><a href="Shell.html">Shell</a></li>
        
            <li><a href="Algorithm.html">Algorithm</a></li>
        
            <li><a href="Mac.html">Mac</a></li>
        
            <li><a href="Presto.html">Presto</a></li>
        
            <li><a href="Mysql.html">Mysql</a></li>
        
            <li><a href="Maven.html">Maven</a></li>
        
            <li><a href="Kafka.html">Kafka</a></li>
        
            <li><a href="Java.html">Java</a></li>
        
            <li><a href="DevTools.html">DevTools</a></li>
        
            <li><a href="Hbase.html">Hbase</a></li>
        
            <li><a href="ELK.html">ELK</a></li>
        
            <li><a href="Druid.html">Druid</a></li>
        
            <li><a href="Docker.html">Docker</a></li>
        
            <li><a href="DesignPattern.html">DesignPattern</a></li>
        
            <li><a href="Computer.html">Computer</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
  $(function(){
    $('#menu_item_index').addClass('is_active');
  });
</script>
<div class="row">
  <div class="large-8 medium-8 columns">
      <div class="markdown-body article-wrap">
       <div class="article">
          
          <h1>Spark算子详解</h1>
     
        <div class="read-more clearfix">
          <span class="date">2016/11/16</span>

          <span>posted in&nbsp;</span> 
          
              <span class="posted-in"><a href='Spark.html'>Spark</a></span>
           
         
          <span class="comments">
            

            
          </span>

        </div>
      </div><!-- article -->

      <div class="article-content">
      <h3 id="toc_0">aggregate</h3>

<blockquote>
<p>def aggregate<a href="zeroValue:%20U">U: ClassTag</a>(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U</p>
</blockquote>

<span id="more"></span><!-- more -->

<pre><code>    val conf=new SparkConf().setAppName(&quot;aggregate&quot;).setMaster(&quot;local[*]&quot;)
    val sc=new SparkContext(conf)
    val z=sc.parallelize(List(1,2,3,4,5,6),2)
//打印每个分区里面的内容
    z.glom().foreach(arr=&gt;{
      println(arr.foldLeft(&quot; &quot;)((x,y)=&gt;x+y))
    })

    // This example returns 16 since the initial value is 5 设置处理值5,注意这个初始值会出现在每个每个分区中作为初始值。就像下面的，每个分区都有5
    // reduce of partition 0 will be max(5, 1, 2, 3) = 5
    // reduce of partition 1 will be max(5, 4, 5, 6) = 6
    // final reduce across partitions will be 5 + 5 + 6 = 16  进行最后一个方法的时候，还会加上初始值5
    // note the final reduce include the initial value 最后得出的才是结果
    // 因此得出结论，aggregate聚合算子的三个参数分别是。第一个是设置初始值。第二个Sep函数是对每个分区中的内容进行操作的函数。第三个Combi函数是聚合每个分区的函数
    val result=z.aggregate(0)(math.max(_,_),_+_)
    val result1=z.aggregate(5)(math.max(_,_),_+_)
    println(&quot;result: &quot;+result)
    println(&quot;result1: &quot;+result1)
//    456
//    123
//    result: 9
//    result1: 16
    sc.stop()
</code></pre>

<h3 id="toc_1">cartesian</h3>

<blockquote>
<p>def cartesian<a href="other:%20RDD%5BU%5D">U: ClassTag</a>: RDD[(T, U)]</p>
</blockquote>

<p>将其中一个分区里面的元素跟另一个分区的每一个元素进行笛卡尔积计算.</p>

<pre><code>    val conf = new SparkConf().setAppName(&quot;Cartesian&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val data1 = Array[(String, Int)]((&quot;A1&quot;, 1), (&quot;A2&quot;, 2),
      (&quot;B1&quot;, 3), (&quot;B2&quot;, 4),
      (&quot;C1&quot;, 5), (&quot;C1&quot;, 6))

    val data2 = Array[(String, Int)]((&quot;A1&quot;, 7), (&quot;A2&quot;, 8),
      (&quot;B1&quot;, 9), (&quot;C1&quot;, 0))
    val pairs1 = sc.parallelize(data1, 3)
    val pairs2 = sc.parallelize(data2, 2)

    val resultRDD = pairs1.cartesian(pairs2)

    resultRDD.foreach(println)
</code></pre>

<h3 id="toc_2">collectAsMap</h3>

<blockquote>
<p>把元组作为元素的List转化成Map,主要是调用可变的HashMap</p>
</blockquote>

<pre><code>    val conf = new SparkConf().setAppName(&quot;CollectAsMap&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val data = Array[(String, Int)]((&quot;A&quot;, 1), (&quot;B&quot;, 2),
      (&quot;B&quot;, 3), (&quot;C&quot;, 4),
      (&quot;C&quot;, 5), (&quot;C&quot;, 6))

    // as same as &quot;val pairs = sc.parallelize(data, 3)&quot; 底层源码,也还是调用的parallelize
    val pairs = sc.makeRDD(data, 3)
    val result = pairs.collectAsMap

    // output Map(A -&gt; 1, C -&gt; 6, B -&gt; 3)
    print(result)

//    list2map(data)
  }

  def list2map(list: Array[(String, Int)]): mutable.HashMap[String, Int] = {
    val map = new mutable.HashMap[String, Int]
    list.foreach(value=&gt; map.put(value._1,value._2))
    map
</code></pre>

<h3 id="toc_3">flatMap</h3>

<blockquote>
<p>def flatMap<a href="f:%20T%20=%3E%20TraversableOnce%5BU%5D">U: ClassTag</a>: RDD[U]</p>
</blockquote>

<pre><code>    val conf = new SparkConf().setAppName(&quot;FlatMap&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val array=Array[String](&quot;Hello&quot;,&quot;World&quot;,&quot;hadoop&quot;)
    val strRDD=sc.parallelize(array)
    val str2arrayRDD=strRDD.flatMap(x=&gt;x.toCharArray)
    str2arrayRDD.foreach(println)
</code></pre>

<h3 id="toc_4">GroupByKey</h3>

<blockquote>
<p>def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]</p>
</blockquote>

<p>会造成shuffle的算子，根据key分组</p>

<pre><code>    var numMappers = 10
    var numPartition=10
    var numKVPairs = 100
    var valSize = 100
    var numReducers = 3

    val conf = new SparkConf().setAppName(&quot;GroupBy Test&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
//  造一些测试数据
//    0到10，10次循环，10个分区
//    每次循环里面造100个键值对
    val pairs1 = sc.parallelize(0 until numMappers, numPartition).flatMap { p =&gt;
      val ranGen = new Random
      var arr1 = new Array[(Int, Array[Byte])](numKVPairs)
      for (i &lt;- 0 until numKVPairs) {
        val byteArr = new Array[Byte](valSize)
        ranGen.nextBytes(byteArr)
        arr1(i) = (ranGen.nextInt(10), byteArr)
      }
      arr1
    }.cache
    // cache一下
//    因此一共是10X100=1000个
    println(&quot;pairs1.count: &quot;+pairs1.count)

    val result = pairs1.groupByKey(numReducers)
//    造数据的时候，key的取值是10以内，所以分组肯定是10个，和结果一样
    println(&quot;result.count: &quot;+result.count)
    println(result.toDebugString)

    sc.stop()
</code></pre>

<h3 id="toc_5">groupBy</h3>

<blockquote>
<p>def groupBy<a href="f:%20T%20=%3E%20K">K</a>(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])]</p>
</blockquote>

<pre><code>    val conf = new SparkConf().setAppName(&quot;GroupByAction&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val data = Array[(String, Int)]((&quot;A1&quot;, 1), (&quot;A2&quot;, 2),
      (&quot;B1&quot;, 6), (&quot;A2&quot;, 4),
      (&quot;B1&quot;, 3), (&quot;B1&quot;, 5))

    val pairs = sc.parallelize(data, 3)
    pairs.foreach(println)
    val result1 = pairs.groupBy(K =&gt; K._1)
    // 设置分区数量
    val result2 = pairs.groupBy((K: (String, Int)) =&gt; K._1,1)
    // 定义分区的类
    val result3 = pairs.groupBy((K: (String, Int)) =&gt; K._1, new RangePartitioner(3, pairs))

    result1.foreach(println)
    result2.foreach(println)
    result3.foreach(println)
</code></pre>

<h3 id="toc_6">groupWith</h3>

<blockquote>
<p>def groupWith<a href="other:%20RDD%5B(K,%20W)%5D">W</a>: RDD[(K, (Iterable[V], Iterable[W]))]</p>
</blockquote>

<p>Alias for cogroup</p>

<pre><code>    val conf = new SparkConf().setAppName(&quot;GroupWith&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)

    val data1 = Array[(String, Int)]((&quot;A1&quot;, 1), (&quot;A2&quot;, 2),
      (&quot;B1&quot;, 3), (&quot;B2&quot;, 4),
      (&quot;C1&quot;, 5), (&quot;C1&quot;, 6)
    )

    val data2 = Array[(String, Int)]((&quot;A1&quot;, 7), (&quot;A2&quot;, 8),
      (&quot;B1&quot;, 9), (&quot;C1&quot;, 0)
    )
    val pairs1 = sc.parallelize(data1, 3)
    val pairs2 = sc.parallelize(data2, 2)

    val result = pairs1.groupWith(pairs2)
    result.foreach(println)
</code></pre>

<h3 id="toc_7">join</h3>

<blockquote>
<p>def join<a href="other:%20RDD%5B(K,%20W)%5D,%20partitioner:%20Partitioner">W</a>: RDD[(K, (V, W))]</p>
</blockquote>

<pre><code>    val conf = new SparkConf().setAppName(&quot;JoinAction&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)

    val data1 = Array[(String, Int)]((&quot;A1&quot;, 1), (&quot;A2&quot;, 2),
      (&quot;B1&quot;, 3), (&quot;B2&quot;, 4),
      (&quot;C1&quot;, 5), (&quot;C1&quot;, 6)
    )

    val data2 = Array[(String, Int)]((&quot;A1&quot;, 7), (&quot;A2&quot;, 8),
      (&quot;B1&quot;, 9), (&quot;C1&quot;, 0)
    )
    val pairs1 = sc.parallelize(data1, 3)
    val pairs2 = sc.parallelize(data2, 2)


    val result = pairs1.join(pairs2)
    result.foreach(println)
</code></pre>

<h3 id="toc_8">lookup</h3>

<blockquote>
<p>def lookup(key: K): Seq[V]</p>
</blockquote>

<p>(针对pair类型的RDD)根据这个K值,找出相应的value值，放入到List里面返回出来</p>

<pre><code>
    val conf = new SparkConf().setAppName(&quot;LookUp&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val data = Array[(String, Int)]((&quot;A&quot;, 1), (&quot;B&quot;, 2),
      (&quot;B&quot;, 3), (&quot;C&quot;, 4),
      (&quot;C&quot;, 5), (&quot;C&quot;, 6))

    val pairs = sc.parallelize(data, 3)

    val finalRDD = pairs.lookup(&quot;B&quot;)

    finalRDD.foreach(println)
</code></pre>

<h3 id="toc_9">mapPartitions</h3>

<blockquote>
<p>def mapPartitions<a href="f:%20Iterator%5BT%5D%20=%3E%20Iterator%5BU%5D,preservesPartitioning:%20Boolean%20=%20false">U: ClassTag</a>: RDD[U]</p>
</blockquote>

<p>是针对这个RDD的每个partition进行操作的，比如在进行数据库链接的时候，使用这个算子，可以减少</p>

<pre><code>val conf = new SparkConf().setAppName(&quot;MapPartitionsRDD&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val data = Array[(String, Int)]((&quot;A1&quot;, 1), (&quot;A2&quot;, 2),
      (&quot;B1&quot;, 1), (&quot;B2&quot;, 4),
      (&quot;C1&quot;, 3), (&quot;C2&quot;, 4)
    )
    val pairs = sc.parallelize(data, 3)

    val finalRDD = pairs.mapPartitions(iter =&gt; iter.filter(_._2 &gt;= 2))

    finalRDD.foreachPartition(iter =&gt; {
      while (iter.hasNext) {
        val next = iter.next()
        println(next._1 + &quot; --- &quot; + next._2)

      }
    })
</code></pre>

<h3 id="toc_10">mapValues</h3>

<blockquote>
<p>def mapValues<a href="f:%20V%20=%3E%20U">U</a>: RDD[(K, U)]</p>
</blockquote>

<p>不改变RDD原有的分区，也不改变Key值，修改value的值</p>

<pre><code>    val conf = new SparkConf().setAppName(&quot;mapValues&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val data1 = Array[(String, Int)]((&quot;K&quot;, 1), (&quot;T&quot;, 2),
      (&quot;T&quot;, 3), (&quot;W&quot;, 4),
      (&quot;W&quot;, 5), (&quot;W&quot;, 6)
    )
    val pairs = sc.parallelize(data1, 3)
    val result = pairs.mapValues(V =&gt; 10 * V)
//    val result=pairs.map { case (k, v) =&gt; (k, v * 10) }
    result.foreach(println)
</code></pre>

<h3 id="toc_11">partitionBy</h3>

<blockquote>
<p>def partitionBy(partitioner: Partitioner): RDD[(K, V)]</p>
</blockquote>

<p>用指定的分区类来处理RDD</p>

<pre><code>    val conf = new SparkConf().setAppName(&quot;partitionBy&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val data1 = Array[(String, Int)]((&quot;K&quot;, 1), (&quot;T&quot;, 2),
      (&quot;T&quot;, 3), (&quot;W&quot;, 4),
      (&quot;W&quot;, 5), (&quot;W&quot;, 6)
    )
    val pairs = sc.parallelize(data1, 3)
    val result = pairs.partitionBy(new RangePartitioner(2, pairs, true))
    //val result = pairs.partitionBy(new HashPartitioner(2))
    result.foreach(println)
</code></pre>

<h3 id="toc_12">pipe</h3>

<blockquote>
<p>def pipe(command: String): RDD[String]</p>
</blockquote>

<p>command命令在windows上测不了额</p>

<pre><code>    val conf = new SparkConf().setAppName(&quot;Pip&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val data1 = Array[(String, Int)]((&quot;K1&quot;, 1), (&quot;K2&quot;, 2),
      (&quot;U1&quot;, 3), (&quot;U2&quot;, 4),
      (&quot;W1&quot;, 3), (&quot;W2&quot;, 4)
    )
    val pairs = sc.parallelize(data1, 3)
    val finalRDD = pairs.pipe(&quot;grep 2&quot;)
    finalRDD.foreach(println)
</code></pre>

<h3 id="toc_13">reduce</h3>

<blockquote>
<p>def reduce(f: (T, T) =&gt; T): T</p>
</blockquote>

<p>迭代处理之前的元素</p>

<pre><code>    val conf = new SparkConf().setAppName(&quot;reduce&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val data1 = Array[(String, Int)]((&quot;K1&quot;, 1), (&quot;K2&quot;, 2),
      (&quot;U1&quot;, 3), (&quot;U2&quot;, 4),
      (&quot;W1&quot;, 3), (&quot;W2&quot;, 4)
    )
    val pairs = sc.parallelize(data1, 3)
    val result = pairs.reduce((A, B) =&gt; (A._1 + &quot;#&quot; + B._1, A._2 + B._2))
    //val result = pairs.fold((&quot;K0&quot;,10))((A, B) =&gt; (A._1 + &quot;#&quot; + B._1, A._2 + B._2))

    println(result)
</code></pre>

<h3 id="toc_14">reduceByKey</h3>

<blockquote>
<p>reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]</p>
</blockquote>

<p>有好几个重载方法,可以指定分区数,分区的类</p>

<pre><code>    val conf = new SparkConf().setAppName(&quot;reduceByKey&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val data1 = Array[(String, Int)]((&quot;K&quot;, 1), (&quot;U&quot;, 2),
      (&quot;U&quot;, 3), (&quot;W&quot;, 4),
      (&quot;W&quot;, 5), (&quot;W&quot;, 6)
    )
    val pairs = sc.parallelize(data1, 3)
    val result = pairs.reduceByKey(_ + _)
    result.foreach(println)
</code></pre>

<h3 id="toc_15">sortByKey</h3>

<blockquote>
<p>def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length): RDD[(K, V)]</p>
</blockquote>

<pre><code>    val conf = new SparkConf().setAppName(&quot;sortByKey&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)

    val data1 = Array[(String, Int)]((&quot;K1&quot;, 1), (&quot;K2&quot;, 2),
      (&quot;U1&quot;, 3), (&quot;U2&quot;, 4),
      (&quot;W1&quot;, 5), (&quot;W1&quot;, 6)
    )
    val pairs1 = sc.parallelize(data1, 3)

    //val result = pairs.fold((&quot;K0&quot;,10))((A, B) =&gt; (A._1 + &quot;#&quot; + B._1, A._2 + B._2))

    val result = pairs1.sortByKey()
    result.foreach(println)
</code></pre>

<h3 id="toc_16">take</h3>

<blockquote>
<p>def take(num: Int): Array[T]</p>
</blockquote>

<pre><code>    val conf = new SparkConf().setAppName(&quot;take&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val data1 = Array[(String, Int)]((&quot;K1&quot;, 1), (&quot;K2&quot;, 2),
      (&quot;U1&quot;, 3), (&quot;U2&quot;, 4),
      (&quot;W1&quot;, 3), (&quot;W2&quot;, 4)
    )
    val pairs = sc.parallelize(data1, 3)
    val result = pairs.take(5)
    result.foreach(println)
</code></pre>

<h3 id="toc_17">union</h3>

<blockquote>
<p>def union(other: RDD[T]): RDD[T]</p>
</blockquote>

<p>union聚合算子,不会去重,如果需要去重,调用distinct算子</p>

<pre><code>    val conf = new SparkConf().setAppName(&quot;union&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)

    val data1 = Array[(String, Int)]((&quot;K1&quot;, 1), (&quot;K2&quot;, 2),
      (&quot;U1&quot;, 3), (&quot;U2&quot;, 4),
      (&quot;W1&quot;, 5), (&quot;W1&quot;, 6)
    )

    val data2 = Array[(String, Int)]((&quot;K1&quot;, 7), (&quot;K2&quot;, 8),
      (&quot;U1&quot;, 9), (&quot;W1&quot;, 0)
    )
    val pairs1 = sc.parallelize(data1, 3)
    val pairs2 = sc.parallelize(data2, 2)
    val result = pairs1.union(pairs2)
    result.foreach(println)
</code></pre>

<h3 id="toc_18">checkpoint</h3>

<blockquote>
<p>def checkpoint(): Unit</p>
</blockquote>

<p>设置了checkpoint后,写下来的数据,不会因为应用的停止而删除</p>

<pre><code>    var numMappers = 10
    var numPartition = 10
    var numKVPairs = 100
    var valSize = 100
    var numReducers = 3

    val conf = new SparkConf().setAppName(&quot;Checkpoint&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    sc.setCheckpointDir(&quot;D:\\data\\checkpoint&quot;)
    //  造一些测试数据
    //    0到10，10次循环，10个分区
    //    每次循环里面造100个键值对
    val pairs1 = sc.parallelize(0 until numMappers, numPartition).flatMap { p =&gt;
      val ranGen = new Random
      var arr1 = new Array[(Int, Array[Byte])](numKVPairs)
      for (i &lt;- 0 until numKVPairs) {
        val byteArr = new Array[Byte](valSize)
        ranGen.nextBytes(byteArr)
        arr1(i) = (ranGen.nextInt(10), byteArr)
      }
      arr1
    }.cache
    // cache一下
    //    因此一共是10X100=1000个
    println(&quot;pairs1.count: &quot; + pairs1.count)

    val result = pairs1.groupByKey(numReducers)
    result.checkpoint

    //    造数据的时候，key的取值是10以内，所以分组肯定是10个，和结果一样
    println(&quot;result.count: &quot; + result.count)
    println(result.toDebugString)

    sc.stop()
</code></pre>

<h3 id="toc_19">zip</h3>

<blockquote>
<p>def zip<a href="other:%20RDD%5BU%5D">U: ClassTag</a>: RDD[(T, U)]</p>
</blockquote>

<p>注意: zip算子默认是认为这两个RDD的分区数和每个分区里面的个数是一样的，如果不能保证这两个条件，那么就会报错。如果不符合，那么可以用zipPartitions</p>

<pre><code>    val conf = new SparkConf().setAppName(&quot;zip&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    //    构造数据集
    val numberRDD = sc.parallelize(1 to 20, 4)
    val numberRDDCopy = sc.parallelize(1 to 20, 4)
    val letterRDD = sc.parallelize(&#39;a&#39; to &#39;z&#39;, 4)

//    val result = numberRDD.zip(letterRDD)
    val result = numberRDD.zip(numberRDDCopy)
    try {
      result foreach {
        case (c, i) =&gt; println(i + &quot;:  &quot; + c)
      }
    } catch {
      case iae: IllegalArgumentException =&gt;
        println(&quot;Exception caught IllegalArgumentException: &quot; + iae.getMessage)
      case se: SparkException =&gt; {
        val t = se.getMessage
        println(&quot;Exception caught SparkException: &quot; + se.getMessage)
      }
    }
</code></pre>

<h3 id="toc_20">zipPartitions</h3>

<blockquote>
<p>def zipPartitions<a href="rdd2:%20RDD%5BB%5D,%20preservesPartitioning:%20Boolean">B: ClassTag, V: ClassTag</a>(f: (Iterator[T], Iterator[B]) =&gt; Iterator[V]): RDD[V]</p>
</blockquote>

<p>这个的灵活性就比zip算子高得多了</p>

<pre><code>    val conf = new SparkConf().setAppName(&quot;zipPartitions&quot;).setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    //    构造数据集
    val numberRDD = sc.parallelize(1 to 20, 4)
    val letterRDD = sc.parallelize(&#39;a&#39; to &#39;z&#39;, 4)
//    printRDD(numberRDD)
//    printRDD(letterRDD)
//    假设所有的partiton里面的元素个数是一样，当不一样的时候，我们需要处理一下咯


    def func(numIter: Iterator[Int], lettIter: Iterator[Char]): Iterator[(Int, Char)] = {
      val arr = new ListBuffer[(Int, Char)]
      while (numIter.hasNext || lettIter.hasNext) {

        if (numIter.hasNext &amp;&amp; lettIter.hasNext) {
          arr += ((numIter.next(), lettIter.next()))
        } else if (numIter.hasNext) {
          arr += ((numIter.next(), &#39; &#39;))
        } else if (lettIter.hasNext) {
          arr += ((0, lettIter.next()))
        }
      }
      arr.iterator
    }

    val result=numberRDD.zipPartitions(letterRDD)(func)
    result.foreach{
      case (k,v)=&gt;println(k+&quot; &quot;+v)
    }
  }

  def printRDD(rdd: RDD[_]) = {
    val nrdd = rdd.glom().zipWithIndex()
    nrdd.foreach {
      case (k, v) =&gt;
        println(&quot;第&quot; + v + &quot;个分区数据: &quot; + k.foldLeft(&quot;&quot;)((x, y) =&gt; x + &quot; &quot; + y))
    }
  }
</code></pre>


    

      </div>

      <div class="row">
        <div class="large-6 columns">
        <p class="text-left" style="padding:15px 0px;">
      
          <a href="15198751889703.html" 
          title="Previous Post: 大数据学习时候,造数据的常用方式">&laquo; 大数据学习时候,造数据的常用方式</a>
      
        </p>
        </div>
        <div class="large-6 columns">
      <p class="text-right" style="padding:15px 0px;">
      
          <a  href="15198737601432.html" 
          title="Next Post: Scala数组">Scala数组 &raquo;</a>
      
      </p>
        </div>
      </div>
      <div class="comments-wrap">
        <div class="share-comments">
          

          

          
        </div>
      </div>
    </div><!-- article-wrap -->
  </div><!-- large 8 -->




 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="https://s.gravatar.com/avatar/2d9bdeec5ad2754d8a47063df1346ee8?s=80" /></div>
            
                <h1>Time渐行渐远</h1>
                <div class="site-des">Coding Changing The World</div>
                <div class="social">








<a target="_blank" class="twitter" target="_blank" href="https://twitter.com/HswTime" title="Twitter">Twitter</a>
<a target="_blank" class="github" target="_blank" href="https://github.com/Timehsw" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:hsw_v5@163.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="BigData.html"><strong>BigData</strong></a>
        
            <a href="Python.html"><strong>Python</strong></a>
        
            <a href="Linux.html"><strong>Linux</strong></a>
        
            <a href="Life.html"><strong>Life</strong></a>
        
            <a href="Spark.html"><strong>Spark</strong></a>
        
            <a href="Yarn.html"><strong>Yarn</strong></a>
        
            <a href="Scala.html"><strong>Scala</strong></a>
        
            <a href="VPS.html"><strong>VPS</strong></a>
        
            <a href="MachineLearning.html"><strong>MachineLearning</strong></a>
        
            <a href="Writer.html"><strong>Writer</strong></a>
        
            <a href="Shell.html"><strong>Shell</strong></a>
        
            <a href="Algorithm.html"><strong>Algorithm</strong></a>
        
            <a href="Mac.html"><strong>Mac</strong></a>
        
            <a href="Presto.html"><strong>Presto</strong></a>
        
            <a href="Mysql.html"><strong>Mysql</strong></a>
        
            <a href="Maven.html"><strong>Maven</strong></a>
        
            <a href="Kafka.html"><strong>Kafka</strong></a>
        
            <a href="Java.html"><strong>Java</strong></a>
        
            <a href="DevTools.html"><strong>DevTools</strong></a>
        
            <a href="Hbase.html"><strong>Hbase</strong></a>
        
            <a href="ELK.html"><strong>ELK</strong></a>
        
            <a href="Druid.html"><strong>Druid</strong></a>
        
            <a href="Docker.html"><strong>Docker</strong></a>
        
            <a href="DesignPattern.html"><strong>DesignPattern</strong></a>
        
            <a href="Computer.html"><strong>Computer</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15199545224004.html">思考</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15199040835190.html">kafka-sparkstreaming长时间运行后异常退出原因分析</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15198768495613.html">Elasticsearch：用Curator辅助Marvel，实现自动删除旧marvel索引</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15198743666304.html">PythonVirtualenv总结</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15198746079468.html">机器学习实战之朴素贝叶斯</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?2a025c7742b90ad62b1e767e1b7f2f29";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  </body>
</html>
